{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W1D2 - as_strided, einsum, and Build Your Own ResNet\n\nResNet is a architecture named after its residual connections. In the 2015 paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf), the authors demonstrated with a winning solution to ILVSRC 2015 that  residual connections were a solution to the problem of training neural networks with hundreds of layers.\n\nAt this time, it was difficult to train networks with more than tens of layers. Even with normalization layers to prevent gradients from vanishing or exploding, and even with robust data augmentation to reduce overfitting, optimizers like SGD struggled to find solutions with low training error, let alone good generalization.\n\nThe authors provided empirical evidence that ResNets were effective, and in recent years the theory of residual connections has developed. For example, we now know that [Residual Networks Behave Like Ensembles of Relatively Shallow Networks](https://arxiv.org/pdf/1605.06431.pdf), and that under some conditions ResNets are [guaranteed to converge to a global (not local!) optimum in polynomial time](https://arxiv.org/pdf/1811.03804.pdf).\n\nThe ResNet we'll build today is no longer state of the art, but you'll find residual connections appearing again and again throughout the course, so it's worth understanding this idea in detail.\n\n## Table of Contents\n\n- [Readings](#readings)\n- [Inference Using a Pre-Trained ResNet](#inference-using-a-pre-trained-resnet)\n    - [Gotcha 1 - Training vs Eval Mode](#gotcha----training-vs-eval-mode)\n    - [Gotcha 2 - Input Normalization](#gotcha----input-normalization)\n    - [Gotcha 3 - no_grad / inference_mode](#gotcha----nograd--inferencemode)\n    - [The `tqdm` library](#the-tqdm-library)\n- [Loading and Caching Images](#loading-and-caching-images)\n- [Torchvision Transforms](#torchvision-transforms)\n- [Image Preprocessing](#image-preprocessing)\n- [ImageNet Class Names](#imagenet-class-names)\n- [Making Predictions (pretrained model)](#making-predictions-pretrained-model)\n- [Practice with `einsum` and `as_strided`](#practice-with-einsum-and-asstrided)\n    - [Trace of a matrix](#trace-of-a-matrix)\n    - [Matrix Multiplication](#matrix-multiplication)\n    - [PyTorch's `conv1d` Exercises](#pytorchs-convd-exercises)\n    - [Your Own `conv1d`](#your-own-convd)\n    - [Your Own `conv2d`](#your-own-convd)\n    - [Padding a Tensor](#padding-a-tensor)\n    - [Padding and Stride for `conv1d`](#padding-and-stride-for-convd)\n    - [Helper functions for pairs](#helper-functions-for-pairs)\n    - [Padding and Stride for `conv2d`](#padding-and-stride-for-convd)\n- [Max Pooling](#max-pooling)\n    - [Module version](#module-version)\n    - [Constructor](#constructor)\n    - [extra_repr](#extrarepr)\n- [Linear Module](#linear-module)\n    - [Initialization](#initialization)\n- [`Conv2d` - nn.Module version](#convd---nnmodule-version)\n- [Batch Normalization](#batch-normalization)\n    - [Train and Eval Modes](#train-and-eval-modes)\n- [ReLU Module](#relu-module)\n- [Sequential](#sequential)\n- [Flatten](#flatten)\n- [ResNet Average Pooling](#resnet-average-pooling)\n- [Assembling ResNet](#assembling-resnet)\n    - [Residual Block](#residual-block)\n    - [BlockGroup](#blockgroup)\n    - [ResNet34](#resnet)\n- [Copying Pretrained Weights](#copying-pretrained-weights)\n- [Running Your Model](#running-your-model)\n- [Training ResNet on CIFAR10](#training-resnet-on-cifar)\n    - [Preparing the CIFAR10 Data](#preparing-the-cifar-data)\n    - [Train your ResNet](#train-your-resnet)\n    - [Test Your ResNet](#test-your-resnet)\n- [Bonus](#bonus)\n    - [Fused BatchNorm and Conv2d](#fused-batchnorm-and-convd)\n    - [Deeper Look at Initialization](#deeper-look-at-initialization)\n    - [Residual Block Identity Initialization](#residual-block-identity-initialization)\n    - [Data Augmentation](#data-augmentation)\n    - [ReLU Benchmarking](#relu-benchmarking)\n    - [Negative Strides](#negative-strides)\n    - [Adversarial Examples](#adversarial-examples)\n    - [Build Your Own Dtype Support](#build-your-own-dtype-support)\n    - [Benchmarking Inference Mode](#benchmarking-inference-mode)\n\n## Readings\n\n- [Batch Normalization in Convolutional Neural Networks](https://www.baeldung.com/cs/batch-normalization-cnn)\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n\n## Inference Using a Pre-Trained ResNet\n\nFirst, we'll load a pre-trained ResNet34 from the [torchvision.models](https://pytorch.org/vision/stable/models.html) package and use it to classify some images. The 34 refers to the number of layers in the model.\n\nThere are at least three things that are easy to forget. If you forget these things then your model will still run without any errors, and will likely even perform decently, which makes it tricky to even detect that you've done something wrong. Unfortunately, this is just the reality of working on ML code.\n\n### Gotcha 1 - Training vs Eval Mode\n\nBy default, the model is in training mode when loaded, but we want it to be in evaluation mode for inference. This means that `model.training` is True, and for each submodule their corresponding flag like `model.bn1.training` is True as well. Calling `model.eval()` sets the flag to False on the model and recursively on all submodels.\n\nWhat does this actually do? The two cases you'll need to know for the course are batch normalization and dropout layers.\n\n- Batch normalization in training mode calculates mean and variance based on the current batch, which can be ok-ish or really bad depending on the size of your batches at inference time. In eval mode it uses aggregated statistics saved in the earlier training process.\n- Dropout in training mode randomly sets activations to zero and rescales the others. In eval mode it does nothing.\n\n### Gotcha 2 - Input Normalization\n\nThe ResNet was trained on images that were normalized using special constants, which are in the PyTorch documentation. It thus expects any images you give it to be also normalized using these special constants. If you forget to do this, you might see the model still gives the right classification, but the logits are small for every class because the image doesn't quite look like anything it was trained on.\n\nAnother way I have personally messed this up is if by using images that are already normalized, but you normalize them a second time by mistake.\n\n### Gotcha 3 - no_grad / inference_mode\n\nPyTorch is built around making backpropagation easy, and by default it does some extra work just in case you want to run backpropagation later. This is especially true on tensors that have `requires_grad` set to True, which pretrained model does by default. When you implement your own backpropagation later, you'll understand this in more detail.\n\nWe can tell PyTorch not to do any of this extra work by wrapping calls to our model in `with t.inference_mode():` as described in the [PyTorch docs](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation).\n\nIf you've seen `with t.no_grad():` before, `inference_mode` is a newer feature (released in PyTorch 1.9) that does everything `no_grad` does plus a couple more optimizations.\n\n### The `tqdm` library\n\nThe [tqdm](https://pypi.org/project/tqdm/) library provides nice progress bars so you can see how long operations are going to take. It's very easy to use - just wrap your iterable in `tqdm()`. I recommend using `tqdm` for everything in the course that takes more than a couple seconds to run.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import json\nimport os\nimport sys\nfrom collections import OrderedDict\nfrom io import BytesIO\nfrom typing import Callable, Optional, Union\nfrom pathlib import Path\nimport requests\nimport torch as t\nimport torchvision\nfrom einops import rearrange\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, UnidentifiedImageError\nfrom torch import nn\nfrom torch.nn.functional import conv1d as torch_conv1d\nfrom torch.utils.data import DataLoader\nfrom torchvision import models, transforms\nfrom tqdm.auto import tqdm\nimport utils\nimport w1d2_test\n\nMAIN = __name__ == \"__main__\"\nIS_CI = os.getenv(\"IS_CI\")\nimages: list[Image.Image] = []\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Loading and Caching Images\n\nWe've provided some sample images; feel free to call `download_image` some URLs of your choice and append them to the `IMAGES` list. It's polite to cache the images locally instead of downloading them every time the notebook is run, to minimize bandwidth usage.\n\n<details>\n\n<summary>I get UnidentifiedImageError when I try to load my own image!</summary>\n\nThis usually means the image format isn't supported on your OS and version of the image library. You can try manually converting the image to a common format like JPEG, or just use another image.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "IMAGE_FOLDER = Path(\"./w1d2_images\")\nIMAGE_FOLDER.mkdir(exist_ok=True)\nIMAGE_FILENAMES = [\n    \"chimpanzee.jpg\",\n    \"golden_retriever.jpg\",\n    \"platypus.jpg\",\n    \"frogs.jpg\",\n    \"fireworks.jpg\",\n    \"astronaut.jpg\",\n    \"iguana.jpg\",\n    \"volcano.jpg\",\n    \"goofy.jpg\",\n    \"dragonfly.jpg\",\n]\n\n\ndef download_image(url: str, filename: Optional[str]) -> None:\n    \"\"\"Download the image at url to w1d2_images/{filename}, if not already cached.\"\"\"\n    if filename is None:\n        filename = url.rsplit(\"/\", 1)[1].replace(\"%20\", \"\")\n    path = IMAGE_FOLDER / filename\n    if not path.exists():\n        response = requests.get(url)\n        data = response.content\n        with path.open(\"wb\") as f:\n            f.write(data)\n\n\nif MAIN:\n    images = [Image.open(IMAGE_FOLDER / filename) for filename in tqdm(IMAGE_FILENAMES)]\n    if not IS_CI:\n        display(images[0])\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Torchvision Transforms\n\nClasses in the `transforms` module are either subclasses of `nn.Module`, or behave similarly in that they are callable. You can compose multiple transforms together with `transforms.Compose` or `nn.Sequential`.\n\nUsing the classes in the `transforms` module, define a variable `preprocess` to be a composition of `transforms.ToTensor`, `transforms.Resize`, and `transforms.Normalize`.\n\nResize every image to (224, 224) - while the model can support larger images than this, making them all of equal size is necessary for batching.\n\nFind the normalization constants in the [documentation](https://pytorch.org/vision/stable/transforms.html)\n\nIt's always a good idea to visualize your data after preprocessing to catch any errors, and get a feeling for what the model \"sees\". You can use [`plt.imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) to display a tensor as an image, but you'll need to `rearrange` the data to the format it expects.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "preprocess: Callable[[Image.Image], t.Tensor]\npreprocess = transforms.Compose([\"your code here\"])\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Image Preprocessing\n\nImplement `prepare_data` below.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def prepare_data(images: list[Image.Image]) -> t.Tensor:\n    \"\"\"Preprocess each image and stack them into a single tensor.\n\n    Return: shape (batch=len(images), num_channels=3, height=224, width=224)\n    \"\"\"\n    pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## ImageNet Class Names\n\nUse the following code to load the class labels for the 1000 classes in ImageNet.\n\nIf you haven't encountered ImageNet before, spend a minute looking at the various categories to appreciate just how difficult this classification task is. When we train the model, we give it only 1 correct answer and all the others are considered equally incorrect. For example, given a photo of a dog, knowing that this is a dog or even a terrier is not enough: the model has to distinguish 29 different categories of terrier.\n\nSome of the labels are duplicates for no apparent reason: \"laptop computer\" is a different category than \"notebook computer\" and \"projectile, missile\" is different from \"missile\". Another issue with the problem setting is that when more than one label truly applies, the model has to infer which one the human labeller had in mind. For example, a scene with a typical desktop PC can contain \"desktop computer\", \"desk\", \"mouse\", \"monitor\", and \"computer keyboard\" categories but the labeller would have only specified one of these.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "with open(\"w1d2_imagenet_labels.json\") as f:\n    imagenet_labels = list(json.load(f).values())\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Making Predictions (pretrained model)\n\nImplement `predict` below and call it with your images. Remember the gotcha discussed above.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def predict(model, images: list[Image.Image], print_topk_preds=3) -> list[int]:\n    \"\"\"\n    Pass the images through the model and print out the top predictions.\n\n    For each image, `display()` the image and the most likely categories according to the model.\n\n    Return: for each image, the index of the top prediction.\n    \"\"\"\n    pass\n\n\nif MAIN and (not IS_CI):\n    model = models.resnet34(weights=\"DEFAULT\")\n    pretrained_categories = predict(model, images)\n    print(pretrained_categories)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Practice with `einsum` and `as_strided`\n\nNow instead of using torchvision's code for ResNet, we're going to implement our own ResNet! This requires being comfortable with `einsum` and `as_strided`.\n\nIf you didn't do the pre-exercises on `as_strided`, it might be worth doing those now. Note that `x.as_strided(...)` is a method equivalent to `torch.as_strided(x, ...)`\n\n\n### Trace of a matrix\n\nImplement the following functions.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def einsum_trace(a: t.Tensor) -> t.Tensor:\n    \"\"\"Compute the trace of the square 2D input using einsum.\"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_trace(einsum_trace)\n    w1d2_test.test_trace_transpose(einsum_trace)\n    w1d2_test.test_trace_expand(einsum_trace)\n\n\ndef as_strided_trace(a: t.Tensor) -> t.Tensor:\n    \"\"\"Compute the trace of the square 2D input using as_strided and sum.\n\n    Tip: the stride argument to `as_strided` must account for the stride of the inputs `a.stride()`.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_trace(as_strided_trace)\n    w1d2_test.test_trace_transpose(as_strided_trace)\n    w1d2_test.test_trace_expand(as_strided_trace)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Matrix Multiplication\n\nImplement the following functions.\n\nThe implementation of `as_strided_matmul` should do the same thing as `einsum_matmul`, but using only as_strided, product, and sum operations. This is hard, but fun!\n\n<details>\n\n<summary>Hint 1 for as_strided_matmul: description of algorithm</summary>\n\n$$A = \\begin{bmatrix}r_1 \\\\ r_2 \\end{bmatrix} \\;\\;\\;\\;\nB = \\begin{bmatrix}c_1 && c_2 && c_3 \\end{bmatrix}$$\n\nWhere $r_1$ and $r_2$ are row vectors and $c_1$, $c_2$, and $c_3$ are column vectors. All of these vectors should have the same length.\n\n$$A_{repeated} = \\begin{bmatrix}\n\tr_1 && r_1 && r_1 \\\\\n\tr_2 && r_2 && r_2 \\\\\n\\end{bmatrix}$$\n$$B_{repeated} = \\begin{bmatrix}\n\tc_1 && c_2 && c_3 \\\\\n\tc_1 && c_2 && c_3 \\\\\n\\end{bmatrix}$$\n\n$A_{repeated}$ and $B_{repeated}$ are both 3-tensors. You can imagine the $r$ and $c$ vectors as \"going into the page\" if that's helpful.\n\nThese vectors are now lined up to do elementwise dot products to get the product of the original matrices.\n\n$$A \\times B = \\begin{bmatrix}\n\tr_1 \\cdot c_1 && r_1 \\cdot c_2 && r_1 \\cdot c_3 \\\\\n\tr_2 \\cdot c_1 && r_2 \\cdot c_2 && r_2 \\cdot c_3 \\\\\n\\end{bmatrix}$$\n\n</details>\n\n<details>\n<summary>Hint 2 for as_strided_matmul: using as_strided for repititions</summary>\nYou can use a stride of 0 in as_strided to repeat along that dimension.\n</details>\n\n<details>\n<summary>Hint 3 for as_strided_matmul: Reading from non-contiguous tensors</summary>\nIt's possible that the input matrices you recieve could themselves be the output of an as_strided operation, so that they're represented in memory in a non-contiguous way.\nMake sure that your as_strided operation is using the strides from the original input matrix on the dimensions that aren't being repeated.\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def einsum_matmul(a: t.Tensor, b: t.Tensor) -> t.Tensor:\n    \"\"\"Matrix multiply 2D matrices a and b (same as a @ b).\"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_matmul(einsum_matmul)\n    w1d2_test.test_matmul_transpose(einsum_matmul)\n    w1d2_test.test_matmul_expand(einsum_matmul)\n    w1d2_test.test_matmul_skip(einsum_matmul)\n\n\ndef as_strided_matmul(a: t.Tensor, b: t.Tensor) -> t.Tensor:\n    \"\"\"Matrix multiply 2D matrices a and b (same as a @ b), but use as_strided this time.\n\n    Use elementwise multiplication and sum.\n\n    Tip: the stride argument to `as_strided` must account for the stride of the inputs `a.stride()` and `b.stride()`.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_matmul(as_strided_matmul)\n    w1d2_test.test_matmul_transpose(as_strided_matmul)\n    w1d2_test.test_matmul_expand(as_strided_matmul)\n    w1d2_test.test_matmul_skip(as_strided_matmul)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### PyTorch's `conv1d` Exercises\n\nPyTorch's `conv1d` operates on 3D tensors, and it's easy to get confused on the shapes involved. To practice, for each test, `reshape` the variable `fix_me` so that the output of the `torch_conv1d` call matches the expected tensor.\n\nFrom the docs:\n- The \"input\" arg to conv1d should be shape (batch_size, in_channels, in_len)\n- The \"weight\" arg should be shape (out_channels, in_channels, kernel_width)\n- The output of this function will be shape (batch_size, out_channels, out_len), where out_len will be in_len - kernel_width + 1 (if stride is 1)\n\nIn these problems, we'll give you two of these three tensors. You can infer the correct shape for the other one, and reshape it accordingly.\n\nReminder: when there are multiple output channels, each output channel is computed independently as a function of _all_ the input channels.\n\n<details>\n\n<summary>Solution to A</summary>\n\nThe input is (batch=1, in_channels=1, width=4).\nThe output should be (batch=1, out_channels=1, out_width=3).\n\nThe weights should be size (out_channels, in_channels, kernel_width), i.e. (1, 1, 2)\n\n\n</details>\n\n<details>\n\n<summary>Solution to B</summary>\n\nThis is a convolution with a width-1 kernel that spans both input channels, producing 1 output channel\n\nThe weights have (out_channels=1, in_channels=2, kernel_width=1)\nThe output has (batch=2, out_channels=1, out_width=3)\n\nFor the input shape, use `out_width = width - kernel_width + 1` to solve `width=3`. So it should be (2, 2, 3)\n\n</details>\n\n<details>\n\n<summary>Solution to C</summary>\n\nThis is a convolution with two filters (i.e. two output channels), each of which operate independently on the single input channel.\n\nThe weights have (out_channels=2, in_channels=1, kernel_width=1)\nThe output has (batch=6, out_channels=2, out_width=1)\n\nFor the input shape, use `out_width = width - kernel_width + 1` to solve `width=1`. So it should be shape (6, 1, 1)\n</details>\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    input = t.tensor([[[1, 2, 3, 4]]])\n    fix_me = t.tensor([1, 3])\n    actual = torch_conv1d(input, fix_me)\n    expected = t.tensor([7.0, 11.0, 15.0])\n    utils.test_is_equal(actual, expected, \"w1d2_test.conv1d_a\")\nif MAIN:\n    fix_me = t.arange(12)\n    weights = t.tensor([[[1], [-1]]])\n    actual = torch_conv1d(fix_me, weights)\n    expected = t.tensor([[[-3, -3, -3]], [[-3, -3, -3]]])\n    utils.test_is_equal(actual, expected, \"w1d2_test.conv1d_b\")\nif MAIN:\n    fix_me = t.arange(6)\n    weights = t.tensor([[[-1]], [[1]]])\n    actual = torch_conv1d(fix_me, weights)\n    expected = t.tensor([[[0], [0]], [[-1], [1]], [[-2], [2]], [[-3], [3]], [[-4], [4]], [[-5], [5]]])\n    utils.test_is_equal(actual, expected, \"w1d2_test.conv1d_c\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Your Own `conv1d`\n\nNow implement your own minimal (i.e. assume padding = 0 and stride = 1) version of `conv1d`. It should use an `einsum` and an `as_strided` with careful consideration of the input strides.\n\n<details>\n<summary>Hint 1 - creating strided x</summary>\nYou should use as_strided to make a strided version of x, which should have the shape (batch, input_channels, output_width, kernel_width).\n\nLike we did before, the output width can be calculated as `input_width - kernel_width + 1`.\n\nWhen you do as_strided, make sure to use appropriate stride values from x.stride().\n</details>\n\n<details>\n<summary>Hint 2 - using strided x and the weights to get the final output</summary>\nYou should use einsum on your strided version of x and the input weights to get your final output. You should contract (elementwise product and then sum) over the input channels and kernel width.\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def conv1d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n    \"\"\"Like torch's conv1d using bias=False and all other keyword arguments left at their default values.\n\n    x: shape (batch, in_channels, width)\n    weights: shape (out_channels, in_channels, kernel_width)\n\n    Returns: shape (batch, out_channels, output_width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_conv1d_minimal(conv1d_minimal)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Your Own `conv2d`\n\nNow write your own 2D version. Reminder: the 2 in 2D means the weights slide over the input in two independent dimensions. The inputs and weights, and outputs will all be 4D.\n\n<details>\n\n<summary>Hint 1 - shape of strided x</summary>\n\nThe shape of `x` strided should be: (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n\nWe can generalize our 1D output shape calculation to 2D as follows: `output_height = input_height - kernel_height + 1` and `output_width = input_width - kernel_width + 1`.\n\n</details>\n\n<details>\n\n<summary>Hint 2 - strides of strided x</summary>\n\nThe first four strides are the strides of the original `x`.\nFor the kernel_height dimension, increasing by 1 means we want to increase by 1 in the height of `x`, so it should be `x.stride()[2]` and similarly for the kernel_width dimension.\n</details>\n\n<details>\n<summary>Hint 3 - final einsum</summary>\nThe einsum should contract (elementwise product and then sum) over the input channels, kernel height, and kernel width dimensions.\n</details>\n\n<details>\n<summary>Why does my version give slightly different output when using float32 compared to PyTorch's version?</summary>\n\nIn floating point, addition is not associative: the sum of two `float32` is generally not representable exactly as a `float32`, so the sum has to be rounded. This means the order that the implementation performs reductions will affect the final result. For example, summing out a vector of four elements could be done from left to right like this:\n\n`((x[0] + x[1]) + x[2]) + x[3]`\n\nOr it could be done recursively like this:\n\n`(x[0] + x[1]) + (x[2] + x[3])`\n\nIt's also possible for an implementation to use an accumulator variable of higher precision, and only round at the end which produces different results.\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def conv2d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n    \"\"\"Like torch's conv2d using bias=False and all other keyword arguments left at their default values.\n\n    x: shape (batch, in_channels, height, width)\n    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n\n    Returns: shape (batch, out_channels, output_height, output_width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_conv2d_minimal(conv2d_minimal, t.float64, 1e-10)\n    w1d2_test.test_conv2d_minimal(conv2d_minimal, t.float32, 0.001)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Padding a Tensor\n\nFor `conv` and the following `maxpool` you'll need to implement `pad` helper functions. PyTorch has some very generic padding functions, but to keep things simple and build up gradually, we'll write 1D and 2D functions individually.\n\nTip: use the `new_full` method of the input tensor. This is a clean way to ensure that the output tensor is on the same device as the input.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def pad1d(x: t.Tensor, left: int, right: int, pad_value: float) -> t.Tensor:\n    \"\"\"Return a new tensor with padding applied to the edges.\n\n    x: shape (batch, in_channels, width), dtype float32\n\n    Return: shape (batch, in_channels, left + right + width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_pad1d(pad1d)\n    w1d2_test.test_pad1d_multi_channel(pad1d)\n\n\ndef pad2d(x: t.Tensor, left: int, right: int, top: int, bottom: int, pad_value: float) -> t.Tensor:\n    \"\"\"Return a new tensor with padding applied to the edges.\n\n    x: shape (batch, in_channels, height, width), dtype float32\n\n    Return: shape (batch, in_channels, top + height + bottom, left + width + right)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_pad2d(pad2d)\n    w1d2_test.test_pad2d_multi_channel(pad2d)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Padding and Stride for `conv1d`\n\nNow extend `conv1d` to handle the `stride` and `padding` arguments.\n\n`stride` is the number of input positions that the kernel slides at each step.\n`padding` is the number of zeros concatenated to each side of the input before the convolution.\n\nOutput shape should be (batch, output_channels, output_length), where output_length can be calculated as follows:\n$$\n\\text{output\\_length} = \\left\\lfloor\\frac{\\text{input\\_length} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} \\right\\rfloor + 1\n$$\n\nVerify for yourself that the forumla above simplifies to the formula we used earlier when padding is 0 and stride is 1.\n\nDocs for pytorch's conv1d can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html).\n\n<details>\n<summary>Hint - how to use the stride argument</summary>\nWhen you perform as_strided on the input, there are two output dimensions that are derived from x.stride()[-1]. One of them should use this value without modification; the other one will use this value multiplied by the stride argument.\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def conv1d(x, weights, stride: int = 1, padding: int = 0) -> t.Tensor:\n    \"\"\"Like torch's conv1d using bias=False.\n\n    x: shape (batch, in_channels, width)\n    weights: shape (out_channels, in_channels, kernel_width)\n\n    Returns: shape (batch, out_channels, output_width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_conv1d(conv1d)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Helper functions for pairs\n\nA recurring pattern in these 2d functions is allowing the user to specify either an int or a pair of ints for an argument: examples are stride and padding. We've provided some type aliases and a helper function to simplify working with these.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "IntOrPair = Union[int, tuple[int, int]]\nPair = tuple[int, int]\n\n\ndef force_pair(v: IntOrPair) -> Pair:\n    \"\"\"Convert v to a pair of int, if it isn't already.\"\"\"\n    if isinstance(v, tuple):\n        if len(v) != 2:\n            raise ValueError(v)\n        return (int(v[0]), int(v[1]))\n    elif isinstance(v, int):\n        return (v, v)\n    raise ValueError(v)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Padding and Stride for `conv2d`\n\nNote the type signature: stride and padding can be either a single number or a tuple for each sliding dimension.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def conv2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> t.Tensor:\n    \"\"\"Like torch's conv2d using bias=False\n\n    x: shape (batch, in_channels, height, width)\n    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n\n\n    Returns: shape (batch, out_channels, output_height, output_width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_conv2d(conv2d, t.float64, 1e-10)\n    w1d2_test.test_conv2d(conv2d, t.float32, 0.001)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Max Pooling\n\nA \"max pooling\" layer is similar to a convolution in that you have a window sliding over some number of dimensions. The main difference is that there's no kernel: instead of multiplying by the kernel and adding, you just take the maximum.\n\nThe way multiple channels work is also different. A convolution has some number of input and output channels, and each output channel is a function of all the input channels. There can be any number of output channels. In a pooling layer, the maximum operation is applied independently for each input channel, meaning the number of output channels is necessarily equal to the number of input channels.\n\nImplement `MaxPool2d` using `torch.as_strided` and `torch.amax` together. Your version should behave the same as the [PyTorch version](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) but only the indicated arguments need to be supported.\n\n<details>\n\n<summary>Help! My forward returns the right shape but wrong values!</summary>\n\nTry using an input with all positive values. Does it work correctly now?\n\n</details>\n\n<details>\n\n<summary>Help! I'm still confused!</summary>\n\nThe most common cause of this is a wrong padding value. You used zero for the padding value in your convolution. Here, it should be a value smaller than any valid input value, namely `float('-inf')`. A real implementation would need to care about integer tensors as well, but you can punt on this for today.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def maxpool2d(\n    x: t.Tensor, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 0\n) -> t.Tensor:\n    \"\"\"Like PyTorch's maxpool2d.\n\n    x: shape (batch, channels, height, width)\n    stride: if None, should be equal to the kernel size\n\n    Return: (batch, channels, out_height, out_width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d2_test.test_maxpool2d(maxpool2d)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Module version\n\nYou've written the functional form of max pooling - next we'll do the `nn.Module` version. A brief primer on `nn.Module` can be found [here](https://www.notion.so/Subclassing-nn-Module-878cd4a1972a4aca822830e49bbaabf4).\n\nSince this module has no learnable parameters, there's only a few steps to do:\n\n### Constructor\n\nRemember to call `super().__init__()` in all your `Module` subclasses.\n\nStore the input arguments and handle the case where stride is `None`.\n\nIt would be possible to validate (ensure `kernel_size` is positive) and standardize all the arguments here (converting ints to tuples, etc) and throw an exception if anything is wrong, which would save users some time instead of waiting until `forward()` throws an exception. We will follow PyTorch in not bothering to do this.\n\n### extra_repr\n\nOne consequence of subclassing `nn.Module` is that this method is called when you `repr()` an instance of this class, such as printing it in a REPL or notebook. It should provide a human-readable string representation of the attributes of the module that we might want to know about. In this case, these are provided as arguments to the constructor: `kernel_size`, `stride`, and `padding`. You may want to delegate this to a helper function, since you'll be writing this method for every `Module` you implement today that has arguments. Hint: you can use `getattr(object, name)` to programmatically look up `object.name`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "\"TODO: YOUR CODE HERE\"\n\n\nclass MaxPool2d(nn.Module):\n    def __init__(self, kernel_size: IntOrPair, stride: Optional[IntOrPair] = None, padding: IntOrPair = 1):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Call the functional version of maxpool2d.\"\"\"\n        pass\n\n    def extra_repr(self) -> str:\n        \"\"\"Add additional information to the string representation of this class.\"\"\"\n        pass\n\n\nif MAIN:\n    w1d2_test.test_maxpool2d_module(MaxPool2d)\n    m = MaxPool2d(3, stride=2, padding=1)\n    print(f\"Manually verify that this is an informative repr: {m}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Linear Module\n\nImplement your own `Linear` module. This is the first `Module` that has learnable weights and biases. What type should these variables be?\n\n<details>\n\n<summary>Solution</summary>\n\nIt has to be a `torch.Tensor` wrapped in a `nn.Parameter` in order for `nn.Module` to recognize it. If you forget to do this, `module.parameters()` won't include your `Parameter`, which prevents an optimizer from being able to modify it during training.\n\n</details>\n\n### Initialization\n\nFor any layer, initialization is very important for the stability of training: with a bad initialization, your model will take much longer to converge or may completely fail to learn anything. The default PyTorch behavior isn't necessarily optimal and you can often improve performance by using something more custom, but we'll follow it for today because it's simple and works decently well.\n\nEach float in the weight and bias tensors are drawn independently from the uniform distribution on the interval:\n\n$$ -\\frac{1}{\\sqrt{fan_{in}}}, \\frac{1}{\\sqrt{fan_{in}}} $$\n\nWhere $fan_{in}$ is the number of input features.\n\n<details>\n\n<summary>Help! I get the error 'RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.'</summary>\n\nYou should initialize the weight `Tensor` before you wrap it in a `Parameter`, because the `Parameter` sets `requires_grad=True` on its argument and this isn't desirable. We don't actually want to backprop through the initialization.\n\n</details>\n\nFor the `forward` method, remember that \"...\" can be used to represent unnamed dimensions in an einsum. (Docs [here](https://pytorch.org/docs/stable/generated/torch.einsum.html), search for \"Ellipsis\".)\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class Linear(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias=True):\n        \"\"\"A simple linear (technically, affine) transformation.\n\n        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n        If `bias` is False, set `self.bias` to None.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (*, in_features)\n        Return: shape (*, out_features)\n        \"\"\"\n        pass\n\n    def extra_repr(self) -> str:\n        pass\n\n\nif MAIN:\n    w1d2_test.test_linear_forward(Linear)\n    w1d2_test.test_linear_parameters(Linear)\n    w1d2_test.test_linear_no_bias(Linear)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## `Conv2d` - nn.Module version\n\nThe initialization of the Conv2d is also very important. PyTorch does the same uniform distribution, considering `in_features` to be the number of inputs contributing to each output value: `in_channels * product(kernel_size)`.\n\nRemember to use `force_pair` before using `kernel_size`, `stride`, and `padding`, which could come as ints or pairs to the constructor.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class Conv2d(t.nn.Module):\n    def __init__(\n        self, in_channels: int, out_channels: int, kernel_size: IntOrPair, stride: IntOrPair = 1, padding: IntOrPair = 0\n    ):\n        \"\"\"Same as torch.nn.Conv2d with bias=False.\n\n        Name your weight field `self.weight` for compatibility with the PyTorch version.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Apply the functional conv2d you wrote earlier.\"\"\"\n        pass\n\n    def extra_repr(self) -> str:\n        \"\"\"\"\"\"\n        pass\n\n\nif MAIN:\n    print(f\"Manually verify that this is a useful looking repr: {Conv2d(1, 2, (3, 4), padding=5)}\")\n    w1d2_test.test_conv2d_module(Conv2d)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Batch Normalization\n\nPrevious modules you've implemented have either held no tensors like MaxPool2d, or held learnable parameters like Conv2d. The `BatchNorm2d` has a third category: **buffers**.\n\nUnlike `nn.Parameter`, a buffer is not its own type and does not wrap a `Tensor`. A buffer is just a regular `Tensor` on which you've called [self.register_buffer](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer) from inside a `nn.Module`.\n\nThe reason we have a different name for this is to describe how it is treated by various machinery within PyTorch.\n\n- It is normally included in the output of `module.state_dict()`, meaning that `torch.save` and `torch.load` will serialize and deserialize it.\n- It is moved between devices when you call `model.to(device)`.\n- It IS NOT included in `module.parameters`, so optimizers won't see or modify it. Instead, your module will modify it as appropriate within `forward`.\n\n### Train and Eval Modes\n\nThis is your first implementation that needs to care about the value of `self.training`, which is set to True by default, and can be set to False by `self.eval()` or to True by `self.train()`.\n\nIn training mode, you should use the mean and variance of the batch you're on, but you should also update a stored `running_mean` and `running_var` on each call to `forward` using the \"momentum\" argument as described in the PyTorch docs. Your `running_mean` shuld be intialized as all zeros; your `running_var` should be initialized as all ones.\n\nIn eval mode, you should use the running mean and variance that you stored before (not the mean and variance from the current batch).\n\nImplement `BatchNorm2d` according to the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html). Call your learnable parameters `weight` and `bias` for consistency with PyTorch.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class BatchNorm2d(nn.Module):\n    running_mean: t.Tensor\n    \"running_mean: shape (num_features,)\"\n    running_var: t.Tensor\n    \"running_var: shape (num_features,)\"\n    num_batches_tracked: t.Tensor\n    \"num_batches_tracked: shape ()\"\n\n    def __init__(self, num_features: int, eps=1e-05, momentum=0.1):\n        \"\"\"Like nn.BatchNorm2d with track_running_stats=True and affine=True.\n\n        Name the learnable affine parameters `weight` and `bias` in that order.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Normalize each channel.\n\n        Compute the variance using `torch.var(x, unbiased=False)`\n\n        x: shape (batch, channels, height, width)\n        Return: shape (batch, channels, height, width)\n        \"\"\"\n        pass\n\n    def extra_repr(self) -> str:\n        pass\n\n\nif MAIN:\n    w1d2_test.test_batchnorm2d_module(BatchNorm2d)\n    w1d2_test.test_batchnorm2d_forward(BatchNorm2d)\n    w1d2_test.test_batchnorm2d_running_mean(BatchNorm2d)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## ReLU Module\n\nWrite the module version of ReLU using `torch.maximum`. Its constructor has no arguments, so it doesn't need an `extra_repr`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class ReLU(nn.Module):\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Sequential\n\n[torch.nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) has a number of features, but we'll just implement the basics that we need for today.\n\nNote: the base class's `repr` already recursively prints out the submodules, so you don't need to write anything in `extra_repr`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class Sequential(nn.Module):\n    def __init__(self, *modules: nn.Module):\n        \"\"\"\n        Call `self.add_module` on each provided module, giving each one a unique (within this Sequential) name.\n        Internally, this adds them to the dictionary `self._modules` in the base class, which means they'll be included in self.parameters() as desired.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Chain each module together, with the output from one feeding into the next one.\"\"\"\n        pass\n\n\nif MAIN:\n    w1d2_test.test_sequential(Sequential)\n    w1d2_test.test_sequential_forward(Sequential)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Flatten\n\nImplement your own [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html).\n\n<details>\n\n<summary>Spoiler - Which function do I use to perform the flattening?</summary>\n\n`torch.reshape` is the best choice since it has the desired behavior: returning a view when possible, otherwise a copy. You could also use `einops.rearrange` but you'd have to construct the rearrangement pattern as a string.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class Flatten(nn.Module):\n    def __init__(self, start_dim: int = 1, end_dim: int = -1) -> None:\n        pass\n\n    def forward(self, input: t.Tensor) -> t.Tensor:\n        \"\"\"Flatten out dimensions from start_dim to end_dim, inclusive of both.\n\n        Return a view if possible, otherwise a copy.\n        \"\"\"\n        pass\n\n    def extra_repr(self) -> str:\n        pass\n\n\nif MAIN:\n    w1d2_test.test_flatten(Flatten)\n    w1d2_test.test_flatten_is_view(Flatten)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## ResNet Average Pooling\n\nLet's end our collection of `Module`s with an easy one :)\n\nThe ResNet has a Linear layer with 1000 outputs at the end in order to produce classification logits for each of the 1000 classes. Any Linear needs to have a constant number of input features, but the ResNet is supposed to be compatible with arbitrary height and width, so we can't just do a pooling operation with a fixed kernel size and stride.\n\nLuckily, the simplest possible solution works decently: take the mean over the spatial dimensions. Intuitively, each position has an equal \"vote\" for what objects it can \"see\".\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class AveragePool(nn.Module):\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, channels, height, width)\n        Return: shape (batch, channels)\n        \"\"\"\n        pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n## Assembling ResNet\n\nNow we have all the building blocks we need to start assembling your own ResNet! The following diagram describes the architecture of ResNet34 - the other versions are broadly similar. Unless otherwise noted, convolutions have a kernel_size of 3x3 and a stride of 1. None of the convolutions have biases. You'll be able to infer the number of input channels by the output of the previous layer, and infer the necessary padding from the input and output shapes.\n\nExercise: there would be no advantage to enabling biases on the convolutional layers. Why?\n\n<details>\n\n<summary>Solution - Why No Biases?</summary>\n\nEvery convolution layer in this network is followed by a batch normalization layer. The first operation in the batch normalization layer is to subtract the mean of each output channel. But a convolutional bias just adds some scalar `b` to each output channel, increasing the mean by `b`. This means that for any `b` added, the batch normalization will subtract `b` to exactly negate the bias term.\n\n</details>\n\n```mermaid\ngraph TD\nsubgraph \" \"\n    subgraph ResNet34\n        Input[Input<br>] --> InConv[7x7 Conv<br>64 channels, stride 2] --> InBN[BatchNorm] --> InReLU[ReLU] --> InPool[3x3 MaxPool<br>Stride 2] --> BlockGroups[BlockGroups<br>0, 1, ..., N] --> AveragePool --> Flatten --> Linear[Linear<br/>1000 outputs] --> Out\n    end\n\n    subgraph BlockGroup\n        BGIn[Input] --> DRB[ResidualBlock<br>WITH optional part] --> RB0[ResidualBlocks<br>0, 1, ..., N<br>WITHOUT optional part] --> BGOut[Out]\n    end\n\n    subgraph ResidualBlock\n        BIn[Input] --> BConv[Strided Conv] --> BBN1[BatchNorm] --> ReLU --> BConv2[Conv] --> BBN2[BatchNorm] --> Add --> ReLu2[ReLU] --> RBOut[Out]\n                BIn --> DBConvD[OPTIONAL<br>1x1 Strided Conv] --> DBDBatchNorm[OPTIONAL<br>BatchNorm] --> Add\n    end\nend\n```\n\n\n### Residual Block\n\nImplement `ResidualBlock` by referring to the diagram. The number of channels changes from `in_feats` to `out_feats` at the first convolution in each branch.\n\n<details>\n\n<summary>I'm confused about where to apply the stride argument!</summary>\n\nThe stride only applies to first convolution in each branch. If you apply it to the second convolution in the left branch, the add won't work because you'll have shrunk the left branch twice.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class ResidualBlock(nn.Module):\n    def __init__(self, in_feats: int, out_feats: int, first_stride=1):\n        \"\"\"A single residual block with optional downsampling.\n\n        For compatibility with the pretrained model, declare the left side branch first using a `Sequential`.\n\n        If first_stride is > 1, this means the optional (conv + bn) should be present on the right branch. Declare it second using another `Sequential`.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Compute the forward pass.\n\n        x: shape (batch, in_feats, height, width)\n\n        Return: shape (batch, out_feats, height / stride, width / stride)\n\n        If no downsampling block is present, the addition should just add the left branch's output to the input.\n        \"\"\"\n        pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### BlockGroup\n\nImplement BlockGroup according to the diagram. The number of channels changes from `in_feats` to `out_feats` at the first convolution in the BlockGroup.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class BlockGroup(nn.Module):\n    def __init__(self, n_blocks: int, in_feats: int, out_feats: int, first_stride=1):\n        \"\"\"An n_blocks-long sequence of ResidualBlock where only the first block uses the provided stride.\"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Compute the forward pass.\n        x: shape (batch, in_feats, height, width)\n\n        Return: shape (batch, out_feats, height / first_stride, width / first_stride)\n        \"\"\"\n        return self.blocks(x)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### ResNet34\n\nLast step! Assemble `ResNet34` using the diagram.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class ResNet34(nn.Module):\n    def __init__(\n        self,\n        n_blocks_per_group=[3, 4, 6, 3],\n        out_features_per_group=[64, 128, 256, 512],\n        strides_per_group=[1, 2, 2, 2],\n        n_classes=1000,\n    ):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, channels, height, width)\n\n        Return: shape (batch, n_classes)\n        \"\"\"\n        pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Copying Pretrained Weights\n\nBefore we go training our ResNet from scratch, let's make sure everything is correct by copying over the weights from the `torchvision` version to yours. Call `state_dict()` on the torchvision model to retrieve its state, and then convert that into a dictionary that you can feed to your model's `load_state_dict`.\n\nYou can do this however you want, but way I prefer is to try and build a 1-1 correspondence between the order of the 218 parameters and buffers in the pretrained model and the 218 in yours. Then you can just pair the names from your model with the data from their model. If you've followed the instructions throughout for what order to initialize things in, then this should \"just work\".\n\nThis will be tedious and if you really hate this, it might be a sign that ML engineering is not for you because this is a realistic representation of an actual task. You'll be doing a very similar process for BERT and GPT-2 models later :)\n\nTip: `load_state_dict` expects an `OrderedDict`, but since Python 3.7, the builtin `dict` is guaranteed to maintain items in the order they're inserted, so you can safely use a regular `dict`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN and (not IS_CI):\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Running Your Model\n\nNow the moment of truth: call the `predict` function that you wrote way back on an instance of your own `ResNet34` class. If you've done everything correctly, your version should give the same classifications, and the percentages should match at least to a couple decimal places.\n\nIf it does, congratulations, you've now run an entire ResNet, using barely any code from `torch.nn`! The only things we used were `nn.Module` and `nn.Parameter`, and you'll reimplement those in W1D3.\n\nIf it doesn't, congratulations, you get to practice model debugging! Don't be afraid to call a TA here if you get stuck.\n\n<details>\n\n<summary>Help! My model is predicting roughly the same percentage for every category!</summary>\n\nThis can indicate that your model weights are randomly initialized, meaning the weight loading process didn't actually take. Or, you reinitialized your model by accident after loading the weights.\n\n</details>\n\n<details>\n\n<summary>Help! My model is outputting `nan`!</summary>\n\nTo debug this, find the first place where `nan` appears. One way to do this is with forward hooks. An example of using hooks is given below.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def check_nan_hook(module: nn.Module, inputs, output):\n    \"\"\"Example of a hook function that can be registered to a module.\"\"\"\n    x = inputs[0]\n    if t.isnan(x).any():\n        raise ValueError(module, x)\n    out = output[0]\n    if t.isnan(out).any():\n        raise ValueError(module, x)\n\n\ndef add_hook(module: nn.Module) -> None:\n    \"\"\"Remove any existing hooks and register our hook.\n\n    Use model.apply(add_hook) to recursively apply the hook to model and all submodules.\n    \"\"\"\n    utils.remove_hooks(module)\n    module.register_forward_hook(check_nan_hook)\n\n\nif MAIN and (not IS_CI):\n    your_model.apply(add_hook)\n    your_model_predictions = predict(your_model, images)\n    w1d2_test.test_same_predictions(your_model_predictions)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Training ResNet on CIFAR10\n\nNext, we'll train our ResNet from scratch, on the GPU.\n\nImageNet is going to take too long, so we'll use a much smaller dataset called CIFAR10, which has only 10 classes and much smaller images.\n\n### Preparing the CIFAR10 Data\n\nThe data preparation is the same as before, so we've provided it to save time. Following good practice, we'll verify that preprocessed data is roughly normalized to mean 0 and std 1, and looks reasonable visually.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "cifar_classes = {\n    0: \"airplane\",\n    1: \"automobile\",\n    2: \"bird\",\n    3: \"cat\",\n    4: \"deer\",\n    5: \"dog\",\n    6: \"frog\",\n    7: \"horse\",\n    8: \"ship\",\n    9: \"truck\",\n}\n\n\ndef get_cifar10():\n    \"\"\"Download (if necessary) and return the CIFAR10 dataset.\"\"\"\n    \"The following is a workaround for this bug: https://github.com/pytorch/vision/issues/5039\"\n    if sys.platform == \"win32\":\n        import ssl\n\n        ssl._create_default_https_context = ssl._create_unverified_context\n    \"Magic constants taken from: https://docs.ffcv.io/ffcv_examples/cifar10.html\"\n    mean = t.tensor([125.307, 122.961, 113.8575]) / 255\n    std = t.tensor([51.5865, 50.847, 51.255]) / 255\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n    cifar_train = torchvision.datasets.CIFAR10(\"w1d2_cifar10_train\", transform=transform, download=True, train=True)\n    cifar_test = torchvision.datasets.CIFAR10(\"w1d2_cifar10_train\", transform=transform, download=True, train=False)\n    return (cifar_train, cifar_test)\n\n\nif MAIN and (not IS_CI):\n    (cifar_train, cifar_test) = get_cifar10()\n    trainloader = DataLoader(cifar_train, batch_size=512, shuffle=True, pin_memory=True)\n    testloader = DataLoader(cifar_test, batch_size=512, pin_memory=True)\nif MAIN and (not IS_CI):\n    batch = next(iter(trainloader))\n    print(\"Mean value of each channel: \", batch[0].mean((0, 2, 3)))\n    print(\"Std value of each channel: \", batch[0].std((0, 2, 3)))\n    (fig, axes) = plt.subplots(ncols=5, figsize=(15, 5))\n    for (i, ax) in enumerate(axes):\n        ax.imshow(rearrange(batch[0][i], \"c h w -> h w c\"))\n        ax.set(xlabel=cifar_classes[batch[1][i].item()])\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Train your ResNet\n\nWe've provided a basic training loop which is far from optimal, but will serve for today. On my GTX 1080Ti, it took 30 seconds per epoch. On a Tesla V100, it took around 20 seconds per epoch. If you don't have access to a GPU, it's going to be pretty slow and you should reduce the number of epochs from the default of 8.\n\nYou may encounter some issues running on GPU. The most common issue is if you manually created any tensors, they could be on the wrong device. The PyTorch docs has a section on [creation ops](https://pytorch.org/docs/stable/torch.html#tensor-creation-ops) which has useful functions for dealing with this cleanly including `empty_like` and `zeros_like`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "MODEL_FILENAME = \"./w1d2_resnet34_cifar10.pt\"\ndevice = \"cuda\" if t.cuda.is_available() else \"cpu\"\n\n\ndef train(trainloader: DataLoader, epochs: int) -> ResNet34:\n    model = ResNet34(n_classes=10).to(device).train()\n    optimizer = t.optim.Adam(model.parameters())\n    loss_fn = t.nn.CrossEntropyLoss()\n    for epoch in range(epochs):\n        for (i, (x, y)) in enumerate(tqdm(trainloader)):\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            y_hat = model(x)\n            loss = loss_fn(y_hat, y)\n            loss.backward()\n            optimizer.step()\n        print(f\"Epoch {epoch}, train loss is {loss}\")\n        print(f\"Saving model to: {os.path.abspath(MODEL_FILENAME)}\")\n        t.save(model, MODEL_FILENAME)\n    return model\n\n\nif MAIN and (not IS_CI):\n    if os.path.exists(MODEL_FILENAME):\n        print(\"Loading model from disk: \", MODEL_FILENAME)\n        model = t.load(MODEL_FILENAME)\n    else:\n        print(\"Training model from scratch\")\n        model = train(trainloader, epochs=8)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Test Your ResNet\n\nAfter one epoch, my ResNet achieved 54% accuracy (random guessing would be 10%), train loss of 1.2 and test loss of 1.3.\nAfter eight epochs, it achieved 68% accuracy, train loss of 0.4 and test loss of 1.1. This generalization gap means the model is overfitting. By the end of the course you'll be able to do better than this.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN and (not IS_CI):\n    model.eval()\n    model.apply(add_hook)\n    loss_fn = t.nn.CrossEntropyLoss(reduction=\"sum\")\n    with t.inference_mode():\n        n_correct = 0\n        n_total = 0\n        loss_total = 0.0\n        for (i, (x, y)) in enumerate(tqdm(testloader)):\n            x = x.to(device)\n            y = y.to(device)\n            with t.autocast(device):\n                y_hat = model(x)\n                loss_total += loss_fn(y_hat, y).item()\n            n_correct += (y_hat.argmax(dim=-1) == y).sum().item()\n            n_total += len(x)\n    print(f\"Test accuracy: {n_correct} / {n_total} = {100 * n_correct / n_total:.2f}%\")\n    print(f\"Test loss: {loss_total / n_total}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Bonus\n\nCongratulations on completing the day's main content! All of the bonus exercises going forward are completely optional, can be done in any order, and nothing later in the course will depend on you having completed them. If you feel uncertain or confused about any of the day's material, it's probably a better idea to just go back and solidify your understanding before tackling these.\n\n### Fused BatchNorm and Conv2d\n\nAfter a model is trained, it's possible to merge adjacent BatchNorm and Conv2d layers into one operation for a speedup. Try doing this and see how much improvement you get. If you need a hint, [this blog](https://nenadmarkus.com/p/fusing-batchnorm-and-conv/) has the equations followed by solution code.\n\n### Deeper Look at Initialization\n\nWe did a pretty basic initialization today. Read up on Xavier and Kaiming initializations, implement one or both, and try some experiments to see if they actually work better on CIFAR10 than the default PyTorch behavior.\n\n### Residual Block Identity Initialization\n\nThe ResNet from `torchvision` has a `zero_init_residual` argument. Investigate the source code in `torchvision/models/resnet.py` and see what this does, then replicate it.\n\n\n### Data Augmentation\n\nOne way to reduce overfitting is ensure that the network doesn't see the exact same image more than once. Play with some of the transforms in the torchvision library like random crops and rotations and see if you can reduce the generalization gap.\n\n### ReLU Benchmarking\n\nPredict which of the ReLU functions is fastest on massive tensors, then benchmark them on CPU and GPU. Why do you think some are faster than others?\n\n### Negative Strides\n\nIn NumPy, an `ndarray` can have a negative stride which means iterating backward through the underlying storage. For example: `np.flip(np.arange(10))`. What happens when you do the same thing in PyTorch?\n\n<details>\n\n<summary>Solution - Negative Strides In PyTorch</summary>\n\nAt least in PyTorch 1.11, `torch.flip` makes a copy instead of a view, and negative strides in `as_strided` throw an error. If you are trying to port code that uses `np.flip` to PyTorch, you will probably have to rewrite those calls otherwise it'll be unexpectedly expensive.\n\nSupporting negative strides in at least some places is an [open issue](https://github.com/pytorch/pytorch/issues/16424) and if the idea of contributing to PyTorch sounds fun to you, this could be a big adventure.\n\n</details>\n\n### Adversarial Examples\n\nAdversarial examples in this context are images that to a human clearly and obviously belong to one class, but the classifier gives a confidently wrong prediction. Either find existing adversarial examples on the Internet and test them on the various sizes of ResNet, or try to create your own adversarial examples that fool your ResNet. You may find the paper [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175.pdf) to be thought-provoking.\n\n### Build Your Own Dtype Support\n\nPyTorch has both integer and floating point datatypes. Go through your modules from today and see if your code does the same thing as PyTorch when different datatypes are involved, and fix your code to behave appropriately.\n\n### Benchmarking Inference Mode\n\nResearch what exactly inference mode does, and see if you can detect a difference in speed or memory consumption from using or not using it.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}