{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D3 - RL with PPO algorithm\n\nToday you'll be implementing the Proximal Policy Gradient algorithm!\n\n## Table of Contents\n\n- [Readings](#readings)\n- [On-Policy vs Off-Policy](#on-policy-vs-off-policy)\n- [Actor-Critic Methods](#actor-critic-methods)\n- [Learning Objectives](#learning-objectives)\n- [References (not required reading)](#references-not-required-reading)\n- [Actor-Critic Agent Implementation (detail #2)](#actor-critic-agent-implementation-detail-)\n- [Generalized Advantage Estimation (detail #5)](#generalized-advantage-estimation-detail-)\n- [Minibatch Update (detail #6)](#minibatch-update-detail-)\n- [Loss Function](#loss-function)\n    - [Gradient Ascent](#gradient-ascent)\n    - [Clipped Surrogate Loss](#clipped-surrogate-loss)\n    - [Minibatch Advantage Normalization (detail #7)](#minibatch-advantage-normalization-detail-)\n    - [Value Function Loss (detail #9)](#value-function-loss-detail-)\n    - [Entropy Bonus (detail #10)](#entropy-bonus-detail-)\n- [Entropy Diagnostic](#entropy-diagnostic)\n- [Adam Optimizer and Scheduler (details #3 and #4)](#adam-optimizer-and-scheduler-details--and-)\n- [Putting It All Together](#putting-it-all-together)\n- [Debug Variables (detail #12)](#debug-variables-detail-)\n    - [Update Frequency](#update-frequency)\n- [Bonus](#bonus)\n    - [Continuous Action Spaces](#continuous-action-spaces)\n    - [Vectorized Advantage Calculation](#vectorized-advantage-calculation)\n\n## Readings\n\n- [Spinning Up in Deep RL - PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html) - you don't need to follow all the derivations, but try to have a qualitative understanding of what all the symbols represent.\n- [Spinning Up in Deep RL - Vanilla Policy Gradient](https://spinningup.openai.com/en/latest/algorithms/vpg.html#background) - PPO is a fancier version of vanilla policy gradient, so if you're struggling to understand PPO it may help to look at the simpler setting first.\n- [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/#solving-pong-in-5-minutes-with-ppo--envpool) - the good news is that you won't need all 37 of these today, so no need to read to the end. We will be tackling the 13 \"core\" details, not in the same order as presented here.\n- [Andy Jones - Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html) - you've already read this for W3D2 but it will come in handy again. You'll want to reuse your probe environments from yesterday, or you can import them from the solution if you didn't implement them all.\n\nI would recommend making a physical checklist of the 13 items and marking them as you go with how confident you are in your implementation. If things aren't working, this will help you notice if you missed one, or focus on the sections most likely to be bugged.\n\n## On-Policy vs Off-Policy\n\nBroadly, RL algorithms can be categorized as off-policy or on-policy. DQN learns from a replay buffer of old experiences that could have been generated by an old policy quite different than the current one. This means it is off-policy.\n\nPPO will only learn from experiences that were generated by the current policy, which is why it's called on-policy. We will generate batch of experiences, train on them once, and then discard them.\n\n## Actor-Critic Methods\n\nIn DQN, there was no neural network representing the policy; the policy was \"sometimes act randomly, otherwise take the action with max q-value\". In PPO, we're going to have two neural networks:\n\n- The actor network takes observations and outputs logits, which we can normalize into a probability distribution and sample from to determine our action.\n- The critic network takes observations and outputs a predicted value for that observation. Again, we're going to equivocate between states and observations as is tradition. The point is that it doesn't have to output a value for every possible action, just the value assuming we did the optimal action. The critic is called a critic because like a movie critic, it just watches what's happening without taking any actions and forms an opinion on whether states are good or bad.\n\n## Learning Objectives\n\nYour implementation might get huge benchmark scores by the end of the day, but don't worry if it actually just doesn't work at all on the simplest of tasks. RL can be frustrating because the feedback you get is extremely noisy and random. The agent can fail even when the code is correct and the agent can succeed even when the code is buggy. Forming a systematic process for coping with the confusion and uncertainty is the point of today, more so than producing a working PPO implementation.\n\nSome parts of your process could include:\n\n- Forming hypotheses about why it isn't working, and thinking about what tests you could write, Gym environments that would test the hypothesis, or where you could set a breakpoint.\n- Getting a sense for the meaning of various logged metrics, and what this implies about the training process\n- Noticing confusion and sections that don't make sense, and investigating this instead of hand-waving over it.\n\n## References (not required reading)\n\n- [The Policy of Truth](http://www.argmin.net/2018/02/20/reinforce/) - a contrarian take on why Policy Gradients are actually a \"terrible algorithm\" that is \"legitimately bad\" and \"never a good idea\".\n- [Tricks from Deep RL Bootcamp at UC Berkeley](https://github.com/williamFalcon/DeepRLHacks/blob/master/README.md) - more debugging tips that may be of use.\n- [What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study](https://arxiv.org/pdf/2006.05990.pdf) - Google Brain researchers trained over 250K agents to figure out what really affects performance. The answers may surprise you.\n- [Lilian Weng Blog](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/#ppo)\n- [A Closer Look At Deep Policy Gradients](https://arxiv.org/pdf/1811.02553.pdf)\n- [Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods](https://arxiv.org/pdf/1810.02525.pdf)\n- [Independent Policy Gradient Methods for Competitive Reinforcement Learning](https://papers.nips.cc/paper/2020/file/3b2acfe2e38102074656ed938abf4ac3-Supplemental.pdf) - requirements for multi-agent Policy Gradient to converge to Nash equilibrium.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import argparse\nimport dataclasses\nimport os\nimport random\nimport sys\nimport time\nfrom dataclasses import dataclass\nfrom distutils.util import strtobool\nfrom typing import Optional\nimport gym\nimport numpy as np\nimport torch\nimport torch as t\nimport torch.nn as nn\nimport torch.optim as optim\nfrom einops import rearrange\nfrom gym.spaces import Discrete\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.tensorboard import SummaryWriter\nimport utils\nimport w3d2_part2_dqn_solution\nimport w3d3_test\nfrom w3d2_utils import make_env\n\nMAIN = __name__ == \"__main__\"\nIS_CI = os.getenv(\"IS_CI\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Actor-Critic Agent Implementation (detail #2)\n\nImplement the `Agent` class according to the diagram, inspecting `envs` to determine the observation shape and number of actions. We are doing separate Actor and Critic networks because detail #13 notes that is performs better than a single shared network in simple environments. Note that today `envs` will actually have multiple instances of the environment inside, unlike yesterday's DQN which had only one instance inside.\n\nUse `layer_init` to initialize each `Linear`, overriding the norm of the rows according to the diagram. What is the benefit of using a small norm for the last actor layer?\n\n\n```mermaid\n\ngraph TD\n    subgraph Critic\n        Linear1[\"Linear(obs_shape, 64)\"] --> Tanh1[Tanh] --> Linear2[\"Linear(64, 64)\"] --> Tanh2[Tanh] --> Linear3[\"Linear(64, 1)<br/>row_norm=1\"] --> Out\n\n    end\n    subgraph Actor\n        ALinear1[\"Linear(obs_shape, 64)\"] --> ATanh1[Tanh]--> ALinear2[\"Linear(64, 64)\"] --> ATanh2[\"Tanh\"] --> ALinear3[\"Linear(64, num_actions)<br/>row_norm=0.01\"] --> AOut[Out]\n    end\n```\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def layer_init(layer: nn.Linear, row_norm=np.sqrt(2), bias_const=0.0) -> nn.Linear:\n    \"\"\"Initialize the provided linear layer.\n\n    - Each row of the weight has the specified norm\n    - Each element of the bias is bias_const.\n    \"\"\"\n    torch.nn.init.orthogonal_(layer.weight, row_norm)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer\n\n\nclass Agent(nn.Module):\n    critic: nn.Sequential\n    actor: nn.Sequential\n\n    def __init__(self, envs: gym.vector.SyncVectorEnv):\n        pass\n\n\nif MAIN:\n    w3d3_test.test_agent_init(Agent)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Generalized Advantage Estimation (detail #5)\n\nThere are various ways to compute advantages - follow detail #5 closely for today.\n\nAt the point where this is called we've already run some number of environments `n_envs` for some number of steps `t`: this is called the rollout phase. Now it's time to compute the advantages so we can use them in the loss function.\n\nGiven a batch of experiences, we want to compute each advantage[t][env]. This is equations 11 and 12 of the [PPO paper](https://arxiv.org/pdf/1707.06347.pdf), reproduced here:\n\n> $\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + (\\gamma\\lambda)^3\\delta_{t+3} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1}$,\n>\n> where $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$\n\n<details>\n<summary>Understanding GAE</summary>\n\nWe can break down the [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions) $V(s_t)$ as follows (where $R$ is the reward function, so the reward at timestep $t = r_t = R(s_t)$):\n$$V(s_t) = E[R(s_t) + \\gamma R(s_{t+1}) + \\gamma^2 R(s_{t+2}) + ...] = E[R(s_t)] + \\gamma V(s_{t+1})$$\n\nWe can then use this to replace $V(s_t)$ in the equation for $\\delta_t$:\n\n$$\n\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n= r_t + \\gamma V(s_{t+1}) - E[R(s_t)] - \\gamma V(s_{t+1})\n= r_t - E[R(s_t)]\n$$\n\n**So $\\delta_t$ is just how much higher the received reward was at timestep $t$ than we would have expected.**\n\nWhen $\\lambda = 1$, the value of $\\hat{A}_t$ is just the values of $\\delta$ at the various timesteps summed using the normal discount:\n$$\n\\lambda = 1: \\hat{A}_t = \\delta_t + \\gamma\\delta_{t+1} + \\gamma^2\\delta_{t+2} + ... + \\gamma^{T-t+1}\\delta_{T-1}\n = R(\\tau) - E[R(\\tau)]\n$$\n\nWhich is just how much better the actual return was than the expected return for this trajectory.\n\nWhen $\\lambda \\lt 1$, the exponential discount of future rewards comes slightly faster which helps to reduce the variance of the estimate. More info about this can be found on page 15 of [this PDF](https://arxiv.org/pdf/1804.02717.pdf), under the section titled MULTI-STEP RETURNS at the start of the page.\n</details>\n\nImplement `compute_advantages`. I recommend using a reversed for loop over `t` to get it working, and not worrying about trying to completely vectorize it. Also note that in the implementation in the solutions, a value of 1 at position `t` in `dones` indicates that `t` is the *first* timestep of a new trajectory (not the last timestep of the previous trajectory).\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@torch.no_grad()\ndef compute_advantages(\n    next_value: t.Tensor,\n    next_done: t.Tensor,\n    rewards: t.Tensor,\n    values: t.Tensor,\n    dones: t.Tensor,\n    device: t.device,\n    gamma: float,\n    gae_lambda: float,\n) -> t.Tensor:\n    \"\"\"Compute advantages using Generalized Advantage Estimation.\n\n    next_value: shape (1, n_envs) - represents V(s_{t+1}) which is needed for the last advantage term\n    next_done: shape (n_envs,)\n    rewards: shape (t, n_envs)\n    values: shape (t, n_envs)\n    dones: shape (t, n_envs)\n\n    Return: shape (t, n_envs)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d3_test.test_compute_advantages(compute_advantages)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Minibatch Update (detail #6)\n\nAfter generating our experiences that have `(t, n_envs)` dimensions, we need to:\n\n- Flatten the `(t, n_envs)` dimensions into one batch dimension\n- Split the batch into minibatches, so we can take an optimizer step for each minibatch.\n\nIf we just randomly sampled the minibatch each time, some of our experiences might not appear in any minibatch due to random chance. This would be wasteful - we're going to discard all these experiences immediately after training, so there's no second chance for the experience to be used, unlike if it was in a replay buffer.\n\nImplement the following functions so that each experience appears exactly once.\n\nTip: `Minibatch` stores the returns, which are just advantages + values.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@dataclass\nclass Minibatch:\n    obs: t.Tensor\n    logprobs: t.Tensor\n    actions: t.Tensor\n    advantages: t.Tensor\n    returns: t.Tensor\n    values: t.Tensor\n\n\ndef minibatch_indexes(batch_size: int, minibatch_size: int) -> list[np.ndarray]:\n    \"\"\"Return a list of length (batch_size // minibatch_size) where each element is an array of indexes into the batch.\n\n    Each index should appear exactly once.\n    \"\"\"\n    assert batch_size % minibatch_size == 0\n    pass\n\n\ndef make_minibatches(\n    obs: t.Tensor,\n    logprobs: t.Tensor,\n    actions: t.Tensor,\n    advantages: t.Tensor,\n    values: t.Tensor,\n    obs_shape: tuple,\n    action_shape: tuple,\n    batch_size: int,\n    minibatch_size: int,\n) -> list[Minibatch]:\n    \"\"\"\n    Flatten the n_envs and steps dimensions into one batch dimension, then shuffle and split into minibatches.\n\n    obs: shape (t, n_envs, *observation_shape)\n    logprobs: shape (t, n_envs)\n    actions: shape (t, n_envs, *action_shape)\n    advantages: shape (t, n_envs)\n    values: shape (t, n_envs)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d3_test.test_minibatch_indexes(minibatch_indexes)\n    w3d3_test.test_make_minibatches(make_minibatches)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Loss Function\n\nThe overall loss function is given by Eq 9 in the paper and is the sum of three terms - we'll implement each term individually.\n\n### Gradient Ascent\n\nEq 9 is presented for gradient ascent, which I find confusing since we've always done gradient descent to this point.\n\nYou can actually configure Adam to do gradient ascent by passing `maximize=True`, but I've chosen to use gradient descent as usual and flip the signs of the objective as needed.\n\n### Clipped Surrogate Loss\n\nFor each minibatch, calculate $L^{CLIP}$ from Eq 7 in the paper. This will allow us to improve the parameters of our actor.\n\nTip: we want to maximize $L^{CLIP}$, so for gradient descent the loss returned needs to be negative of the equation.\n\nTip: In the paper, don't confuse $r_{t}$ which is reward at time $t$ with $r_{t}(\\theta)$, which is the probability ratio between the current policy (output of the actor) and the old policy (stored in mb_logprobs).\n\n### Minibatch Advantage Normalization (detail #7)\n\nRemember to normalize the minibatch of advantages before using it.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def calc_policy_loss(\n    probs: Categorical, mb_action: t.Tensor, mb_advantages: t.Tensor, mb_logprobs: t.Tensor, clip_coef: float\n) -> t.Tensor:\n    \"\"\"Return the negative policy loss, suitable for minimization with gradient descent.\n\n    probs: a torch Categorical distribution containing the actor's unnormalized logits of shape (minibatch_size, n_actions)\n    mb_action: shape (minibatch_size, *action_shape)\n    mb_advantages: shape (minibatch_size,)\n    mb_logprobs: shape (minibatch_size,)\n    clip_coef: amount of clipping, denoted by epsilon in Eq 7.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d3_test.test_calc_policy_loss(calc_policy_loss)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Value Function Loss (detail #9)\n\nThe value function loss lets us improve the parameters of our critic. Today we're going to implement the simple form: this is just 1/2 the mean squared difference between the critic's prediction and the observed returns. We're defining returns as `returns = advantages + values`.\n\nThe PPO paper did a more complicated thing with clipping, but we're going to deviate from the paper and NOT clip, since detail #9 gives evidence that it isn't beneficial.\n\nImplement `calc_value_function_loss` which returns the term denoted $c_1 L_t^{VF}$ in Eq 9.\n\nExercise: what should the sign be on the return value from this function?\n\n<details>\n\n<summary>Solution - sign of value loss</summary>\n\nWe want to minimize the difference between predicted and observed, so this term should be always non-negative.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def calc_value_function_loss(critic: nn.Sequential, mb_obs: t.Tensor, mb_returns: t.Tensor, v_coef: float) -> t.Tensor:\n    \"\"\"Compute the value function portion of the loss function.\n\n    v_coef: the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d3_test.test_calc_value_function_loss(calc_value_function_loss)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Entropy Bonus (detail #10)\n\nThe entropy bonus term is intended to incentivize exploration by increasing the entropy of the actions distribution. For a discrete probability distribution, entropy is just the sum over x of $-p(x) log(p(x))$.\n\nYou should understand what entropy of a discrete distribution means, but you don't have to implement it yourself: `probs.entropy` computes it using the above formula but in a numerically stable way.\n\nExercise: in CartPole, what are the minimum and maximum values that entropy can take? What behaviors correspond to each of these cases?\n\n<details>\n\n<summary>Solution - CartPole Entropy</summary>\n\nThe minimum entropy is zero, under the policy \"always move left\" or \"always move right\".\n\nThe maximum entropy is $log(2) \\approx 0.693$ under the uniform random policy over the 2 actions.\n\n</details>\n\n## Entropy Diagnostic\n\nSeparately from its role in the loss function, the entropy of our action distribution is a useful diagnostic to have: if the entropy of agent's actions is near the maximum, it's playing nearly randomly which means it isn't learning anything (assuming the optimal policy isn't random). If it is near the minimum especially early in training, then the agent might not be exploring enough.\n\nImplement `calc_entropy_loss`.\n\nTip: make sure the sign is correct; for gradient descent, to actually increase entropy this term needs to be negative.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def calc_entropy_loss(probs: Categorical, ent_coef: float):\n    \"\"\"Return the entropy loss term.\n\n    ent_coef: the coefficient for the entropy loss, which weights its contribution to the overall loss. Denoted by c_2 in the paper.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d3_test.test_calc_entropy_loss(calc_entropy_loss)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Adam Optimizer and Scheduler (details #3 and #4)\n\nEven though Adam is already an adaptive learning rate optimizer, empirically it's still beneficial to decay the learning rate.\n\nImplement a linear decay from `initial_lr` to `end_lr` over num_updates steps.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class PPOScheduler:\n    def __init__(self, optimizer, initial_lr: float, end_lr: float, num_updates: int):\n        self.optimizer = optimizer\n        self.initial_lr = initial_lr\n        self.end_lr = end_lr\n        self.num_updates = num_updates\n        self.n_step_calls = 0\n\n    def step(self):\n        \"\"\"Implement linear learning rate decay so that after num_updates calls to step, the learning rate is end_lr.\"\"\"\n        pass\n\n\ndef make_optimizer(agent: Agent, num_updates: int, initial_lr: float, end_lr: float) -> tuple[optim.Adam, PPOScheduler]:\n    \"\"\"Return an appropriately configured Adam with its attached scheduler.\"\"\"\n    pass\n\n\nif MAIN:\n    w3d3_test.test_ppo_scheduler(PPOScheduler)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Putting It All Together\n\nAgain, we've provided the boilerplate for you. It looks worse than it is - a lot of it is just tracking metrics for debugging. Implement the sections marked with placeholders.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@dataclass\nclass PPOArgs:\n    exp_name: str = os.path.basename(globals().get(\"__file__\", \"PPO_implementation\").rstrip(\".py\"))\n    seed: int = 1\n    torch_deterministic: bool = True\n    cuda: bool = True\n    track: bool = False\n    wandb_project_name: str = \"mlab2_ppo\"\n    wandb_entity: Optional[str] = None\n    capture_video: bool = False\n    env_id: str = \"CartPole-v1\"\n    total_timesteps: int = 500000\n    learning_rate: float = 0.00025\n    num_envs: int = 4\n    num_steps: int = 128\n    gamma: float = 0.99\n    gae_lambda: float = 0.95\n    num_minibatches: int = 4\n    update_epochs: int = 4\n    clip_coef: float = 0.2\n    ent_coef: float = 0.01\n    vf_coef: float = 0.5\n    max_grad_norm: float = 0.5\n\n    def __post_init__(self):\n        self.batch_size: int = int(self.num_envs * self.num_steps)\n        self.minibatch_size: int = int(self.batch_size // self.num_minibatches)\n\n\narg_help_strings = {\n    \"exp_name\": \"the name of this experiment\",\n    \"seed\": \"seed of the experiment\",\n    \"torch_deterministic\": \"if toggled, `torch.backends.cudnn.deterministic=False`\",\n    \"cuda\": \"if toggled, cuda will be enabled by default\",\n    \"track\": \"if toggled, this experiment will be tracked with Weights and Biases\",\n    \"wandb_project_name\": \"the wandb's project name\",\n    \"wandb_entity\": \"the entity (team) of wandb's project\",\n    \"capture_video\": \"whether to capture videos of the agent performances (check out `videos` folder)\",\n    \"env_id\": \"the id of the environment\",\n    \"total_timesteps\": \"total timesteps of the experiments\",\n    \"learning_rate\": \"the learning rate of the optimizer\",\n    \"num_envs\": \"the number of parallel game environments\",\n    \"num_steps\": \"the number of steps to run in each environment per policy rollout\",\n    \"gamma\": \"the discount factor gamma\",\n    \"gae_lambda\": \"the lambda for the general advantage estimation\",\n    \"num_minibatches\": \"the number of mini-batches\",\n    \"update_epochs\": \"the K epochs to update the policy\",\n    \"clip_coef\": \"the surrogate clipping coefficient\",\n    \"ent_coef\": \"coefficient of the entropy\",\n    \"vf_coef\": \"coefficient of the value function\",\n    \"max_grad_norm\": \"the maximum norm for the gradient clipping\",\n}\ntoggles = [\"torch_deterministic\", \"cuda\", \"track\", \"capture_video\"]\n\n\ndef parse_args(arg_help_strings=arg_help_strings, toggles=toggles) -> PPOArgs:\n    parser = argparse.ArgumentParser()\n    for field in dataclasses.fields(PPOArgs):\n        flag = \"--\" + field.name.replace(\"_\", \"-\")\n        type_function = field.type if field.type != bool else lambda x: bool(strtobool(x))\n        toggle_kwargs = {\"nargs\": \"?\", \"const\": True} if field.name in toggles else {}\n        parser.add_argument(\n            flag, type=type_function, default=field.default, help=arg_help_strings[field.name], **toggle_kwargs\n        )\n    return PPOArgs(**vars(parser.parse_args()))\n\n\ndef train_ppo(args: PPOArgs) -> Agent:\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n    )\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n    )\n    action_shape = envs.single_action_space.shape\n    assert action_shape is not None\n    assert isinstance(envs.single_action_space, Discrete), \"only discrete action space is supported\"\n    agent = Agent(envs).to(device)\n    num_updates = args.total_timesteps // args.batch_size\n    (optimizer, scheduler) = make_optimizer(agent, num_updates, args.learning_rate, 0.0)\n    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, args.num_envs) + action_shape).to(device)\n    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    global_step = 0\n    old_approx_kl = 0.0\n    approx_kl = 0.0\n    value_loss = t.tensor(0.0)\n    policy_loss = t.tensor(0.0)\n    entropy_loss = t.tensor(0.0)\n    clipfracs = []\n    info = []\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n    num_episodes = 0\n    for _ in range(num_updates):\n        for i in range(0, args.num_steps):\n            \"YOUR CODE: Rollout phase (see detail #1)\"\n            for item in info:\n                if \"episode\" in item.keys():\n                    num_episodes += 1\n                    if num_episodes % 100 == 0:\n                        print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n                    break\n        next_value = rearrange(agent.critic(next_obs), \"env 1 -> 1 env\")\n        advantages = compute_advantages(\n            next_value, next_done, rewards, values, dones, device, args.gamma, args.gae_lambda\n        )\n        clipfracs.clear()\n        for _ in range(args.update_epochs):\n            minibatches = make_minibatches(\n                obs,\n                logprobs,\n                actions,\n                advantages,\n                values,\n                envs.single_observation_space.shape,\n                action_shape,\n                args.batch_size,\n                args.minibatch_size,\n            )\n            for mb in minibatches:\n                \"YOUR CODE: compute loss on the minibatch and step the optimizer (not the scheduler). Do detail #11 (global gradient clipping) here using nn.utils.clip_grad_norm_.\"\n        scheduler.step()\n        (y_pred, y_true) = (mb.values.cpu().numpy(), mb.returns.cpu().numpy())\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n        with torch.no_grad():\n            newlogprob: t.Tensor = probs.log_prob(mb.actions)\n            logratio = newlogprob - mb.logprobs\n            ratio = logratio.exp()\n            old_approx_kl = (-logratio).mean().item()\n            approx_kl = (ratio - 1 - logratio).mean().item()\n            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl, global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n        if global_step % 10 == 0:\n            print(\"steps per second (SPS):\", int(global_step / (time.time() - start_time)))\n    envs.close()\n    writer.close()\n    return agent\n\n\nif MAIN and (not IS_CI):\n    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n        filename = globals().get(\"__file__\", \"<filename of this script>\")\n        print(f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\")\n        args = PPOArgs()\n    else:\n        args = parse_args()\n    agent = train_ppo(args)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n    if args.env_id == \"Probe1-v0\":\n        print(\n            \"Probe1-v0: Checking if agent learns constant reward. On my machine this consistently passed after 10K timesteps.\"\n        )\n        batch = t.tensor([[0.0]]).to(device)\n        value = agent.critic(batch)\n        print(\"Value: \", value)\n        expected = t.tensor([[1.0]]).to(device)\n        utils.allclose_atol(value, expected, 0.0001)\n    elif args.env_id == \"Probe2-v0\":\n        print(\n            \"Probe2-v0: Checking if agent learns predictable reward. On my machine this consistently passed after 10K timesteps.\"\n        )\n        batch = t.tensor([[-1.0], [+1.0]]).to(device)\n        value = agent.critic(batch)\n        print(\"Value:\", value)\n        expected = batch\n        utils.allclose_atol(value, expected, 0.0001)\n    elif args.env_id == \"Probe3-v0\":\n        print(\n            \"Probe3-v0: Checking reward discounting. The value of the 0 observation in the initial state should converge to gamma.\"\n        )\n        batch = t.tensor([[0.0], [1.0]]).to(device)\n        value = agent.critic(batch)\n        print(\"Value: \", value)\n        expected = t.tensor([[args.gamma], [1.0]])\n        utils.allclose_atol(value, expected, 0.001)\n    elif args.env_id == \"Probe4-v0\":\n        print(\"Probe4-v0: Checking policy & advantage. May take 30K+ steps to converge!\")\n        batch = t.tensor([[0.0]]).to(device)\n        value = agent.critic(batch)\n        expected_value = t.tensor([[1.0]])\n        print(\"Value: \", value)\n        policy_probs = agent.actor(batch).softmax(dim=-1)\n        expected_probs = t.tensor([[0, 1]]).to(device)\n        print(\"Policy: \", policy_probs)\n        utils.allclose_atol(policy_probs, expected_probs, 0.01)\n        utils.allclose_atol(value, expected_value, 0.01)\n    elif args.env_id == \"Probe5-v0\":\n        print(\"Checking dependence on both obs and action. May also take 30K+ steps to converge.\")\n        batch = t.tensor([[0.0], [1.0]]).to(device)\n        value = agent.critic(batch)\n        expected_value = t.tensor([[1.0], [1.0]]).to(device)\n        print(\"Value: \", value)\n        policy_probs = agent.actor(batch).softmax(dim=-1)\n        expected_probs = t.tensor([[1.0, 0.0], [0.0, 1.0]]).to(device)\n        print(\"Policy: \", policy_probs)\n        utils.allclose_atol(policy_probs, expected_probs, 0.01)\n        utils.allclose_atol(value, expected_value, 0.01)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Debug Variables (detail #12)\n\nGo through and check each of the debug variables that are logged. Make sure your implementation computes or calculates the values and that you have an understanding of what they mean and what they should look like.\n\n### Update Frequency\n\nNote that the debug values are currently only logged once per update, meaning some are computed from the last minibatch of the last epoch in the update. This isn't necessarily the best thing to do, but if you log too often it can slow down training. You can experiment with logging more often, or tracking the average over the update or even an exponentially moving average.\n\n## Bonus\n\n### Continuous Action Spaces\n\nThe `MountainCar-v0` environment has discrete actions, but there's also a version `MountainCarContinuous-v0` with continuous action spaces. Unlike DQN, PPO can handle continuous actions with minor modifications. Try to adapt your agent; you'll need to handle `gym.spaces.Box` instead of `gym.spaces.Discrete` and make note of the \"9 details for continuous action domains\" section of the reading.\n\n### Vectorized Advantage Calculation\n\nTry optimizing away the for-loop in your advantage calculation. It's tricky, so an easier version of this is: find a vectorized calculation and try to explain what it does.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}