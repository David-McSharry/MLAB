{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W2D2 Part 2 - BERT Fine-Tuning\n\n## Table of Contents\n\n    - [BertClassifier](#bertclassifier)\n- [Training Loop](#training-loop)\n    - [Training All Parameters](#training-all-parameters)\n    - [Learning Rate](#learning-rate)\n    - [Loss Functions](#loss-functions)\n    - [Gradient Clipping](#gradient-clipping)\n    - [Batch Size](#batch-size)\n    - [Optimizer](#optimizer)\n    - [Logging](#logging)\n- [Evaluation](#evaluation)\n    - [Inspecting the Errors](#inspecting-the-errors)\n- [Bonus](#bonus)\n    - [Advanced Fine-Tuning](#advanced-fine-tuning)\n    - [More on Metrics](#more-on-metrics)\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import os\nimport sys\nimport time\nfrom dataclasses import dataclass\nimport torch as t\nimport transformers\nfrom einops import rearrange\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm.auto import tqdm\nimport wandb\nfrom w2d1_solution import BertCommon, BertConfig, load_pretrained_weights\nfrom w2d2_part1_data_prep_solution import DATA_FOLDER, SAVED_TOKENS_PATH\n\nMAIN = __name__ == \"__main__\"\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nIS_CI = os.getenv(\"IS_CI\")\nif IS_CI:\n    sys.exit(0)\nif MAIN:\n    (train_data, test_data) = t.load(SAVED_TOKENS_PATH)\n    bert_config = BertConfig()\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### BertClassifier\n\nNow we'll set up our BERT to do classification, starting with the `BertCommon` from before and adding a few layers on the end:\n\n- Use only the output logits at the first sequence position (index 0).\n- Add a dropout layer with the same dropout probability as before.\n- Add a `Linear` layer from `hidden_size` to `2` for the classification as positive/negative.\n- Add a `Linear` layer from `hidden_size` to `1` for the star rating.\n- By default, our star rating Linear layer is initialized to give inputs of roughly mean 0 and std 1. Multiply the output of this layer by 5 and add 5 to bring these closer to the 1-10 output we want; this isn't strictly necessary but helps speed training.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@dataclass(frozen=True)\nclass BertClassifierOutput:\n    \"\"\"The output of BertClassifier.\"\"\"\n\n    is_positive: t.Tensor\n    star_rating: t.Tensor\n\n\nclass BertClassifier(nn.Module):\n    def __init__(self, config: BertConfig):\n        pass\n\n    def forward(self, input_ids: t.Tensor, one_zero_attention_mask: t.Tensor) -> BertClassifierOutput:\n        pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Training Loop\n\nLoad your `BertLanguageModel` from yesterday and use the pretrained `BertCommon` inside it for the initial parameters of your `BertClassifier`.\n\nCopy over a training loop from before and modify it.\n\n### Training All Parameters\n\nWhen fine-tuning a language model, ensure all the parameters have `requires_grad=True`. This is different from fine-tuning an image model, where you typically \"freeze\" (`requires_grad=False`) the existing layers and just train your new layers.\n\n### Learning Rate\n\nThe learning rate for fine-tuning should be much lower than when training from scratch. In Appendix A.3 of the [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf), they suggest a learning rate for Adam between 2e-5 and 5e-5. I found that even 2e-5 was too high for this problem, and that 1e-5 worked well.\n\n### Loss Functions\n\nUse `torch.nn.CrossEntropyLoss` for the classification loss. For the star loss, empirically `F.l1_loss` works well. When you have multiple loss terms, you usually need to weight them by importance so their scales aren't too different. A default parameter for this is part of the training config.\n\n### Gradient Clipping\n\nEspecially early in training, some batches can have very large gradients, like more than 1.0. The resulting large parameter updates can break training. To work around this, you can manually limit the size of gradients using `t.nn.utils.clip_grad_norm_`. Generally, a limit of 1.0 works decently.\n\n### Batch Size\n\nFor a model the size of BERT, you typically want the largest batch size that fits in GPU memory. I found that a batch size of 8 used around 8GB of GPU memory, and a batch size of 16 used about 12GB of GPU memory. The BERT paper suggests a batch size of 16, so if your GPU doesn't have enough memory you could use a smaller size, accumulate your gradients, and call `optimizer.step` every second batch. In a later day, we'll learn how to use multiple GPUs instead.\n\n### Optimizer\n\nI found that `t.optim.AdamW` worked well.\n\n### Logging\n\nSend detailed information to Weights and Biases. Sending too much information too often can slow down training, but something I would recommend is every few batches, periodically decoding some training data back to text and logging the text and model's predictions using `wandb.Table`.\n\n<details>\n\n<summary>Training isn't converging and I don't know why!</summary>\n\n- Double check that your BertClassifier is actually using the pretrained weights and not random ones.\n- The classification loss for positive/negative should be around `log(2)` before any optimizer steps are taken, because the model is predicting randomly. If this isn't the case, there might be a bug in your loss calculation.\n- Try decoding a batch from your DataLoader and verify that the labels match up and the tokens and padding are right. It should be [CLS], the review, [SEP], and then [PAD] up to the end of the sequence.\n- Try using an even smaller learning rate to see if this affects the loss curve. It's usually better to have a learning rate that is too low and spend more iterations reaching a good solution than to use one that is too high, which can cause training to not converge at all.\n- If your model is predicting all 1 or all 0, this can be a helpful thing to investigate.\n- Check your model output for `NaN` and if you find any, use hooks to track down where it comes from.\n- It may just be a bad seed. The paper [On the Stability of Fine-Tuning BERT: Misconceptions, Explanations, and Strong Baselines](https://arxiv.org/pdf/2006.04884.pdf) notes that random seed can make a large difference to the results.\n</details>\n\n<details>\n\n<summary>My loss seems too good to be true!</summary>\n\nThis can happen if your training set isn't shuffled, and the model learns it can always predict a constant label. This can also happen if you've mixed up some tensors and the model is getting the labels as input.\n\n</details>\n\nOn a V100, my model was able to reach 0.20 classification loss after 2000 training examples, with slight improvement after that, corresponding to 92% accuracy.\n\nState of the art for this problem is around [96% accuracy as of 2022](https://paperswithcode.com/sota/sentiment-analysis-on-imdb).\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def train(tokenizer, config_dict: dict) -> BertClassifier:\n    pass\n\n\nif MAIN:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n    config_dict = dict(\n        lr=1e-05,\n        batch_size=16,\n        step_every=1,\n        epochs=1,\n        weight_decay=0.01,\n        num_steps=9600,\n        star_loss_weight=0.02,\n        filename=\"./data/w2d2/bert_classifier.pt\",\n    )\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Evaluation\n\nProduce predictions for a random subset of the test set, then use the provided code to measure your model's performance. On a V100, it would take around 4 minutes for all 25,000 test examples, so test your code on a tiny subset and then try a medium sized one.\n\n- Use `torch.inference_mode` to ensure gradients aren't calculated.\n- Unlike in training, for evaluation there's no such thing as a batch size that is too big. Since we don't need to store optimizer state or gradients, we can fit a larger batch onto the GPU. Depending on the specifics of the computation, it will either be equally as fast or faster if you use the largest batch that fits in the GPU.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def test_set_predictions(model: BertClassifier, test_data: TensorDataset, batch_size=256) -> tuple[t.Tensor, t.Tensor]:\n    \"\"\"\n    Return (predicted sentiment, predicted star rating) for each test set example.\n\n    predicted sentiment: shape (len(test_data),) - 0 or 1 for positive/negative\n    star: shape (len(test_data), ) - star rating\n    \"\"\"\n    pass\n\n\nif MAIN:\n    n = len(test_data)\n    perm = t.randperm(n)[:1000]\n    test_subset = TensorDataset(*test_data[perm])\n    (pred_sentiments, pred_stars) = test_set_predictions(model, test_subset)\n    correct = pred_sentiments.cpu() == test_subset.tensors[2]\n    sentiment_acc = correct.float().mean()\n    star_diff = pred_stars.cpu() - test_subset.tensors[3]\n    star_error = star_diff.abs().mean()\n    print(f\"Test accuracy: {sentiment_acc:.2f}\")\n    print(f\"Star MAE: {star_error:.2f}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Inspecting the Errors\n\nPrint out an example that your model got egregiously wrong - for example, the predicted star rating is very different from the actual, or the model placed a very high probability on the incorrect class. Ideally, do this without looking at the true label.\n\nDecode the text and make your own prediction, then check the true label. How good was your own prediction? Do you agree more with the \"true\" label or with your model?\n\nIf the model was in fact wrong, speculate on why it got that example wrong.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Bonus\n\nGo on to Part 3, but if you have time at the end you can come back and try the bonus exercise for this part.\n\n### Advanced Fine-Tuning\n\nRead the paper [Revisiting Few-Sample BERT Fine-Tuning](https://openreview.net/pdf?id=cO1IH43yUF). Summarize their claims and see if you can improve your accuracy by implementing one.\n\n### More on Metrics\n\nOur evaluation metric is not fully aligned with the loss function:\n\n    - The cross entropy loss rewards the model more for getting the probability of the correct class closer and closer to 1.\n    - In evaluation, the accuracy metric only cares if the correct class prediction is the highest, and make no distinction between predicting the correct class with 51% probability and with 99% probability.\n\nThis suggests that we could potentially obtain a higher accuracy by using a different loss function that would encourage the model to focus more on getting past the 50% mark. Propose an alternative loss function, or research existing alternatives, and try implementing one to see if it improves accuracy.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}