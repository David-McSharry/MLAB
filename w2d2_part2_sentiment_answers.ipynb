{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","# W2D2 Part 2 - BERT Fine-Tuning\n","\n","## Table of Contents\n","\n","    - [BertClassifier](#bertclassifier)\n","- [Training Loop](#training-loop)\n","    - [Training All Parameters](#training-all-parameters)\n","    - [Learning Rate](#learning-rate)\n","    - [Loss Functions](#loss-functions)\n","    - [Gradient Clipping](#gradient-clipping)\n","    - [Batch Size](#batch-size)\n","    - [Optimizer](#optimizer)\n","    - [Logging](#logging)\n","- [Evaluation](#evaluation)\n","    - [Inspecting the Errors](#inspecting-the-errors)\n","- [Bonus](#bonus)\n","    - [Advanced Fine-Tuning](#advanced-fine-tuning)\n","    - [More on Metrics](#more-on-metrics)\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (0.15.4)\n","Requirement already satisfied: appdirs>=1.4.3 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (1.4.4)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: psutil>=5.0.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (5.9.4)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (3.1.31)\n","Requirement already satisfied: pathtools in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setuptools in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (63.2.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (2.28.2)\n","Requirement already satisfied: setproctitle in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (1.3.2)\n","Requirement already satisfied: PyYAML in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (6.0)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (4.23.3)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (8.1.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from wandb) (1.25.1)\n","Requirement already satisfied: six>=1.4.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.0.1)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["import os\n","import sys\n","import time\n","from dataclasses import dataclass\n","import torch as t\n","import transformers\n","from einops import rearrange\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm.auto import tqdm\n","!pip install wandb\n","import wandb\n","# import wandb\n","from w2d1_solution import BertCommon, BertConfig, load_pretrained_weights\n","# not used appearantly\n","DATA_FOLDER = ''\n","SAVED_TOKENS_PATH = \"data/w2d2/tokens.pt\"\n","MAIN = __name__ == \"__main__\"\n","device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n","IS_CI = os.getenv(\"IS_CI\")\n","if IS_CI:\n","    sys.exit(0)\n","if MAIN:\n","    (train_data, test_data) = t.load(SAVED_TOKENS_PATH)\n","    bert_config = BertConfig()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","### BertClassifier\n","\n","Now we'll set up our BERT to do classification, starting with the `BertCommon` from before and adding a few layers on the end:\n","\n","- Use only the output logits at the first sequence position (index 0).\n","- Add a dropout layer with the same dropout probability as before.\n","- Add a `Linear` layer from `hidden_size` to `2` for the classification as positive/negative.\n","- Add a `Linear` layer from `hidden_size` to `1` for the star rating.\n","- By default, our star rating Linear layer is initialized to give inputs of roughly mean 0 and std 1. Multiply the output of this layer by 5 and add 5 to bring these closer to the 1-10 output we want; this isn't strictly necessary but helps speed training.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["BertConfig(vocab_size=28996, intermediate_size=3072, hidden_size=768, num_layers=12, num_heads=12, head_size=64, max_position_embeddings=512, dropout=0.1, type_vocab_size=2, layer_norm_epsilon=1e-12)\n"]}],"source":["print(bert_config)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[2.5400]], grad_fn=<AddBackward0>)\n"]}],"source":["@dataclass(frozen=True)\n","class BertClassifierOutput:\n","    \"\"\"The output of BertClassifier.\"\"\"\n","\n","    is_positive: t.Tensor\n","    star_rating: t.Tensor\n","\n","\n","class BertClassifier(nn.Module):\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.common = BertCommon(config)\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.star_linear = nn.Linear(config.hidden_size, 1)\n","        self.sentiment_linear = nn.Linear(config.hidden_size, 2)\n","\n","\n","    def forward(self, input_ids: t.Tensor, one_zero_attention_mask: t.Tensor) -> BertClassifierOutput:\n","        x = self.common(input_ids, None, one_zero_attention_mask)\n","        x = x[:, 0, :]\n","        x = self.dropout(x)\n","        star_rating = self.star_linear(x)\n","        # print the shape of the star_linear layer\n","        is_positive = self.sentiment_linear(x)\n","        bert_classifier_output = BertClassifierOutput\n","        bert_classifier_output.is_positive = is_positive\n","        bert_classifier_output.star_rating = 5 * star_rating + 5\n","        return bert_classifier_output\n","\n","if MAIN:\n","    model = BertClassifier(bert_config).to(device)\n","    input_ids_datapoint = train_data[0][0]\n","    attention_mask_datapoint = train_data[0][1]\n","    input_ids_datapoint = input_ids_datapoint.unsqueeze(0)\n","    attention_mask_datapoint = attention_mask_datapoint.unsqueeze(0)\n","    print(model.forward(input_ids_datapoint, attention_mask_datapoint).star_rating)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Training Loop\n","\n","Load your `BertLanguageModel` from yesterday and use the pretrained `BertCommon` inside it for the initial parameters of your `BertClassifier`.\n","\n","Copy over a training loop from before and modify it.\n","\n","### Training All Parameters\n","\n","When fine-tuning a language model, ensure all the parameters have `requires_grad=True`. This is different from fine-tuning an image model, where you typically \"freeze\" (`requires_grad=False`) the existing layers and just train your new layers.\n","\n","### Learning Rate\n","\n","The learning rate for fine-tuning should be much lower than when training from scratch. In Appendix A.3 of the [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf), they suggest a learning rate for Adam between 2e-5 and 5e-5. I found that even 2e-5 was too high for this problem, and that 1e-5 worked well.\n","\n","### Loss Functions\n","\n","Use `torch.nn.CrossEntropyLoss` for the classification loss. For the star loss, empirically `F.l1_loss` works well. When you have multiple loss terms, you usually need to weight them by importance so their scales aren't too different. A default parameter for this is part of the training config.\n","\n","### Gradient Clipping\n","\n","Especially early in training, some batches can have very large gradients, like more than 1.0. The resulting large parameter updates can break training. To work around this, you can manually limit the size of gradients using `t.nn.utils.clip_grad_norm_`. Generally, a limit of 1.0 works decently.\n","\n","### Batch Size\n","\n","For a model the size of BERT, you typically want the largest batch size that fits in GPU memory. I found that a batch size of 8 used around 8GB of GPU memory, and a batch size of 16 used about 12GB of GPU memory. The BERT paper suggests a batch size of 16, so if your GPU doesn't have enough memory you could use a smaller size, accumulate your gradients, and call `optimizer.step` every second batch. In a later day, we'll learn how to use multiple GPUs instead.\n","\n","### Optimizer\n","\n","I found that `t.optim.AdamW` worked well.\n","\n","### Logging\n","\n","Send detailed information to Weights and Biases. Sending too much information too often can slow down training, but something I would recommend is every few batches, periodically decoding some training data back to text and logging the text and model's predictions using `wandb.Table`.\n","\n","<details>\n","\n","<summary>Training isn't converging and I don't know why!</summary>\n","\n","- Double check that your BertClassifier is actually using the pretrained weights and not random ones.\n","- The classification loss for positive/negative should be around `log(2)` before any optimizer steps are taken, because the model is predicting randomly. If this isn't the case, there might be a bug in your loss calculation.\n","- Try decoding a batch from your DataLoader and verify that the labels match up and the tokens and padding are right. It should be [CLS], the review, [SEP], and then [PAD] up to the end of the sequence.\n","- Try using an even smaller learning rate to see if this affects the loss curve. It's usually better to have a learning rate that is too low and spend more iterations reaching a good solution than to use one that is too high, which can cause training to not converge at all.\n","- If your model is predicting all 1 or all 0, this can be a helpful thing to investigate.\n","- Check your model output for `NaN` and if you find any, use hooks to track down where it comes from.\n","- It may just be a bad seed. The paper [On the Stability of Fine-Tuning BERT: Misconceptions, Explanations, and Strong Baselines](https://arxiv.org/pdf/2006.04884.pdf) notes that random seed can make a large difference to the results.\n","</details>\n","\n","<details>\n","\n","<summary>My loss seems too good to be true!</summary>\n","\n","This can happen if your training set isn't shuffled, and the model learns it can always predict a constant label. This can also happen if you've mixed up some tensors and the model is getting the labels as input.\n","\n","</details>\n","\n","On a V100, my model was able to reach 0.20 classification loss after 2000 training examples, with slight improvement after that, corresponding to 92% accuracy.\n","\n","State of the art for this problem is around [96% accuracy as of 2022](https://paperswithcode.com/sota/sentiment-analysis-on-imdb).\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76f4ad3124dd4a44a96455b969d6b097","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n","[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     44\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     45\u001b[0m     lr\u001b[39m=\u001b[39m\u001b[39m1e-05\u001b[39m,\n\u001b[1;32m     46\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/w2d2/bert_classifier.pt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39m# train \u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m bert_classifier \u001b[39m=\u001b[39m train(tokenizer, config_dict)\n","Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(tokenizer, config_dict)\u001b[0m\n\u001b[1;32m     27\u001b[0m bert_classifier_output \u001b[39m=\u001b[39m bert_classifier(input_ids, attention_masks)\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39mstar_loss_weight\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m loss_star(bert_classifier_output\u001b[39m.\u001b[39mstar_rating\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), y_star) \u001b[39m+\u001b[39m loss_sentiment(bert_classifier_output\u001b[39m.\u001b[39mis_positive, y_sentiment)\n\u001b[0;32m---> 29\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     30\u001b[0m t\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(bert_classifier\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m opt\u001b[39m.\u001b[39mstep()\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","def train(tokenizer, config_dict: dict) -> BertClassifier:\n","   # load pretrained weights\n","    bert_classifier = BertClassifier(bert_config).to(device)\n","    bert_vanilla = load_pretrained_weights(bert_config)\n","    # copy the common module from bert_vanilla to bert_classifier\n","    bert_classifier.common.load_state_dict(bert_vanilla.common.state_dict())\n","\n","    # wandb.init(project=\"w2d2\")\n","\n","    bert_classifier.train()\n","    opt = t.optim.AdamW(bert_classifier.parameters(), lr=config_dict[\"lr\"], weight_decay=config_dict[\"weight_decay\"])\n","    # create a dataloader\n","    dataloader = DataLoader(train_data, batch_size=config_dict[\"batch_size\"], shuffle=True)\n","    # create a loss function\n","    loss_star = nn.L1Loss()\n","    loss_sentiment = nn.CrossEntropyLoss()\n","\n","\n","    i = 0\n","    for epoch in range(config_dict[\"epochs\"]):\n","        for input_ids, attention_masks, stars, sentiment_binaries in tqdm(dataloader):\n","            opt.zero_grad()\n","            input_ids = input_ids.to(device)\n","            attention_masks = attention_masks.to(device)\n","            y_star = sentiment_binaries.to(device)\n","            y_sentiment = stars.to(device)\n","            bert_classifier_output = bert_classifier(input_ids, attention_masks)\n","            loss = config_dict[\"star_loss_weight\"] * loss_star(bert_classifier_output.star_rating.squeeze(1), y_star) + loss_sentiment(bert_classifier_output.is_positive, y_sentiment)\n","            loss.backward()\n","            t.nn.utils.clip_grad_norm_(bert_classifier.parameters(), max_norm=1)\n","            opt.step()\n","            # wandb.log({\"loss\": loss.item()})\n","            # wandb not working, log the loss manually every 100 steps\n","            if (epoch * len(dataloader) + i) % 100 == 0:\n","                print(f\"loss: {loss.item()}\")\n","            i += 1\n","            \n","\n","    return bert_classifier                \n","\n","\n","if MAIN:\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","    config_dict = dict(\n","        lr=1e-05,\n","        batch_size=8,\n","        step_every=1,\n","        epochs=1,\n","        weight_decay=0.01,\n","        num_steps=9600,\n","        star_loss_weight=0.02,\n","        filename=\"./data/w2d2/bert_classifier.pt\",\n","    )\n","    # train \n","    bert_classifier = train(tokenizer, config_dict)\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Evaluation\n","\n","Produce predictions for a random subset of the test set, then use the provided code to measure your model's performance. On a V100, it would take around 4 minutes for all 25,000 test examples, so test your code on a tiny subset and then try a medium sized one.\n","\n","- Use `torch.inference_mode` to ensure gradients aren't calculated.\n","- Unlike in training, for evaluation there's no such thing as a batch size that is too big. Since we don't need to store optimizer state or gradients, we can fit a larger batch onto the GPU. Depending on the specifics of the computation, it will either be equally as fast or faster if you use the largest batch that fits in the GPU.\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53bae707a91443448ab4b4acaa18c222","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m perm \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mrandperm(n)[:\u001b[39m1000\u001b[39m]\n\u001b[1;32m     30\u001b[0m test_subset \u001b[39m=\u001b[39m TensorDataset(\u001b[39m*\u001b[39mtest_data[perm])\n\u001b[0;32m---> 31\u001b[0m (pred_sentiments, pred_stars) \u001b[39m=\u001b[39m test_set_predictions(model, test_subset)\n\u001b[1;32m     32\u001b[0m correct \u001b[39m=\u001b[39m pred_sentiments\u001b[39m.\u001b[39mcpu() \u001b[39m==\u001b[39m test_subset\u001b[39m.\u001b[39mtensors[\u001b[39m2\u001b[39m]\n\u001b[1;32m     33\u001b[0m sentiment_acc \u001b[39m=\u001b[39m correct\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n","Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtest_set_predictions\u001b[0;34m(model, test_data, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m attention_masks \u001b[39m=\u001b[39m attention_masks\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m bert_classifier_output \u001b[39m=\u001b[39m model(input_ids, attention_masks)\n\u001b[1;32m     18\u001b[0m predicted_sentiment\u001b[39m.\u001b[39mappend(bert_classifier_output\u001b[39m.\u001b[39mis_positive\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     19\u001b[0m predicted_star\u001b[39m.\u001b[39mappend(bert_classifier_output\u001b[39m.\u001b[39mstar_rating\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, input_ids, one_zero_attention_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids: t\u001b[39m.\u001b[39mTensor, one_zero_attention_mask: t\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BertClassifierOutput:\n\u001b[0;32m---> 19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommon(input_ids, \u001b[39mNone\u001b[39;49;00m, one_zero_attention_mask)\n\u001b[1;32m     20\u001b[0m     x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m, :]\n\u001b[1;32m     21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Documents/mlab/w2d1_solution.py:542\u001b[0m, in \u001b[0;36mBertCommon.forward\u001b[0;34m(self, input_ids, token_type_ids, one_zero_attention_mask)\u001b[0m\n\u001b[1;32m    540\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x))\n\u001b[1;32m    541\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 542\u001b[0m     x \u001b[39m=\u001b[39m block(x, additive_attention_mask\u001b[39m=\u001b[39;49madditive_attention_mask)\n\u001b[1;32m    543\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Documents/mlab/w2d1_solution.py:456\u001b[0m, in \u001b[0;36mBertBlock.forward\u001b[0;34m(self, x, additive_attention_mask)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mSOLUTION\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(x, additive_attention_mask)\n\u001b[0;32m--> 456\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(x)\n\u001b[1;32m    457\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Documents/mlab/w2d1_solution.py:403\u001b[0m, in \u001b[0;36mBertMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    401\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msecond_linear(x)\n\u001b[1;32m    402\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m--> 403\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_norm(x \u001b[39m+\u001b[39;49m skip)\n\u001b[1;32m    404\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Documents/mlab/w2d1_solution.py:317\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39m# Chris: MLAB1 repo solution had .detach() here but I think that is wrong\u001b[39;00m\n\u001b[1;32m    316\u001b[0m mean \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_dims, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 317\u001b[0m var \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mvar(dim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize_dims, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, unbiased\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    319\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39m mean\n\u001b[1;32m    320\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m/\u001b[39m ((var \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def test_set_predictions(model: BertClassifier, test_data: TensorDataset, batch_size=256) -> tuple[t.Tensor, t.Tensor]:\n","    \"\"\"\n","    Return (predicted sentiment, predicted star rating) for each test set example.\n","\n","    predicted sentiment: shape (len(test_data),) - 0 or 1 for positive/negative\n","    star: shape (len(test_data), ) - star rating\n","    \"\"\"\n","    # create a dataloader\n","    dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","    model.eval()\n","    predicted_sentiment = []\n","    predicted_star = []\n","    with t.inference_mode():\n","        for input_ids, attention_masks, _, _ in tqdm(dataloader):\n","            input_ids = input_ids.to(device)\n","            attention_masks = attention_masks.to(device)\n","            bert_classifier_output = model(input_ids, attention_masks)\n","            predicted_sentiment.append(bert_classifier_output.is_positive.argmax(dim=1))\n","            predicted_star.append(bert_classifier_output.star_rating.squeeze(1))\n","    predicted_sentiment = t.cat(predicted_sentiment)\n","    predicted_star = t.cat(predicted_star)\n","    return predicted_sentiment, predicted_star\n","    \n","\n","\n","if MAIN:\n","    model = BertClassifier(bert_config).to(device)\n","    n = len(test_data)\n","    perm = t.randperm(n)[:1000]\n","    test_subset = TensorDataset(*test_data[perm])\n","    (pred_sentiments, pred_stars) = test_set_predictions(model, test_subset)\n","    correct = pred_sentiments.cpu() == test_subset.tensors[2]\n","    sentiment_acc = correct.float().mean()\n","    star_diff = pred_stars.cpu() - test_subset.tensors[3]\n","    star_error = star_diff.abs().mean()\n","    print(f\"Test accuracy: {sentiment_acc:.2f}\")\n","    print(f\"Star MAE: {star_error:.2f}\")\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","### Inspecting the Errors\n","\n","Print out an example that your model got egregiously wrong - for example, the predicted star rating is very different from the actual, or the model placed a very high probability on the incorrect class. Ideally, do this without looking at the true label.\n","\n","Decode the text and make your own prediction, then check the true label. How good was your own prediction? Do you agree more with the \"true\" label or with your model?\n","\n","If the model was in fact wrong, speculate on why it got that example wrong.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MAIN:\n","    \"TODO: YOUR CODE HERE\"\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Bonus\n","\n","Go on to Part 3, but if you have time at the end you can come back and try the bonus exercise for this part.\n","\n","### Advanced Fine-Tuning\n","\n","Read the paper [Revisiting Few-Sample BERT Fine-Tuning](https://openreview.net/pdf?id=cO1IH43yUF). Summarize their claims and see if you can improve your accuracy by implementing one.\n","\n","### More on Metrics\n","\n","Our evaluation metric is not fully aligned with the loss function:\n","\n","    - The cross entropy loss rewards the model more for getting the probability of the correct class closer and closer to 1.\n","    - In evaluation, the accuracy metric only cares if the correct class prediction is the highest, and make no distinction between predicting the correct class with 51% probability and with 99% probability.\n","\n","This suggests that we could potentially obtain a higher accuracy by using a different loss function that would encourage the model to focus more on getting past the 50% mark. Propose an alternative loss function, or research existing alternatives, and try implementing one to see if it improves accuracy.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":4}
