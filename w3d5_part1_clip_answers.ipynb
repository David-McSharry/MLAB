{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D5 - Part 1 - Contrastive Language-Image Pre-Training (CLIP)\n\nCLIP is a model that contains a vision model and a language model, and it is trained so that the embeddings produced by the two parts are similar when the image and the text have a similar meaning or topic. Today, we're using a vision transformer (which is exactly what it sounds like) and a GPT-style transformer, but you could also use a ResNet and a bag-of-words model or whatever you like.\n\nFor training data, the authors scraped the Internet for images that have captions under them to obtain (image, text) pairs that we assume are both \"about\" the same thing. For example, the image might be a guinea pig eating a cucumber and the caption says \"Mr. Fluffers fueling up with some Vitamin C.\"\n\n<p align=\"center\">\n    <img src=\"clip_images/guineapig_cucumber2.jpg\" width=\"400\" /><br>\n    Mr. Fluffers fueling up with some Vitamin C.\n</p>\n\nTo do traditional supervised learning, we would start with the image, feed it in, and then try to unembed to generate text and compare the predicted text to the actual caption. Or start with the text, generate an image, and compare the image to the actual.\n\nFor either of these, it's tricky to define a differentiable loss function that captures \"how close is the meaning of these two pieces of text\" or \"how much do these two images depict the same thing\".\n\nCLIP instead uses a **contrastive loss**, which just means that the embedding of an image and the embedding of its matching caption should be similar, while the embedding of an image and all the other captions in the batch should be dissimilar. \"Similar\" is efficiently calculated using the cosine similarity.\n\nIt turns out that if you use large enough batch sizes (like 32,768) and a large enough dataset (400 million pairs), this works quite well. It takes thousands of GPU-days to train a CLIP, so we will just play with the pretrained weights today.\n\nSome cool applications of this are:\n\n- Given an image, you can embed it and see how similar that embedding is to the embedding of the string \"photo of a dog\" versus the string \"photo of a cat\". This means you can classify images, but in a much more flexible way than a traditional supervised classifier where the set of categories is fixed up front.\n- Given some text, you can search a large database of image embeddings for images that are similar to the text embedding.\n\nThe outline of today's tasks are:\n\nPart 1:\n\n- Implement your own vision transformer\n- Reuse old GPT code for the text transformer\n- Assemble a CLIP and load pretrained weights\n- Play with CLIP!\n\nPart 2:\n\n- Complete the implementation of a Stable Diffusion inference pipeline\n- Run inference on your model\n- Play with other things you can do with Stable Diffusion, such as animations\n\n## Table of Contents\n\n- [References (optional) for Part 1](#references-optional-for-part-)\n- [Vision Transformers](#vision-transformers)\n    - [Patch Embedding](#patch-embedding)\n    - [Positional Embedding](#positional-embedding)\n    - [Class Embedding (replacement for \"begin\" token)](#class-embedding-replacement-for-begin-token)\n    - [Config Classes](#config-classes)\n    - [position_ids](#positionids)\n- [CLIP MLP](#clip-mlp)\n- [Self-Attention](#self-attention)\n- [CLIP Layer](#clip-layer)\n- [CLIP Encoder](#clip-encoder)\n- [CLIPVisionTransformer](#clipvisiontransformer)\n- [CLIPTextTransformer](#cliptexttransformer)\n- [CLIPModel](#clipmodel)\n- [Data Preparation](#data-preparation)\n- [Cosine Similarities](#cosine-similarities)\n- [Running the Model](#running-the-model)\n- [Implementing Constrastive Loss](#implementing-constrastive-loss)\n- [Bonus](#bonus)\n    - [Prompt Engineering and Zero Shot Classification](#prompt-engineering-and-zero-shot-classification)\n    - [GELU approximations](#gelu-approximations)\n    - [X-CLIP](#x-clip)\n\n## References (optional) for Part 1\n\nCLIP\n\n- [Paper](https://arxiv.org/pdf/2103.00020.pdf)\n- [Official OpenAI repo](https://github.com/openai/CLIP)\n- [HuggingFace implementation](https://huggingface.co/sentence-transformers/clip-ViT-L-14)\n\nX-CLIP - Includes experimental improvements from recent papers\n\n- [Code repo](https://github.com/lucidrains/x-clip)\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import glob\nimport os\nimport sys\nfrom typing import Callable, Union, cast\nimport pandas as pd\nimport torch as t\nfrom einops import rearrange, repeat\nfrom fancy_einsum import einsum\nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom transformers.models.clip import modeling_clip\nimport utils\nimport w3d5_test\nfrom utils import allclose\nfrom w3d5_globals import (\n    CLIPConfig,\n    CLIPOutput,\n    CLIPTextConfig,\n    CLIPVisionConfig,\n    get_reference_model,\n    get_reference_clip_model,\n)\n\nMAIN = __name__ == \"__main__\"\nIS_CI = os.getenv(\"IS_CI\")\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Vision Transformers\n\nOur first task for today is to implement a vision transformer.\n\nUsing transformers on image data is actually easier than using it on text data. Because our input data is already continuous, we don't need a special tokenizer and we don't need a `nn.Embedding` to translate from discrete token ids to a continuous representation. (Technically the RGB pixel values are discrete in the range [0, 256), but we won't worry about this).\n\nThe only issue is that for our images (which for today we'll assume to be exactly 224 x 224px) treating each pixel as a sequence element would result in a sequence length around 50K. Since self-attention is quadratic in the sequence length, we'd prefer to decrease this sequence length to something more manageable. This is analogous to how we won't model text as individual characters, but as slightly larger chunks.\n\nThe original [Vision Transformers paper](https://arxiv.org/pdf/2010.11929.pdf) used 14x14 patches of 16x16 pixels each, but in our implementation of CLIP (matching CLIP ViT-L/14) the patch size specified in `CLIPVisionConfig` is 14x14 pixels, which means there are 16x16 = 256 total patches.\n\nThe rest of the vision transformer is going to look extremely similar to what you've seen with GPT.\n\n### Patch Embedding\n\nThere are a couple equivalent ways to obtain an embedding vector for each patch. For example, you could use `einops.rearrange` and a `Linear(patch_pixels, hidden_size)`. Instead, we're going to follow HuggingFace and use a `nn.Conv2d` with appropriate stride and kernel size (and no bias).\n\n### Positional Embedding\n\nWhen first learning vision transformers, I expected the positional embedding would work best by indicating (x, y) coordinates for each patch so that the model can easily understand the 2D spatial relationships.\n\nHowever, the Vision Transformers paper found no difference between this 2D method and just simply numbering the patches (see appendix D.4). This means that the model has to learn itself that patch 16 is correlated with patches 0 and 32 (because they are vertically adjacent), but this doesn't seem to be a problem. They speculate that there are so few patches that it's just very easy to memorize these patterns.\n\n### Class Embedding (replacement for \"begin\" token)\n\nWhen using text, it's common practice to have the tokenizer prepend a special placeholder token called the \"begin token\". When we train the model for sequence classification, we use the final layer's embedding at this sequence position for the representation of the entire sequence, so attention heads learn to copy relevant data to this position.\n\nSince there's no separate tokenizer for vision, we're going to initialize a random normal embedding vector of `embedding_size` and prepend that to every sequence. This embedding is called the class embedding because it's used for classification.\n\n### Config Classes\n\nConfig dataclasses for CLIP have been defined and imported from `w3d5_globals.py` These will be a helpful reference as you build the CLIP components.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def print_class_attrs(cls: type) -> None:\n    print(f\"\\n\\n{cls.__name__}\\n---\")\n    for (k, v) in ((k, v) for (k, v) in vars(cls).items() if k[0] != \"_\"):\n        print(f\"{k}: {v}\")\n\n\nif MAIN:\n    print_class_attrs(CLIPVisionConfig)\n    print_class_attrs(CLIPTextConfig)\n    print_class_attrs(CLIPConfig)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n### position_ids\n\nRegister a buffer called `position_ids` which just contains `arange(0, (self.image_size // self.patch_size) ** 2 + 1)`. The extra index is for the class embedding in addition to the standard patches. This avoids redundantly allocating the `arange` on the target device on every forward pass.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class CLIPVisionEmbeddings(nn.Module):\n    config: CLIPVisionConfig\n    patch_size: int\n    image_size: int\n    embed_dim: int\n    num_patches: int\n    class_embedding: nn.Parameter\n    patch_embedding: nn.Conv2d\n    position_embedding: nn.Embedding\n    position_ids: t.Tensor\n\n    def __init__(self, config: CLIPVisionConfig):\n        \"\"\"Assign values from input config to class member variables as appropriate,\n        e.g. self.patch_size = config.patch_size\"\"\"\n        super().__init__()\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Apply the patch embeddings and the positional embeddings and return their sum.\n\n        x: shape (batch, channels=3, height=224, width=224)\n        out: shape (batch, sequence, hidden)\n        \"\"\"\n        pass\n\n\nif MAIN and (not IS_CI):\n    w3d5_test.test_vision_embeddings(CLIPVisionEmbeddings)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## CLIP MLP\n\nThe remaining layers of CLIP operate on embedding vectors of `hidden_size`, so they're independent of whether the input was text or images.\n\nThe MLP uses a faster approximation to the [GELU](https://arxiv.org/pdf/1606.08415.pdf) nonlinearity. Note that as of PyTorch 1.11, `nn.GELU` and `F.gelu` compute the exact equation for GELU.\n\nUse the equation from the paper and implement the sigmoid approximation from section 2 yourself. Plot the absolute difference on the interval [-5, 5] and check how different the approximation is from the exact. Then implement the MLP using the approximation.\n\nThe MLP looks the same as in a standard transformer: a Linear layer that goes from hidden size to an intermediate size 4 times larger, a GELU, and a second Linear back down to the hidden size.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def gelu_sigmoid_approximation(x: t.Tensor) -> t.Tensor:\n    \"\"\"Return sigmoid approximation of GELU of input tensor x with same shape.\"\"\"\n    pass\n\n\ndef plot_gelu_approximation(x: t.Tensor):\n    (fig, (ax0, ax1)) = plt.subplots(nrows=2, figsize=(12, 12))\n    actual = F.gelu(x)\n    approx = gelu_sigmoid_approximation(x)\n    diff = (actual - approx).abs()\n    x_cpu = x.cpu()\n    ax0.plot(x_cpu, diff.cpu(), label=\"absolute error\")\n    ax0.legend()\n    ax1.plot(x_cpu, actual.cpu(), label=\"exact\", alpha=0.5)\n    ax1.plot(x_cpu, approx.cpu(), label=\"sigmoid\", alpha=0.5)\n    ax1.legend()\n    ax1.set(xlabel=f\"x ({x.dtype})\")\n\n\nif MAIN and (not IS_CI):\n    x = t.linspace(-5, 5, 400)\n    plot_gelu_approximation(x)\n    if t.cuda.is_available():\n        x16 = t.linspace(-5, 5, 400, dtype=t.float16, device=device)\n        plot_gelu_approximation(x16)\n\n\nclass CLIPMLP(nn.Module):\n    fc1: nn.Linear\n    fc2: nn.Linear\n\n    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n        \"\"\"Initialize parent class, then assign fully-connected layers based\n        on shape in input config\"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Run forward pass of MLP, including fully-connected layers and non-linear\n        activations where appropriate\"\"\"\n        pass\n\n\nif MAIN and (not IS_CI):\n    w3d5_test.test_mlp(CLIPMLP)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Self-Attention\n\nFor the vision transformer, the authors don't use masked attention. You should be able to copy and paste from your `BertSelfAttention` class you wrote previously and fix up the variable names. Or try writing it from memory for the practice.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class CLIPAttention(nn.Module):\n    num_heads: int\n    head_size: int\n    q_proj: nn.Linear\n    k_proj: nn.Linear\n    v_proj: nn.Linear\n    out_proj: nn.Linear\n    dropout: nn.Dropout\n\n    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n        \"\"\"Assign values from input config to class member variables as appropriate\"\"\"\n        pass\n\n    def attention_pattern_pre_softmax(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Return the attention pattern after scaling but before softmax.\n\n        pattern[batch, head, q, k] should be the match between a query at sequence position q and a key at sequence position k.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Perform forward pass through attention layer, computing attention pattern and value projections\n        to combine into output. Remember to apply dropout.\"\"\"\n        pass\n\n\nif MAIN and (not IS_CI):\n    w3d5_test.test_vision_attention(CLIPAttention)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## CLIP Layer\n\nIdentical to GPT (besides calling our slightly different MLP), so this is provided for you. Make sure to read through and understand\nthe operations being performed.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class CLIPEncoderLayer(nn.Module):\n    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = CLIPAttention(config)\n        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n        self.mlp = CLIPMLP(config)\n        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n\n    def forward(self, x):\n        x = x + self.self_attn(self.layer_norm1(x))\n        x = x + self.mlp(self.layer_norm2(x))\n        return x\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## CLIP Encoder\n\nThis is also provided as it's trivial. Note that a full-fledged implementation this would have more code in it for things like checkpointing.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class CLIPEncoder(nn.Module):\n    layers: utils.StaticModuleList[CLIPEncoderLayer]\n\n    def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n        super().__init__()\n        self.layers = utils.StaticModuleList([CLIPEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## CLIPVisionTransformer\n\nThis is the last class to implement before we can load pretrained weights for the vision transformer!\n\nThe output will consist of only the first sequence position corresponding to the prepended \"class embedding\". Do the slice before the final layer norm to avoid unnecessary computation.\n\nWe've made all the variable names identical so far with the idea that the state dict should exactly match. However, the pretrained weights have spelled `pre_layrnorm` incorrectly. Sad! If this really bothers you, you can fix it in your version and adjust the weight loading code to adapt.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class CLIPVisionTransformer(nn.Module):\n    config: CLIPVisionConfig\n    embeddings: CLIPVisionEmbeddings\n    pre_layrnorm: nn.LayerNorm\n    encoder: CLIPEncoder\n    post_layernorm: nn.LayerNorm\n\n    def __init__(self, config: CLIPVisionConfig):\n        \"\"\"Assign values from input config to class member variables as appropriate\"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Perform forward pass through vision transformer: embedding, layer norm, encoder, layer norm\n        Return output corresponding to prepended class_embedding\"\"\"\n        pass\n\n\nif MAIN and (not IS_CI):\n    w3d5_test.test_vision_transformer(CLIPVisionTransformer)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n## CLIPTextTransformer\n\nThe text transformer looks a lot like BERT, except it does have the causal attention mask like GPT.\n\nIt supports sequences of varying lengths with padding at the end, and padding tokens are also masked out during attention. We won't bother re-implementing the code, since this is very similar to what you've done before.\n\nWe do need a tokenizer for the text stuff, and again we'll use the provided one since it works the same as you've seen previously.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN and (not IS_CI):\n    tokenize = get_reference_model().tokenize\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## CLIPModel\n\nNow we're ready to put together the full model. In general, since we allow mixing and matching any models, the embedding of the image and text aren't going to have the same dimension.\n\nThe CLIPModel has two linear projections that take the individual model outputs to a common hidden size of `config.projection_dim`.\n\nCLIPModel also takes care of normalizing each unit vector to have a L2 norm of 1. This is because cosine similarity can be calculated as a dot product of two unit vectors. Finally, the two embeddings are packaged into a tuple.\n\nThe scalar parameter `logit_scale` is only used during training, where it's used to multiply the similarity scores before computing the contrastive loss.\n\n```mermaid\n\ngraph TD\n    subgraph CLIPModel\n\n    Image --> ImageTransformer --> VisualProjection --> Normalize1[Normalize] --> CLIPOutput\n    Text --> TextTransformer --> TextProjection --> Normalize2[Normalize] --> CLIPOutput\n\n    end\n```\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class CLIPModel(nn.Module):\n    config: CLIPConfig\n    text_config: CLIPTextConfig\n    vision_config: CLIPVisionConfig\n    projection_dim: int\n    text_embed_dim: int\n    vision_embed_dim: int\n    text_model: modeling_clip.CLIPTextTransformer\n    vision_model: CLIPVisionTransformer\n    visual_projection: nn.Linear\n    text_projection: nn.Linear\n    logit_scale: nn.Parameter\n\n    def __init__(self, config: CLIPConfig):\n        \"\"\"Assign values from input config to class member variables as appropriate.\n\n        The typechecker will complain when passing our CLIPTextConfig to CLIPTextTransformer, because the latter expects type transformers.models.clip.configuration_clip.CLIPTextConfig. You can ignore this as our type is in fact compatible.\n        \"\"\"\n        pass\n\n    def forward(self, input_ids, attention_mask, pixel_values) -> CLIPOutput:\n        \"\"\"\n        Perform forward pass through CLIP model, applying text and vision model/projection.\n\n        input_ids: (batch, sequence)\n        attention_mask: (batch, sequence). 1 for visible, 0 for invisible.\n        pixel_values: (batch, channels, height, width)\n        \"\"\"\n        pass\n\n\nif MAIN and (not IS_CI):\n    w3d5_test.test_clip_model(CLIPModel)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Data Preparation\n\nThe data preparation is the same as you've seen before. The ImageNet normalization constants are used. Feel free to supply some of your own text and/or images here.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def get_images(glob_fnames: str) -> tuple[list[str], list[Image.Image]]:\n    filenames = glob.glob(glob_fnames)\n    images = [Image.open(filename).convert(\"RGB\") for filename in filenames]\n    image_names = [os.path.splitext(os.path.basename(filename))[0] for filename in filenames]\n    for im in images:\n        display(im)\n    return (image_names, images)\n\n\nif MAIN and (not IS_CI):\n    preprocess = cast(\n        Callable[[Image.Image], t.Tensor],\n        transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Resize((224, 224)),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        ),\n    )\n    texts = [\n        \"A guinea pig eating a cucumber\",\n        \"A pencil sketch of a guinea pig\",\n        \"A rabbit eating a carrot\",\n        \"A paperclip maximizer\",\n    ]\n    out = tokenize(texts)\n    input_ids = out[\"input_ids\"]\n    attention_mask = out[\"attention_mask\"]\n    (image_names, images) = get_images(\"./clip_images/*\")\n    pixel_values = t.stack([preprocess(im) for im in images], dim=0)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Cosine Similarities\n\nSince the model already normalizes each embedding to be a unit vector, this function becomes a one-liner.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def cosine_similarities(a: t.Tensor, b: t.Tensor) -> t.Tensor:\n    \"\"\"Return cosine similarities between all pairs of embeddings.\n\n    Each element of the batch should be a unit vector already.\n\n    a: shape (batch_a, hidden_size)\n    b: shape (batch_b, hidden_size)\n    out: shape (batch_a, batch_b)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d5_test.test_cosine_similarity(cosine_similarities)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Running the Model\n\nRun the model and compute the cosine similarities between each image and each piece of text. Visualize the results and see if they match what you expect.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def load_trained_model(config: CLIPConfig):\n    model = CLIPModel(config)\n    full_state_dict = get_reference_clip_model().state_dict()\n    model.load_state_dict(full_state_dict)\n    return model\n\n\nif MAIN and (not IS_CI):\n    config = CLIPConfig(CLIPVisionConfig(), CLIPTextConfig())\n    model = load_trained_model(config).to(device)\n    with t.inference_mode():\n        out = model(input_ids.to(device), attention_mask.to(device), pixel_values.to(device))\n    similarities = cosine_similarities(out.text_embeds, out.image_embeds)\n    df = pd.DataFrame(similarities.detach().cpu().numpy(), index=texts, columns=image_names).round(3)\n    display(df)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Implementing Constrastive Loss\n\nWe're not going to train today, but we'll implement the contrastive loss to make sure we understand it.\n\nThere's a nice trick to implement the contrastive loss using of the average of two `F.cross_entropy` terms. See if you can find it.\n\n<details>\n\n<summary>Spoiler - Contrastive Loss Calculation</summary>\n\nFirst compute the matrix `similarities[text_index][image_index]`, of shape (batch, batch).\n\nSince the ith text corresponds to the ith image in the training data, `similarities[i]` should have a value near 1 at index i, and be low otherwise. This is just like cross entropy where the target is class i.\n\nThe same holds for `similarities[:, i]`, so that's the second cross entropy term. Each value in our matrix contributed to each term, so taking the average prevents double-counting.\n\n</details>\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def contrastive_loss(text_embeds: t.Tensor, image_embeds: t.Tensor, logit_scale: t.Tensor) -> t.Tensor:\n    \"\"\"Return the contrastive loss between a batch of text and image embeddings.\n\n    The embeddings must be in order so that text_embeds[i] corresponds with image_embeds[i].\n\n    text_embeds: (batch, output_dim)\n    image_embeds: (batch, output_dim)\n    logit_scale: () - log of the scale factor to apply to each element of the similarity matrix\n\n    Out: scalar tensor containing the loss\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d5_test.test_contrastive_loss(contrastive_loss)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n# On to Part 2\n\nIn the following part of this day's exercises, we will finally get to play with the exciting and *very* state-of-the-art Stable Diffusion model, with ideas for bonus tasks after you complete the implementation of the model.\n\nIf you would like to continue working on CLIP-related models, here are some bonus tasks that you can return to after completing Part 2.\n\n## Bonus\n\n### Prompt Engineering and Zero Shot Classification\n\nThinking back to Part 1, CLIP can be used as a classifier by comparing the unknown image's embedding with the embedding of a prompt like \"a photo of [class name]\". Implement this idea and see how good the results are, then try to improve them by finding a better prompt. Or, use several prompts and ensemble the outputs together.\n\n### GELU approximations\n\nIn the CLIP model, could we have \"gotten away\" with using PyTorch's GELU instead of the approximation the authors used? Or are the pretrained weights precisely adapted to the approximation? Try running the pretrained weights using the PyTorch exact implementation and see how different the results are.\n\n### X-CLIP\n\nRead through the code at the [X-CLIP](https://github.com/lucidrains/x-clip) repo and try to understand some of the modifications and improvements.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}