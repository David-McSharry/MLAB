{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D1 - Parallelism\n\nToday's material is divided into three parts.\n\n- Part 1: practice with `torch.distributed` and understand how the different backends and operations work.\n- Part 2: implement data parallelism to speed up a training loop.\n- Part 3: implement tensor parallelism and perform inference using a model that is too large for one GPU.\n\n## Table of Contents\n\n- [Readings](#readings)\n- [Data Parallelism](#data-parallelism)\n    - [Pros of Data Parallelism](#pros-of-data-parallelism)\n    - [Cons of Data Parallelism](#cons-of-data-parallelism)\n- [torch.distributed](#torchdistributed)\n    - [Lockstep Multiprocessing](#lockstep-multiprocessing)\n    - [GPU Topology](#gpu-topology)\n    - [Your First Broadcast](#your-first-broadcast)\n    - [Distributed Logging](#distributed-logging)\n    - [FakeDistributed Class](#fakedistributed-class)\n- [Utilities](#utilities)\n    - [Your First Broadcast](#your-first-broadcast-)\n    - [Using the Correct GPU](#using-the-correct-gpu)\n- [Error Checking With `dist`](#error-checking-with-dist)\n- [Benchmarking `gloo` and `nccl`](#benchmarking-gloo-and-nccl)\n    - [Results](#results)\n- [Benchmarking Broadcast For Real](#benchmarking-broadcast-for-real)\n- [All-Reduce](#all-reduce)\n    - [Naive All-Reduce](#naive-all-reduce)\n- [Onto Part 2](#onto-part-)\n\n## Readings\n\n- [GPUs for Deep Learning](https://lambdalabs.com/blog/best-gpu-2022-sofar/): give this a skim to get a sense of how different GPUs compare to each other. Sometimes, it's more cost effective to use a smaller number of more powerful GPUs, and other times the opposite is true. Some hardware configurations of GPU+[interlink](https://www.nvidia.com/en-us/design-visualization/nvlink-bridges/) can scale close to linearly in ideal conditions, while others become bottlenecked on communication.\n- [Fast Multi-GPU collectives with NCCL](https://developer.nvidia.com/blog/fast-multi-gpu-collectives-nccl/): Introduces the `nccl` library that we'll use today. You should understand the \"ring\" algorithm for broadcasting data.\n- Optional - [PyTorch Distributed: Experiences on Accelerating Data Parallel Training](https://arxiv.org/pdf/2006.15704.pdf): this isn't necessary to understand today's material, but if you're interested in the engineering and API design aspect of the PyTorch data parallel module, this gives a lot of good detail.\n\nModern deep learning almost always requires multiple devices for training, as the scale of experiments has grown much more quickly than the capabilities of an individual device. The two basic problems with single device training are that (a) the model may not fit on one device, in which case it can't be trained at all, and (b) even if the model does fit on one device, the experiment would take a long time to run in wall clock time.\n\nHowever, we don't yet know how to optimally train models in parallel, because there are a vast number of possible configurations and it requires a lot of effort on both the software and hardware sides. As of 2022, large scale experiments like [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html) that are running on 6144 TPU v4 are able to achieve only around 46% of the potential maximum hardware utilization. This might seem low, but it reflects substantial engineering progress - 46% is already more than double GPT-3's utilization.\n\nThe problem is so hard because different parts of the system can be bottlenecked at different times on compute, GPU memory bandwidth, or communication between devices. The possibilities for dividing the computation are also combinatorial - for each operation you could think of partitioning the input among any subset of the dimensions, and then either merging the results or leaving the output partitioned for the next stage.\n\nThe three simplest parallelization strategies are:\n\n- Data parallelism: each GPU is responsible for a slice of inputs along the batch dimension.\n- Tensor parallelism: each GPU is responsible for a slice of weights (for example, the weights for a subset of attention heads).\n- Pipeline parallelism: each GPU is responsible for a layer or set of layers.\n\nThe PaLM paper combines 12-way tensor parallelism and 256-way data parallelism. This means that for a batch size of 2048 and 48 attention heads, a single device would be responsible for computing the output of 48/12 = 4 attention heads on 2048/256 = 8 training examples.\n\n## Data Parallelism\n\nWe're going to start today with data parallelism. It's not the most efficient, but it's comparatively simple to understand and implement, and can be further optimized and/or used in combination with other techniques.\n\nWe start with N identical GPUs and:\n\n- Copy identical weights to each\n- Divide our batch of size B into N \"minibatches\" of size M=B//N. We'll assume that N evenly divides B for today.\n- Each GPU runs a forward and backward pass on its minibatch to obtain the local gradients for its minibatch.\n- Synchronize the gradients with all-reduce.\n\n\"All-reduce\" means that each GPU will send and receive minibatch gradients until all devices have the same sum. This sum of gradients is exactly identical to if we'd run a single forward pass on the full batch B, with some special treatment needed around batch normalization and dropout layers.\n\nBatch normalization is a special case because it normally computes a mean over the full batch, but now each device is computing a mean over the minibatch. If you want dropout to be the same, you need to carefully initialize the random number generator on each device.\n\nAssuming the special cases are handled and all devices have an identical sum of gradients, each GPU can now independently compute an identical optimizer step, which will deterministically modify the parameters to the same result. This completes a single iteration and all the devices are still in sync.\n\n### Pros of Data Parallelism\n\nThe best thing is that any model that fits on 1 GPU can be \"wrapped\" in a data parallel version without thinking too hard about it. Because the batch elements are already independent of each other (again, except for batch norm), your devices only need to communicate once per batch at the end to sum their gradients.\n\nIn comparison, tensor parallelism and pipeline parallelism require you to send activations around during both forward and backwards passes, and there's some cleverness involved to minimize this amount of communication.\n\n### Cons of Data Parallelism\n\nOne issue is that the communication between GPUs can easily get saturated - all of the GPUs want to send data at once when summing gradients. This can be mitigated somewhat by sending gradients of the later layers immediately as they're computed, interleaved with computing gradients of the earlier layers. It's also mitigated by using special extremely fast interconnects like NVLink.\n\nAnother issue is that if your model doesn't run on 1 GPU even with a minibatch size of 1, you can't use data parallelism alone; you are forced to use one of the other two strategies, possibly combined with data parallelism.\n\nIn terms of GPU memory, data parallelism is wasteful because all the parameters are duplicated N times, as is the optimizer state. This can be mitigated by splitting the optimizer state across devices, at the cost of more communication.\n\nAs N grows, the minibatch size B/N becomes too small to fully utilize the GPU. You can increase the total batch size B somewhat to compensate, but large batches tend to generalize more poorly so there's a problem-dependent cap to increasing B.\n\nLet's get to it and build our own data parallel implementation using the `torch.distributed` package!\n\n## torch.distributed\n\nThe `torch.distributed` package (`dist` for short) provides functions to communicate between different devices, which could mean multiple CPUs/GPUs on the same computer or on different computers. If we only wanted our code to run on a single computer with multiple GPUs, we don't need `torch.distributed` at all. We could just use `tensor.to()` to move data to and from the appropriate devices, and if needed we could use threads and shared memory or multiple process for concurrent operations.\n\nToday, we're using `torch.distributed` so that everything you do would transparently scale to multiple computers connected by a network.\n\nThe actual implementation of an operation like broadcast or all-reduce is delegated to one of three backends:\n\n- [`gloo`](https://github.com/facebookincubator/gloo/tree/main/gloo) is developed by Facebook. Its advantages are that it supports both CPU and GPU, but as you'll see the GPU version is much slower than NVIDIA's `nccl`. Usually, it has better error messages than `nccl` so it can be useful to debug against `gloo` before switching to `nccl`.\n- [`nccl`](https://github.com/NVIDIA/nccl) (pronounced 'nickel') is developed by NVIDIA. It only works on NVIDIA GPUs, but it's been highly optimized and specialized to NVIDIA's GPUs and other products like [NVLink](https://www.nvidia.com/en-us/data-center/nvlink/) and NVSwitch.\n- [`mpi`](https://github.com/open-mpi/ompi) stands for Message Passing Interface. Unlike the other two, this isn't a specific library but rather an old school open standard from the 90s. This is primarily aimed at clusters with thousands of CPUs, and we won't be using this today.\n\n### Lockstep Multiprocessing\n\nThe programming paradigm with `dist` is called **lockstep multiprocessing**. It's called \"lockstep\" because whenever one process calls a communication function from the `dist` package, every process has to call that function or your program will deadlock. The multiprocessing refers to the fact that there are multiple processes on potentially different computers. For today, we'll use the common configuration where the processes each have a unique number called the `rank`, running from 0 to `world_size`. We give each process exclusive access to a single GPU.\n\nAll the processes except rank 0 are equal peers; in `dist`, rank 0 has additional responsibilities by convention. It's going to set up a key-value store that other processes can connect to and get and set values. A common way to do this is by providing the hostname and port of the machine running the rank 0 process to each other process. Today we'll do this for you as part of the utility code.\n\nThe store is an instance of [`torch.distributed.TCPStore`](https://pytorch.org/docs/stable/distributed.html#torch.distributed.TCPStore) and isn't how we actually send tensors around. It's needed to have all the processes agree on their ranks and how big `world_size` is, and to figure out the topology of the network. (TBD: can we see output of nccl doing this?)\n\nRank 0 is also used by convention in your code when you have work that only one rank needs to do. For example, it makes sense for rank 0 to save model checkpoints to disk instead of all the ranks doing it redundantly.\n\n### GPU Topology\n\nIf you're doing this on an AWS box with a `p3.16xl` instance, your 8 V100 GPUs are arranged like this:\n\n<p align=\"center\">\n    <img src=\"w3d1_p3_topology.png\"/>\n</p>\n\nThe thick arrows are NVIDIA's fast interconnects called `NVLink`. GPUs can simultaneously send and receive data at 25GB/s over each NVLink, meaning that a GPU with 6 NVLinks can send at a max of 150GB/s and simultaneously receive a max of 150GB/s. This is much faster than the PCIe and QPI connections, which are hardware dependent but let's call it 32GB/s and 20GB/s respectively.\n\nRun `nvidia-smi topo -m` at the terminal to see the topology of your machine. For example, if you have 4 V100s it might look like this:\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\nGPU0     X      NV1     NV1     NV2     0-31            N/A\nGPU1    NV1      X      NV2     NV1     0-31            N/A\nGPU2    NV1     NV2      X      NV2     0-31            N/A\nGPU3    NV2     NV1     NV2      X      0-31            N/A\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\nNV1 means there is one NVLink connecting the devices, and NV2 means there are two. NV12 is equivalent to NV6 and means that there is a bonded set of of 6 NVLinks\n\nIn the topology in the image above, suppose that GPU0 wants to `broadcast` a 1GB tensor to all the other GPUs. Doing this the naive way where GPU0 is only sending to each other GPU and each other GPU is only receiving, roughly how long will this take?\n\n<details>\n\n<summary>Solution - Naive Broadcast</summary>\n\nFirst, start the slow transfer to GPU4, GPU5, and GPU7 over the QPI (a slow connection between CPUs), which takes 3GB at 20 GB/s, or 150ms.\n\nDuring this time, you can complete the transfer to 1, 2, 3, and 6 simultaneously at the maximum bandwidth of a single NVLink, 1GB at 25 GB/s, 40ms.\n\nThe total time is then just the slow transfer total 150ms.\n\n</details>\n\nSuppose we chunk the transfer into 100 chunks of 10MB, and allow each GPU to both send and receive. Describe an efficient way to perform the broadcast and compute roughly how long it would take.\n\n<details>\n\n<summary>Solution - Efficient Broadcast</summary>\n\nA reasonable ring order would be 0, 2, 4, 6, 7, 5, 3, 1 with NVLink at each step. Then the transfer from 0->2 will complete in 1/25 s. Since each GPU can simultaneously be sending and receiving, the send from 2->4 can start after a negligible delay of 1/25/100 s. The whole operation should complete in roughly 40ms, about 4 times faster than the naive version.\n\n</details>\n\nIt's the backend's job to figure out the most efficient way to perform your broadcast given the topology, but it's your job to ensure that your operating system reports the correct topology. For example, if you're running inside a virtual machine or a container, `nccl` might see a virtual topology that doesn't reflect the real topology. If your system is running slowly, this is one thing you can check.\n\n### Your First Broadcast\n\nNow that you have an appreciation for the complexity behind the scenes of doing a broadcast, let's walk through the steps of making a broadcast at the PyTorch level.\n\nFirst, we have to launch a process for each rank that we want. We're using `torch.multiprocessing` (`mp` for short) to create the processes. You can only launch processes on the same machine with this, but this is enough for our needs and it's easy to do this from the REPL. To launch processes on multiple machines, PyTorch provides something called [`torchrun`](https://pytorch.org/docs/stable/elastic/run.html#launcher-api) but there are many other alternatives available.\n\nWithin each child process, we have to call `dist.init_process_group` first thing, which does the rendevous procedure and sets up the store. You can have multiple process groups and processes can belong to more than one group, which is useful if you're nesting multiple kinds of parallelism.\n\nOnce initialized, you can make use of `dist.get_rank()` and `dist.get_world_size()`.\n\nEach receiving process has to allocate a tensor of the appropriate size. Sometimes, you'll need to communicate this size first but we can hardcode it for demonstration purposes. Then the tensor is sent by calling `dist.broadcast(tensor, 0)`. The 0 is the \"source rank\" meaning that when rank 0 calls `broadcast(tensor, 0)`, it'll be sending the data in `tensor` and when each other rank calls `broadcast(tensor, 0)`, it'll actually be receiving data into `tensor`, overwriting whatever is currently there.\n\nIt's confusing that the same function call with the same arguments can be either a send or a receive operation based on the rank of the currently running process, but you'll get used to it.\n\n**Example**\n\nGPU 0:\n"}, {"cell_type": "code", "metadata": {}, "source": "x = torch.tensor([1])\ndist.broadcast(x, 0)\n# x = [1]\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nGPU 1:\n"}, {"cell_type": "code", "metadata": {}, "source": "x = torch.tensor([2])\ndist.broadcast(x, 0)\n# x = [1]\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Distributed Logging\n\nUsing `print` statements is problematic when you have a bunch of processes involved that could potentially be on different computers. Even on the same computer, you might not see your print statements in some scenarios because the standard output stream isn't connected to something you can see, or the prints might appear but overlapping when multiple processes print at once to the same stream. The Python `logging` module will just work, even in the distributed scenario by sending log messages over TCP.\n\nToday you can use `logger.info(message: str)` to log a message and it will append to a local file `w3d1_log.txt`. Use info for messages that you'd want to see when the model is running normally, and debug for messages that you'd only want to see if something was wrong.\n\n### FakeDistributed Class\n\nThe unit tests run against a `distributed` replacement called `FakeDistributed` which actually runs multiple threads in the same process.\n\nYou'll use this class more in Part 3, but for now this is all you need to know about it.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import logging\nimport logging.handlers\nimport os\nimport signal\nimport sys\nimport time\nimport traceback\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Union\nimport torch as t\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom matplotlib import pyplot as plt\nimport w3d1_test\n\nif not dist.is_available():\n    print(\"torch.distributed is not available - exiting.\")\n    sys.exit(0)\nIS_CI = os.getenv(\"IS_CI\")\nMAIN = __name__ == \"__main__\"\nif MAIN:\n    mp.set_start_method(\"spawn\")\ncurrent_pids: list[int] = []\nlogger = logging.getLogger()\nlogger.setLevel(\"INFO\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Utilities\n\nRead through for a general idea but don't worry about understanding every line.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def log_receiver_process(queue: mp.Queue, log_filename=\"./w3d1_log.txt\") -> None:\n    \"\"\"Dequeue records from other processes and write them to the specified file.\"\"\"\n    root = logging.getLogger()\n    handler = logging.FileHandler(log_filename, mode=\"a\")\n    formatter = logging.Formatter(fmt=\"%(asctime)s.%(msecs)03d %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n    handler.setFormatter(formatter)\n    root.addHandler(handler)\n    while True:\n        try:\n            record = queue.get()\n            logger = logging.getLogger(record.name)\n            logger.handle(record)\n        except Exception:\n            logger.exception(\"Exception in log handler\")\n            traceback.print_exc(file=sys.stderr)\n\n\ndef make_logger(log_queue: mp.Queue) -> logging.Logger:\n    h = logging.handlers.QueueHandler(log_queue)\n    logger = logging.getLogger()\n    logger.addHandler(h)\n    logger.setLevel(logging.INFO)\n    return logger\n\n\ndef rank_process(\n    rank: int, world_size: int, log_queue: mp.Queue, target_func: Callable, args: tuple, backend: str\n) -> None:\n    \"\"\"Runs in the child process. Initializes dist, the store, and the logger and calls target_func\"\"\"\n    global logger, store\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank)\n    store = dist.TCPStore(\"127.0.0.1\", 29502, world_size, rank == 0)\n    dist.init_process_group(backend, store=store, rank=rank, world_size=world_size)\n    logger = make_logger(log_queue)\n    target_func(*args)\n\n\ndef spawn_multiple(func: Callable, args: tuple, world_size: int, backend: str) -> None:\n    \"\"\"Spawns world_size processes, each with a different rank, that run func(*args).\"\"\"\n    queue = mp.Queue(-1)\n    listener = mp.Process(target=log_receiver_process, args=(queue,))\n    listener.start()\n    all_args = (world_size, queue, func, args, backend)\n    context = mp.spawn(rank_process, all_args, nprocs=world_size, join=False)\n    assert context is not None\n    current_pids.extend(context.pids())\n    while not context.join():\n        pass\n    listener.kill()\n\n\ndef kill() -> int:\n    \"\"\"Kill all child processes spawned by spawn_multiple() and not yet killed.\n\n    Similar to \"kill -9 pid\" on Linux.\n\n    Very useful when those processes are deadlocked.\n    Return the number of processes successfully killed.\"\"\"\n    killed = []\n    for pid in current_pids:\n        try:\n            os.kill(pid, signal.SIGTERM)\n        except ProcessLookupError:\n            pass\n        else:\n            killed.append(pid)\n    for pid in killed:\n        current_pids.remove(pid)\n    return len(killed)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Your First Broadcast\n\nGive it a try now! Implement `broadcast_from_0_cpu`, which will run in each process with a different value for `dist.get_rank()`.\n\nSometimes unsaved changes to your .py file may not be run when you run a code block; make sure to save your .py file before running.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def broadcast_from_0_cpu(shape: tuple, dist=dist):\n    \"\"\"Broadcast a random tensor of the given shape from rank 0 to others, and log the rank along with the sum of the tensor.\n\n    Remember that only rank 0 generates the random tensor, so use dist.get_rank() to distinguish these cases. Example:\n    If the tensor generated is [[-2, 1], [-.5, -1]], log 'Rank 27: got sum -2.5'\n\n    Args:\n    - shape: shape of the tensor to create and broadcast\n    - dist: the torch.distributed package. This argument is included for testing purposes, you should ignore it.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d1_test.test_broadcast_from_device(broadcast_from_0_cpu, device=\"cpu\")\nif MAIN and (not IS_CI):\n    world_size = min(4, os.cpu_count() or 4)\n    print(f\"Testing broadcast on {world_size} CPUs\")\n    args = ((1024, 1024),)\n    spawn_multiple(broadcast_from_0_cpu, args, world_size=world_size, backend=\"gloo\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Using the Correct GPU\n\nThere are multiple ways to ensure your processes are using the correct GPU. Setting the environment variable `CUDA_VISIBLE_DEVICES=2` in the child process means that PyTorch will ignore the existence of any device other than physical GPU 2, which it will call `cuda:0`.\n\nWe've done this for you today in the utility code, so your code literally won't be able to see any of the other GPUs and will report `t.cuda.device_count()` as 1. This prevents you from accidentally using the wrong device, which leads to all kinds of confusing bugs.\n\nImplement `broadcast_from_0_gpu`. It's the same as before except create your tensors on CUDA.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def broadcast_from_0_gpu(shape: tuple, dist=dist):\n    \"\"\"Broadcast a random tensor of the given shape from rank 0 to others, and log the rank along with the sum of the tensor.\"\"\"\n    pass\n\n\nif MAIN:\n    w3d1_test.test_broadcast_from_device(broadcast_from_0_gpu, device=\"cuda\")\nif MAIN and (not IS_CI):\n    world_size = t.cuda.device_count()\n    print(f\"Testing broadcast on {world_size} GPU\")\n    args = ((1024, 1024),)\n    spawn_multiple(broadcast_from_0_gpu, args, world_size=world_size, backend=\"gloo\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Error Checking With `dist`\n\nFirst with `gloo` and then with `nccl`, try deliberately making the receiving GPU tensor too small for the data. I've done this many times by accident. What happens?\n\n<details>\n\n<summary>Spoiler - Receiving Buffer too Small</summary>\n\nOn my machine, `gloo` raises with a relatively clear error message (although it doesn't indicate the line where the error occurred) and releases its resources.\n\n`nccl` is much worse - it silently causes all the receiving processes to hang until the timeout is reached. The default timeout is something like 30 minutes, but can be configured in the call to `init_process_group`. Interrupting the kernel and even restarting the kernel doesn't remove those processes. To fix it, interrupt the kernel, then use the `kill()` utility to remove those processes and get your GPU memory back.\n\nYou can verify that all the processes were indeed killed and GPU memory was released by running `nvidia-smi`. If, for some reason, they weren't killed, try running `pkill python`. (Warning: this command will also kill your Jupyter kernel, so make sure to save all your unsaved changes before running it.)\n\nYou can get additional debugging information from `nccl` by setting environment variables:\n"}, {"cell_type": "code", "metadata": {}, "source": "os.environ['NCCL_DEBUG'] = 'INFO'\nos.environ['NCCL_DEBUG_FILE'] = './filename.%h.%p'\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nIn my experience, this information is only occasionally useful.\n\n</details>\n\nNow try making the number of bytes correct, but the dtype incorrect. For example, make it a `float16` buffer with double the number of elements, or a `float64` with half the number of elements. What happens?\n\n<details>\n\n<summary>Spoiler - Wrong Dtype</summary>\n\nBoth `gloo` and `nccl` copy the bytes in while ignoring the discrepancy. The correct number of bytes are received, but the bytes will be interpreted as the wrong type by the receiver. If you're lucky, one of the bit patterns will be a representation of `NaN` and the sum will be `NaN`, but with decent probability you'll just get a sum of nonsense numbers.\n\n</details>\n\nFortunately, PyTorch provides a way to detect both of these issues by setting the environment variable `TORCH_DISTRIBUTED_DEBUG=DETAIL`. This does decrease performance, but when debugging it's extremely useful to have enabled.\n\n\n## Benchmarking `gloo` and `nccl`\n\nWe will now compare the performance of `gloo` and `nccl` on the broadcast operation. To do so, you will write the following:\n\n1. A benchmarking case that sets up and calls the operation to be benchmarked.\n\n2. A benchmarking function that is run on each device, and instantiates and benchmarks the provided case.\n\nThen you'll make a plot with a line for each backend showing throughput (GB/s sent by rank 0) versus the data size. Data size should range between 100KB and around 10GB. Note that you can't allocate the full amount of your GPU memory because some of it is used by PyTorch and the CUDA driver.\n\nFirst, here are some helper functions and the abstract class for the benchmarking cases. You don't need to write any code here.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def bytes(n_elems: int):\n    \"\"\"Convert number of float32 elements transfered to bytes\"\"\"\n    bytes_per_elem = 4\n    return bytes_per_elem * n_elems\n\n\ndef throughput(n_elems: int, duration: float):\n    \"\"\"Convert number of elements transferred and duration to throughput (GB/s)\"\"\"\n    bytes_to_gb = 1 / 2**30\n    return bytes(n_elems) * bytes_to_gb / duration\n\n\nclass BenchmarkCase(ABC):\n    \"\"\"\n    An abstract class representing a distributed operation to be benchmarked.\n    An instance of this class will be instantiated in each process, then the call method will be called and benchmarked.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, shape: tuple, rank: int, device: Union[t.device, str]) -> None:\n        \"\"\"Set up the test case\"\"\"\n\n    @property\n    @abstractmethod\n    def tensor(self) -> t.Tensor:\n        \"\"\"Returns the tensor used in the distributed operation\"\"\"\n\n    @abstractmethod\n    def __call__(self, dist=dist) -> None:\n        \"\"\"Call the test case. This is the function to be benchmarked.\"\"\"\n\n    @property\n    def log_info(self) -> str:\n        \"\"\"Additional info to be logged, besides the benchmark results\"\"\"\n        return f\"Tensor Sum: {self.tensor.sum()}\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Results\n\nTo get the timings back into your REPL, create a `mp.Queue` and pass it in for `result_queue`, then have process rank 0 do the timing and enqueue the results. You can use `time.perf_counter()` for a high resolution clock. To add results to the queue, use `result_queue.put(object)`, and to retrieve them use `result_queue.get()`.\n\nGotcha: when sending data with `dist.broadcast` on GPU, it will return as soon as the data has been queued up on the GPU, which doesn't mean that the receiver has gotten it. This is because GPU operations are performed [asynchronously](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution).\n\nTo ensure that the transfer is really complete, add a `t.cuda.synchronize()` after the broadcast and before stopping the timer. This will sleep until all CUDA kernels on the current device are complete.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class BenchmarkBroadcast(BenchmarkCase):\n    def __init__(self, shape: tuple, rank: int, device: Union[t.device, str]) -> None:\n        \"\"\"\n        Create a random tensor of the given shape on the rank 0 process and empty tensors on the others\n        \"\"\"\n        pass\n\n    @property\n    def tensor(self) -> t.Tensor:\n        pass\n\n    def __call__(self, dist=dist) -> None:\n        \"\"\"Broadcast a tensor from the rank 0 process to others\"\"\"\n        pass\n\n\ndef benchmark(case: type[BenchmarkCase], shapes: list[tuple], result_queue: mp.Queue, dist=dist) -> None:\n    \"\"\"\n    Benchmark the time it takes to perform a distributed operation, and log the time elapsed.\n    This function (benchmark) will be run on each spawned process.\n     - case: The class of the BenchmarkCase subclass to be benchmarked, which will be instantiated and called..\n     - shapes: a list of tensor shapes to benchmark the case for\n     - result_queue: a queue to log results to. Log results as tuples of the form (backend: String, shape: tuple, time_elapsed: float)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d1_test.test_benchmark_broadcast_single(BenchmarkBroadcast)\nif MAIN:\n    w3d1_test.test_benchmark_broadcast_multiple(BenchmarkBroadcast)\nif MAIN:\n    w3d1_test.test_benchmark(benchmark)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nWith 4 V100s, `gloo` was marginally faster for the 100K transfer and got absolutely stomped on all the rest, ranging from 70-120 times slower than `nccl`. I don't actually know why this is so bad, but I would speculate that NVLink is maybe not being used at all and the data is going over PCI Express. This means that in practice, `gloo` can't be used for deep learning on NVIDIA GPUs.\n\nFor the remainder of today, we'll be using `nccl`, but if you get any weird errors then it's worth trying `gloo` to see if you get a more informative error message.\n\n\n\n## Benchmarking Broadcast For Real\n\nThe tests above ran in the `FakeDistributed` simulator which uses multiple threads instead of multiple processes. Next run the benchmark on multiple processes for real.\n\nCreate an `mp.Queue` instance and use it to store your performance results: tuples of form `(backend, shape, time_elapsed)`. Recall that `spawn_multiple()` takes as arguments a function (here: `benchmark`), the arguments to that function, the world size, and the backend. You can use t.cuda.device_count() to access the number of GPUs.\n\nWhen all benchmark calls finish, create and populate a list `results` with popped out from your queue. This may take a minute to run; reduce the size of shapes for testing purposes as needed.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN and (not IS_CI):\n    print(\"Benchmarking broadcast on multi-GPU\")\n    shapes = [(256 * 10**power,) for power in range(2, 8)]\n    results: list[tuple[str, tuple, float]] = []\n    \"TODO: YOUR CODE HERE\"\nif MAIN and (not IS_CI):\n    assert results\n    bytes_transferred = [bytes(shape[0]) for (backend, shape, dt) in results if backend == \"gloo\"]\n    gloo_throughput = [throughput(shape[0], dt) for (backend, shape, dt) in results if backend == \"gloo\"]\n    nccl_throughput = [throughput(shape[0], dt) for (backend, shape, dt) in results if backend == \"nccl\"]\n    (fig, ax) = plt.subplots()\n    ax.loglog(bytes_transferred, gloo_throughput, label=\"gloo\")\n    ax.loglog(bytes_transferred, nccl_throughput, label=\"nccl\")\n    ax.set(xlabel=\"Bytes transferred\", ylabel=\"Throughput (GB/s)\")\n    fig.legend()\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## All-Reduce\n\nWe've seen how to send the initial parameters from rank 0 to the other processes using `broadcast`. This same procedure is also how you would send module buffers like batch norm statistics. We only need one more communication to have a working data parallel implementation, which is sending around each GPU's local gradients until each GPU has the sum of all gradients.\n\nThis operation is called \"all-reduce\", and is the communication equivalent of matrix multiply - an expensive, frequently used operation that has been heavily optimized, both in terms of programmer effort on the software side and architecture design on the hardware side. It's fair to say that we couldn't scale deep learning without efficient all-reduce.\n\n**Example**\n\nGPU 0:\n"}, {"cell_type": "code", "metadata": {}, "source": "x = torch.tensor([1])\ndist.all_reduce(x)\n# x = [3]\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nGPU 1:\n"}, {"cell_type": "code", "metadata": {}, "source": "x = torch.tensor([2])\ndist.all_reduce(x)\n# x = [3]\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n<details>\n\n<summary>Floating Point Rounding Error</summary>\n\nYour implementation should behave identically to the real one, which guarantees the sum to be identical on every rank. This requires some care due to floating point rounding error; floating point addition is not associative in general and the difference is larger in lower precision arithmetic.\n\nWill these three operations result in the same sum?\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    a = t.tensor(0.1, dtype=t.float16)\n    b = t.tensor(0.2, dtype=t.float16)\n    c = t.tensor(0.3, dtype=t.float16)\n    print((a + b) + c)\n    print((a + (b + c)))\n    print(t.tensor([0.1, 0.2, 0.3], dtype=t.float16).sum())\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "</details>\n\n### Naive All-Reduce\n\nImplement all_reduce_broadcast in a naive manner: call `broadcast` `world_size` times, then sum the results. The only constraint on the order of summation is that the sum must end up bitwise identical on every rank.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def all_reduce_broadcast(tensor: t.Tensor, dist=dist) -> None:\n    pass\n\n\nif MAIN:\n    w3d1_test.test_all_reduce_broadcast(all_reduce_broadcast)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNow, set up BenchmarkCases to compare the perfomance of your `all_reduce_broadcast` and `dist.all_reduce`.\n\nHow much slower is the naive implementation than `dist.all_reduce`?\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class BenchmarkAllReduce(BenchmarkCase):\n    \"\"\"A Benchmark case to benchmark dist.all_reduce\"\"\"\n\n    def __init__(self, shape: tuple, rank: int, device: Union[t.device, str]) -> None:\n        pass\n\n    @property\n    def tensor(self) -> t.Tensor:\n        pass\n\n    def __call__(self, dist=dist) -> None:\n        pass\n\n\nclass BenchmarkAllReduceBroadcast(BenchmarkCase):\n    \"\"\"A Benchmark case to benchmark all_reduce_broadcast\"\"\"\n\n    def __init__(self, shape: tuple, rank: int, device: Union[t.device, str]) -> None:\n        pass\n\n    @property\n    def tensor(self) -> t.Tensor:\n        pass\n\n    def __call__(self, dist=dist) -> None:\n        pass\n\n\nif MAIN:\n    w3d1_test.test_benchmark_all_reduce(BenchmarkAllReduce)\nif MAIN:\n    w3d1_test.test_benchmark_all_reduce(BenchmarkAllReduceBroadcast)\nif MAIN and (not IS_CI):\n    shapes = [(int(256 * 10**i),) for i in range(3, 7)]\n    results_dist: list[tuple[str, tuple, float]] = []\n    print(\"Benchmarking dist allreduce on multi-GPU\")\n    \"TODO: YOUR CODE HERE\"\nif MAIN and (not IS_CI):\n    shapes = [(int(256 * 10**i),) for i in range(3, 7)]\n    results_naive: list[tuple[str, tuple, float]] = []\n    print(\"Benchmarking naive broadcast-based allreduce on multi-GPU\")\n    \"TODO: YOUR CODE HERE\"\nif MAIN and (not IS_CI):\n    assert results_naive and results_dist\n    bytes_transferred = [bytes(shape[0]) for (backend, shape, dt) in results_naive]\n    naive_throughput = [throughput(shape[0], dt) for (backend, shape, dt) in results_naive]\n    dist_throughput = [throughput(shape[0], dt) for (backend, shape, dt) in results_dist]\n    (fig, ax) = plt.subplots()\n    ax.loglog(bytes_transferred, naive_throughput, label=\"naive\")\n    ax.loglog(bytes_transferred, dist_throughput, label=\"dist\")\n    ax.set(xlabel=\"Bytes transferred\", ylabel=\"Throughput (GB/s)\")\n    fig.legend()\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Onto Part 2\n\nNow you're ready for Part 2, where we'll adapt a real training loop for data parallelism and try to improve its performance.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}