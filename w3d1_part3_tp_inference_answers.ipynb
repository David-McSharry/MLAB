{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D1 Part 3 - Tensor Parallel\n\nWe've seen that data parallel training can allow you to scale training to many GPUs by dividing up the batch elements. It can be used on any model with minimal adjustments to the code.\n\nHowever, data parallel alone has limitations:\n\n- The model must be small enough that one GPU can run it with a batch size of at least 1.\n- The maximum number of GPUs you can use is limited by the batch size; if you want to do inference on a batch size of 1, data parallel won't help.\n\nIn this part we'll see how to gain speedups when data parallel can't help.\n\n## Table of Contents\n\n- [Running Large Models](#running-large-models)\n    - [Pipeline Parallel](#pipeline-parallel)\n    - [Tensor Parallel](#tensor-parallel)\n- [Tensor Parallel - Linear Layer](#tensor-parallel---linear-layer)\n- [FakeDistributed class](#fakedistributed-class)\n- [Memory Mapping](#memory-mapping)\n    - [Helper function - part()](#helper-function---part)\n    - [Linear - Split Columns](#linear---split-columns)\n    - [Linear - Split Rows](#linear---split-rows)\n- [LinearSplitRows](#linearsplitrows)\n    - [Embedding](#embedding)\n    - [UnidirectionalAttentionSplit](#unidirectionalattentionsplit)\n- [Tensor Parallel OPT](#tensor-parallel-opt)\n- [Load with OPT](#load-with-opt)\n- [Bonus](#bonus)\n    - [Float16 for LinearSplitRows](#float-for-linearsplitrows)\n    - [Optimizing the MLP](#optimizing-the-mlp)\n    - [Optimizing the Embedding](#optimizing-the-embedding)\n    - [Tensor Parallel Training](#tensor-parallel-training)\n    - [2D Parallel Training (challenging)](#d-parallel-training-challenging)\n    - [Uneven Size Partitions](#uneven-size-partitions)\n\n## Running Large Models\n\nAs of this writing, an A100 GPU in the 80GB RAM configuration is the most RAM in a single device that you can readily purchase. Large models like GPT-3 175B which have 175B * 2 bytes per `float16` = 350GB of weights cannot even be loaded on an A100 to do inference, let alone also fit optimizer state and gradients for training.\n\nObviously, it would be nice if we could just put more RAM on each device, but the engineering challenges are substantial. In fact, the 80GB A100 actually has 6x16GB memory modules totalling 96GB of RAM, but it's so difficult to make them without defects that only 5 are active on any given chip. Even the next generation H100 GPU also has 80GB of RAM in this same 5 of 6 configuration.\n\nTo solve this, there are two popular ways to partition a single instance of the model across multiple GPUs: pipeline parallelism (which we won't be implementing today) and tensor parallelism. The term \"model parallelism\" is ambiguous; sometimes it refers broadly to any technique where the model is partitioned, but sometimes it's used more narrowly as a synonym for tensor parallelism. It's worth clarifying what's intended if you hear or read this term.\n\n### Pipeline Parallel\n\nIn pipeline parallelism, we divide up the layers across GPUs. So the first GPU might compute the embedding, then the second GPU might compute the first attention layer, and so on with GPUs passing activations forward in the forward pass, and passing gradients back in the backward pass. Systems like [PipeDream](https://www.microsoft.com/en-us/research/uploads/prod/2019/08/pipedream.pdf) use this strategy, but it has some drawbacks. The implementation is quite complicated, because layers can be of greatly different sizes. Naively, most of the GPUs are idling most of the time because GPU 5 can't do anything for a given input until GPU 4 finishes. This can be optimized by splitting up the batch into smaller \"microbatches\" and streaming them through, at the cost of even more implementation complexity.\n\n### Tensor Parallel\n\nIn tensor parallelism, we divide up individual parameter tensors across GPUs. In some cases like the transformer's embedding weight, this could be necessary because the parameter itself is too large to fit on one GPU.\n\nOnce divided, each GPU does as much as possible on its own, and we insert the minimum amount of communication between GPUs needed.\n\nToday, we'll build up a tensor parallel implementation of the GPT architecture and use it to perform inference on 4 GPUs simultaneously. We will need tensor parallel versions of these layers that you've previously implemented:\n\n- Linear\n- Embedding\n- UnidirectionalAttention\n\nTo start, we'll test with a simulated multi-GPU setup and then once our code is working, move up to a real machine with multiple GPUs.\n\n\n## Tensor Parallel - Linear Layer\n\nA `Linear(in_channels, out_channels)` has a weight matrix of shape `(out_channels, in_channels)`, and the forward method computes $y = x {W}^\\intercal + b$.\n\nThe fact that the weight is transposed is an implementation detail: it means that columns of ${W}^\\intercal$ are contiguous in memory which allows faster multiplication.\n\nWe will implement two different methods for splitting up the calculation: partitioning either rows or columns of ${W}^\\intercal$ across devices. To be specific, for `Linear(3, 4)` the weight multiplication could look like this:\n\nPartition columns, concatenating results to form output:\n$$\n\\begin{equation*}\n\\begin{gather*}\n\\left[\n\\begin{array}{ccc}\nx_0 & x_1 & x_2\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc:cc}\nw_{00} & w_{01} & w_{02} & w_{03} \\\\\nw_{10} & w_{11} & w_{12} & w_{13} \\\\\nw_{20} & w_{21} & w_{22} & w_{23} \\\\\n\\end{array}\n\\right] \\\\\n\\begin{array}{cc}\n\\hspace{7.4em}\\text{\\scriptsize GPU 0}&\\hspace{2.2em} \\text{\\scriptsize GPU 1}\n\\end{array}\n\\end{gather*}\n=\n\\begin{gather*}\n\\left[\n\\begin{array}{c}\n\\sum_i w_{i0} x_i \\\\\n\\sum_i w_{i1} x_i\n\\end{array} \\\\\n\\right] \\\\\n\\left[\n\\begin{array}{c}\n\\sum_i w_{i0} x_i \\\\\n\\sum_i w_{i1} x_i\n\\end{array} \\\\\n\\right]\n\\end{gather*}\n\n\\begin{array}{c}\n\\text{\\scriptsize GPU 0} \\\\ \\\\\n\\text{\\scriptsize GPU 1}\n\\end{array}\n\n\\end{equation*}\n$$\n\nPartition rows, adding elementwise to combine contributions:\n$$\n\\begin{equation*}\n\\begin{gather*}\n\\begin{array}{c}\n\\end{array}\\\\\n\n\\left[\n\\begin{array}{cc:c}\nx_0 & x_1 & x_2\n\\end{array}\n\\right] \\\\\n\n\\begin{array}{cc}\n\\hspace{1em}\\text{\\scriptsize GPU 0} & \\text{\\scriptsize GPU 1}\n\\end{array}\n\\end{gather*}\n\n\\left[\n\\begin{array}{cccc}\nw_{00} & w_{01} & w_{02} & w_{03} \\\\\nw_{10} & w_{11} & w_{12} & w_{13} \\\\\n\\hdashline\nw_{20} & w_{21} & w_{22} & w_{23} \\\\\n\\end{array}\n\\right]\n\n\\begin{array}{c}\n\\\\[0.5pt]\n\\text{\\scriptsize GPU 0} \\\\[4pt]\n\\text{\\scriptsize GPU 1}\n\\end{array}\n\n=\n\\begin{gather*}\n\\left[\n\\begin{array}{c}\nw_{00} x_0 + w_{10} x_1 \\\\\nw_{01} x_0 + w_{11} x_1 \\\\\nw_{02} x_0 + w_{12} x_1 \\\\\nw_{03} x_0 + w_{13} x_1\n\\end{array} \\\\\n\\right] \\\\\n\\text{\\scriptsize GPU 0}\n\\end{gather*}\n+\n\\begin{gather*}\n\\left[\n\\begin{array}{c}\nw_{20} x_2 \\\\\nw_{21} x_2 \\\\\nw_{22} x_2 \\\\\nw_{23} x_2\n\\end{array} \\\\\n\\right] \\\\\n\\text{\\scriptsize GPU 1}\n\\end{gather*}\n\\end{equation*}\n$$\n\nIn the first scheme, each device needs the full input `x` and is solely responsible for a subset of the output elements. Concatenating all the subsets gives the full output.\n\nIn the second scheme, each device can take a partition of `x` and computes partial sums for every output element. Summing all the partial sums gives the full output.\n\nExercise: we've described partitioning the weight. In each scheme, how should you partition the bias?\n\n<details>\n\n<summary>Solution - Partitioning the Bias</summary>\n\nIn the first scheme, if we want each rank to have the final output for a subset, then we should partition the bias along the output dimension as well.\n\nIn the second scheme, two reasonable ideas are:\n\n- store the entire bias on rank 0 and just add it there before communicating.\n- partition the bias and have each rank add their slice at the appropriate offset.\n\nThe second way distributes the work evenly, but in practice both the storage for the bias and the computation are negligible.\n\n</details>\n\n## FakeDistributed class\n\nIt can be painful to debug when using the real `torch.distributed` module, so for testing we've provided a replacement class `FakeDistributed` which supports everything needed for today.\n\nIn particular, you can run your script in the debugger and have full visibility into what's happening on every rank, because `FakeDistributed` actually uses threads under the hood.\n\nTo keep straight whether you're using the real or fake one, I recommend `import torch.distributed as real_dist` and then explicitly pass the version you want to use to each function instead of relying on variables defined globally.\n\n## Memory Mapping\n\nThe `mmap_parameter` helper uses the operating system's \"memory mapping\" capability to load weights from disk. Going into detail of how this works is out of scope, but for today you need to know that this is fast, and that you shouldn't write to the memory map.\n\nThis is faster than using `t.load` because the bytes are already on disk in exactly the format we need them, so there's no overhead of deserialization. Deserialization is actually a bottleneck when you want to load large models, especially if you're trying to do comparisons between different large models and need to swap back and forth.\n\nMemory mapped files are lazy and only read from disk as needed. You can even mmap something that doesn't fit in CPU RAM, and if you only read slices that do fit, it will still work.\n\nEven better, when you read from the memory map, the operating system will transparently cache the chunk, and then later if you read it again it will probably hit the cache and be available at RAM speed. When running multiple processes, all of them can benefit from this cache without you having to do anything special.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import logging\nimport os\nimport pprint\nimport threading\nfrom functools import partial\nfrom typing import Callable, Optional, Type, Union\nimport torch as t\nfrom einops import rearrange\nfrom torch import nn\nfrom torch.nn import functional as F\nimport utils\nimport w2d3_part2_sampling_solution\nimport w3d1_test\nimport w3d1_utils\nfrom w2d3_part1_loading_solution import ACTIVATION_FUNCTIONS, GPTConfig, UnidirectionalAttention\nfrom w3d1_fake_distributed import AbstractDistributed, FakeDistributed\nfrom w3d1_utils import CONFIGS, DATA_ROOT, STAT_FILENAME, UniAttnWeights, init_on_device, mmap_parameter\n\nMAIN = __name__ == \"__main__\"\nHIDDEN_SIZE = 16\ntry:\n    print(\"Loading OPT data files. It's normal to see a UserWarning here.\")\n    folder = os.path.join(DATA_ROOT, \"mlab/opt-125m\")\n    test_linear_weight = mmap_parameter(folder, \"layers.0.fc1.weight\")\n    test_linear_bias = mmap_parameter(folder, \"layers.0.fc1.bias\")\n    test_qkv_weight = mmap_parameter(folder, \"layers.0.self_attn_qkv.weight\")\n    test_qkv_bias = mmap_parameter(folder, \"layers.0.self_attn_qkv.bias\")\n    test_out_proj_weight = mmap_parameter(folder, \"layers.0.self_attn.out_proj.weight\")\n    test_out_proj_bias = mmap_parameter(folder, \"layers.0.self_attn.out_proj.bias\")\n    test_embed_tokens_weight = mmap_parameter(folder, \"embed_tokens.weight\")\nexcept OSError as e:\n    print(\n        \"Failed to load OPT-125M data files. Try running w3d1_preload.py first, then contact a TA if the problem persists.\"\n    )\n    raise e\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Helper function - part()\n\nToday we're going to be slicing a lot of tensors into equally sized pieces. Implement the helper function `part` to make this more safe and concise.\n\nRecall that `slice` is a built-in type containing `start`, `stop`, and `step` fields which can be integers or `None`. Given `x=[1,2,3,4,5,6,7]`, writing `x[1:5:2]` is syntactic sugar for `x[slice(1, 5, 2)]`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def part(n: int, rank: int, world_size: int, must_be_even=True) -> slice:\n    \"\"\"For a sequence of n elements, return a slice object representing rank's partition.\n\n    must_be_even: if True, raise ValueError unless all partitions are equal length.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    assert part(10, 0, 2) == slice(0, 5)\n    assert part(10, 1, 2) == slice(5, 10)\n    x = t.rand((10, 2))\n    row_partitions = [x[part(10, 0, 2)], x[part(10, 1, 2)]]\n    utils.assert_all_equal(x, t.cat(row_partitions, 0))\n    col_partitions = [x[:, part(2, 0, 2)], x[:, part(2, 1, 2)]]\n    utils.assert_all_equal(x, t.cat(col_partitions, 1))\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Linear - Split Columns\n\nNow we will implement the first strategy in the diagram, taking care to keep all the dimensions straight.\n\nImplement `linear_split_columns` using a call to `part`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def linear_split_columns(weight: t.Tensor, bias: t.Tensor, rank: int, world_size: int) -> tuple[t.Tensor, t.Tensor]:\n    \"\"\"Given the weight and bias of a nn.Linear, return just the slice of the weight and bias needed for the specified rank.\n\n    Assume all slices are of equal size.\n\n    weight: (out_channels, in_channels)\n    bias: (out_channels,)\n\n    Return (weight_slice, bias_slice)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d1_test.test_linear_split_columns(linear_split_columns, test_linear_weight, test_linear_bias)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNext, implement the `LinearSplitColumns` module.\n\nWe're going to have one instance of this per GPU, each with a different weight slice.\nIn forward, make a call to `dist.all_gather` and then combine the results from all GPUs together.\n\nDon't read any global variables; use the provided dist from the constructor.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class LinearSplitColumns(t.nn.Module):\n    \"\"\"Input and output are exactly like nn.Linear.\n\n    Multiple distributed instances will collaborate to compute the result.\n    Note that out_channels is the TOTAL number of output channels, NOT the dimension of the weight slice.\n    \"\"\"\n\n    dist: AbstractDistributed\n    weight: nn.Parameter\n    bias: nn.Parameter\n\n    def __init__(self, in_channels: int, out_channels: int, dist: AbstractDistributed):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Compute the same y = x W^t + b as a regular Linear by collaborating.\"\"\"\n        pass\n\n\ndef child_test_linear(\n    dist: FakeDistributed,\n    module: Union[Type[\"LinearSplitColumns\"], Type[\"LinearSplitRows\"]],\n    split_fn: Callable,\n    batch_size=2,\n    seq_len=3,\n    dtype=t.float64,\n) -> None:\n    \"\"\"This function is called on each thread/process.\n\n    Using random placeholder data, it verifies that multiple modules have the same input/output as the serial version.\n    \"\"\"\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    dist.logger.info(f\"{rank}: starting\")\n    test_weight = test_linear_weight.to(dtype=dtype)\n    test_bias = test_linear_bias.to(dtype=dtype)\n    (out_channels, in_channels) = test_weight.shape\n    reference = nn.Linear(in_channels, out_channels)\n    reference.weight = nn.Parameter(test_weight)\n    reference.bias = nn.Parameter(test_bias)\n    yours = module(in_channels, out_channels, dist)\n    (w, b) = split_fn(test_weight, test_bias, rank, world_size)\n    yours.weight = nn.Parameter(w)\n    yours.register_parameter(\"bias\", b if b is None else nn.Parameter(b))\n    if rank == 0:\n        x = t.randn((batch_size, seq_len, in_channels), dtype=dtype)\n    else:\n        x = t.zeros((batch_size, seq_len, in_channels), dtype=dtype)\n    dist.broadcast(x, 0)\n    with t.inference_mode():\n        actual = yours(x)\n    expected = reference(x)\n    utils.allclose_atol(actual, expected, 1e-05)\n    dist.logger.info(f\"{rank}: Output matched serial version!\")\n\n\ndef launch_threads(target: Callable, num_threads: int, local_dist: FakeDistributed, *args, **kwargs) -> None:\n    threads = []\n    for rank in range(num_threads):\n        thread_args = (local_dist,) + args\n        thread_kwargs = dict(**kwargs, rank=rank)\n        thread_target = local_dist.with_rank(target)\n        thread = threading.Thread(\n            target=thread_target, args=thread_args, kwargs=thread_kwargs, name=f\"Rank{rank}Thread\"\n        )\n        threads.append(thread)\n        thread.start()\n    for thread in threads:\n        thread.join()\n\n\nif MAIN:\n    for num_threads in [1, 4]:\n        local_dist = FakeDistributed(world_size=num_threads)\n        local_dist.logger.info(f\"Testing LinearSplitColumns with {num_threads} ranks...\")\n        target = partial(child_test_linear, module=LinearSplitColumns, split_fn=linear_split_columns)\n        launch_threads(target, num_threads, local_dist)\n        if local_dist.exceptions:\n            local_dist.logger.error(\"One or more threads raised an exception; see the log and the exceptions field.\")\n            break\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Linear - Split Rows\n\nNow implement splitting in the other dimension. The test assumes you've either partitioned the bias evenly between ranks or put the full bias on rank 0 and None on the other ranks; if you want to use another strategy, you may need to replace the test with your own.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def linear_split_rows(\n    weight: t.Tensor, bias: t.Tensor, rank: int, world_size: int\n) -> tuple[t.Tensor, Optional[t.Tensor]]:\n    \"\"\"Given the weight and bias of a nn.Linear, return just the slice of the weight and bias needed for the specified rank.\n\n    You can assume all slices are of equal size.\n\n    weight: (out_channels, in_channels)\n    bias: (out_channels,)\n\n    Return (weight_slice, bias_slice)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d1_test.test_linear_split_rows(linear_split_rows, test_linear_weight, test_linear_bias)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## LinearSplitRows\n\nAs before, use the provided dist from the constructor.\n\nOptional exercise: which is preferable when using low-precision floats like float16, LinearSplitRows or LinearSplitColumns?\n\n<details>\n\n<summary>Solution - Numerical Stability</summary>\n\nLinearSplitColumns is as stable as the serial version, because internally F.linear will compute each term using a higher precision accumulator and only round at the end.\n\nFor LinearSplitRows, a naive implementation will round each partial sum to fp16, all-reduce, and then round the final result again to fp16, which will make the result quite different.\n\nIn the bonus section you can implement a smarter method.\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class LinearSplitRows(t.nn.Module):\n    \"\"\"Like nn.Linear, but input can be either the full input or just our partition of the input.\n\n    Multiple distributed instances will collaborate to compute the result.\n    Note that in_channels is the TOTAL number of input channels, NOT the dimension of the weight slice and similarly for out_channels.\n    \"\"\"\n\n    weight: nn.Parameter\n    bias: nn.Parameter\n    dist: AbstractDistributed\n\n    def __init__(self, in_channels: int, out_channels: int, dist: AbstractDistributed):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Compute the same y = x W^t + b as a regular Linear by collaborating.\n\n        x: shape of either (..., in_channels) or (..., in_channels // world_size)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    for num_threads in [1, 4]:\n        local_dist = FakeDistributed(world_size=num_threads)\n        local_dist.logger.info(f\"Testing LinearSplitRows with {num_threads} ranks...\")\n        target = partial(child_test_linear, module=LinearSplitRows, split_fn=linear_split_rows, dtype=t.float32)\n        launch_threads(target, num_threads, local_dist)\n        if local_dist.exceptions:\n            local_dist.logger.error(\"One or more threads raised an exception; see the log and the exceptions field.\")\n            break\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Embedding\n\nThe embedding weight is the largest single weight in the network because of the long dimension `vocab_size`, aka `num_embeddings` in `nn.Embedding`. We don't speed up our computation in wall time by splitting it, but we may need to split it anyway to spread out the memory usage.\n\nAgain, we have two possible dimensions to split on.\n\nSplitting on the `num_embeddings` dimension, it means that each device has the full embedding vector for a subset of specific tokens.\n\nSplitting on the hidden dimension means that each device has a subset of embedding dimensions for all tokens.\n\nFor now, implement it splitting on the vocabulary dimension and don't worry about doing the most efficient implementation. Your implementation should work even if `num_embeddings` doesn't evenly divide the world size.\n\n<details>\n\n<summary>I'm confused on the implementation!</summary>\n\nFrom the rank and world size, your module can compute the start and end range of tokens that it's responsible for. For example, with `world_size=2` and `num_embeddings=10`, the second rank is responsible for tokens [5, 10) and its local weight[3] is the embedding vector for token 5+3=8.\n\nA straightforward solution is to initialize a (*, embedding_dim) matrix of zeros, fill in the vectors that we are locally responsible for, and all-reduce.\n\nThis does transmit a lot of zeros over the network, but you can optimize this in the bonus section.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class EmbeddingSplitVocab(t.nn.Module):\n    \"\"\"Input and output are exactly like nn.Embedding.\n\n    Multiple distributed instances will collaborate to compute the result.\n\n    Again, num_embeddings is the total number (the vocab size), not the size of the local weight.\n    \"\"\"\n\n    weight: nn.Parameter\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, dist: AbstractDistributed):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (*)\n\n        Return (*, embedding_dim)\n        \"\"\"\n        pass\n\n\ndef embedding_split_vocab(weight: t.Tensor, rank: int, world_size: int) -> t.Tensor:\n    \"\"\"Given the weight of a nn.Embedding, return just the slice of the weight needed for the specified rank.\n\n    Assume all slices are of equal size.\n\n    weight: (in_channels, out_channels) aka (vocab_size, embedding_size)\n\n    Return weight_slice\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d1_test.test_embedding_split_vocab(embedding_split_vocab, test_embed_tokens_weight[:, :10])\n\n\ndef child_test_embedding(\n    dist: FakeDistributed, module: Type[EmbeddingSplitVocab], split_fn: Callable, batch_size=20, seq_len=30\n) -> None:\n    \"\"\"This function is called on each thread/process.\n\n    Using random placeholder data, it verifies that multiple modules have the same input/output as the serial version.\n    \"\"\"\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    dist.logger.info(f\"{rank}: starting\")\n    small_embed = test_embed_tokens_weight[:, :10]\n    (num_embeddings, embedding_dim) = small_embed.shape\n    reference = nn.Embedding(num_embeddings, embedding_dim)\n    reference.weight = nn.Parameter(small_embed)\n    yours = module(num_embeddings, embedding_dim, dist)\n    w = split_fn(small_embed, rank, world_size)\n    yours.weight = nn.Parameter(w)\n    if rank == 0:\n        x = t.randint(0, num_embeddings, (batch_size, seq_len), dtype=t.int64)\n    else:\n        x = t.zeros((batch_size, seq_len), dtype=t.int64)\n    dist.broadcast(x, 0)\n    with t.inference_mode():\n        actual = yours(x)\n    expected = reference(x)\n    utils.allclose_atol(actual, expected, 1e-05)\n    dist.logger.info(f\"{rank}: Output matched serial version!\")\n\n\nif MAIN:\n    for num_threads in [1, 4]:\n        local_dist = FakeDistributed(world_size=num_threads)\n        local_dist.logger.info(f\"Testing EmbeddingSplitVocab with {num_threads} ranks...\")\n        target = partial(child_test_embedding, module=EmbeddingSplitVocab, split_fn=embedding_split_vocab)\n        launch_threads(target, num_threads, local_dist)\n        if local_dist.exceptions:\n            local_dist.logger.error(\"One or more threads raised an exception; see the log and the exceptions field.\")\n            break\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### UnidirectionalAttentionSplit\n\nWe could implement self-attention in terms of our tensor parallel Linear layers, but this uses two communication calls (QKV and O).\n\nExercise: how can we partition the weights to do just one communication call?\n\n<details>\n\n<summary>Solution - UnidirectionalAttention weight partition</summary>\n\nEach attention head already produces its output independently without communication to the other heads.\n\nThis means that a natural way to partition is along the head dimension. We can just reuse a regular UnidirectionalAttention instance with a subset of heads inside, and then an all-reduce at the end to sum the outputs from each subset.\n\n</details>\n\n<details>\n\n<summary>Unidirectional bias partition</summary>\n\nAgain, the bias uses so little compute that it doesn't matter much what we do as long as it's correct.\n\nTo reuse our UnidirectionalAttention forward method without changes, we could put the full bias tensor on rank 0 and use None on the other ranks.\n\n</details>\n\n<details>\n\n<summary>My shapes aren't matching up and I'm confused!</summary>\n\nIn the serial version, it was safe to assume that `num_heads * head_size == hidden_size` and use these two expressions interchangeably. We're still going to assume this for the full model, but on the partitions this no longer holds.\n\nNow that we are dividing up the `num_heads`, on each rank have a new term `local_num_heads = num_heads // world_size`. Check that you aren't using `hidden_size` when you really need `local_num_heads * head_size` or vice versa.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import math\nfrom einops import rearrange\nfrom fancy_einsum import einsum\n\n\nclass UnidirectionalAttentionSplit(t.nn.Module):\n    \"\"\"Input and output are exactly like UnidirectionalAttention.\n\n    Multiple distributed instances will collaborate to compute the result.\n\n    Again, constructor parameters are the total size, not the local size. Assume the number of heads evenly divides the world size.\n    \"\"\"\n\n    inner: UnidirectionalAttention\n\n    def __init__(self, hidden_size: int, num_heads: int, dist: AbstractDistributed, dropout=0.0):\n        assert hidden_size % num_heads == 0\n        pass\n\n    def forward(self, x: t.Tensor, cache=None):\n        pass\n\n\ndef uni_split_heads(weights: UniAttnWeights, rank: int, world_size: int) -> UniAttnWeights:\n    \"\"\"Split the QKV and output_proj by attention head.\n\n    As in GPT, the qkv_weight consists of the Q, K, and V parts concatenated in that order.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    attn_weights = UniAttnWeights(12, 768, test_qkv_weight, test_qkv_bias, test_out_proj_weight, test_out_proj_bias)\n    w3d1_test.test_uni_split_heads(uni_split_heads, attn_weights)\n\n\ndef child_test_attn(\n    dist: FakeDistributed,\n    module: Type[UnidirectionalAttentionSplit],\n    split_fn: Callable,\n    batch_size=2,\n    seq_len=3,\n    num_heads=12,\n    hidden_size=768,\n    dtype=t.float64,\n) -> None:\n    \"\"\"This function is called on each thread/process.\n\n    Using random placeholder data, it verifies that multiple modules have the same input/output as the serial version.\n    \"\"\"\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n    dist.logger.info(f\"{rank}: starting\")\n    weights = UniAttnWeights(\n        num_heads, hidden_size, test_qkv_weight, test_qkv_bias, test_out_proj_weight, test_out_proj_bias\n    ).to(dtype=dtype)\n    assert weights.qkv_bias is not None\n    assert weights.out_proj_bias is not None\n    reference = UnidirectionalAttention(hidden_size, num_heads, dropout=0.0)\n    reference.qkv_proj.weight = nn.Parameter(weights.qkv_weight)\n    reference.qkv_proj.bias = nn.Parameter(weights.qkv_bias)\n    reference.output_proj.weight = nn.Parameter(weights.out_proj_weight)\n    reference.output_proj.bias = nn.Parameter(weights.out_proj_bias)\n    yours = module(hidden_size, num_heads, dist)\n    split_w: UniAttnWeights = split_fn(weights, rank, world_size)\n    yours.inner.qkv_proj.weight = nn.Parameter(split_w.qkv_weight)\n    b = split_w.qkv_bias\n    yours.inner.qkv_proj.register_parameter(\"bias\", b if b is None else nn.Parameter(b))\n    yours.inner.output_proj.weight = nn.Parameter(split_w.out_proj_weight)\n    ob = split_w.out_proj_bias\n    yours.inner.output_proj.register_parameter(\"bias\", ob if ob is None else nn.Parameter(ob))\n    if rank == 0:\n        x = t.randn((batch_size, seq_len, hidden_size), dtype=dtype)\n    else:\n        x = t.zeros((batch_size, seq_len, hidden_size), dtype=dtype)\n    dist.broadcast(x, 0)\n    with t.inference_mode():\n        actual = yours(x)\n    expected = reference(x)\n    utils.allclose_atol(actual, expected, 1e-05)\n    dist.logger.info(f\"{rank}: Output matched serial version!\")\n\n\nif MAIN:\n    for num_threads in [1, 4]:\n        local_dist = FakeDistributed(world_size=num_threads)\n        local_dist.logger.info(f\"Testing UnidirectionalAttentionSplit with {num_threads} ranks...\")\n        target = partial(child_test_attn, module=UnidirectionalAttentionSplit, split_fn=uni_split_heads)\n        launch_threads(target, num_threads, local_dist)\n        if local_dist.exceptions:\n            local_dist.logger.error(\"One or more threads raised an exception; see the log and the exceptions field.\")\n            break\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Tensor Parallel OPT\n\nOPT has the same architecture as GPT with some minor tweaks like using ReLU instead of GeLU. We've provided a model definition which is the same as your GPT, except with our new tensor parallel layers.\n\nNote that since the embedding is split, the unembedding operation needs an `all_gather`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class GPT2Block_TensorParallel(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        dropout: float,\n        layer_norm_epsilon: float,\n        activation_function: str,\n        dist: AbstractDistributed,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n        self.attn = UnidirectionalAttentionSplit(hidden_size, num_heads, dist, dropout=dropout)\n        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n        self.linear1 = LinearSplitColumns(hidden_size, hidden_size * 4, dist)\n        self.nonlinearity = ACTIVATION_FUNCTIONS[activation_function]\n        self.linear2 = LinearSplitColumns(hidden_size * 4, hidden_size, dist)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: t.Tensor, cache=None) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, seq, hidden_size)\n\n        Return: shape (batch, seq, hidden_size)\n        \"\"\"\n        pass\n\n\nclass GPT2_TensorParallel(nn.Module):\n    def __init__(self, config: GPTConfig, dist: AbstractDistributed):\n        super().__init__()\n        self.config = config\n        self.vocab_size = config.vocab_size\n        self.dist = dist\n        self.token_embedding = EmbeddingSplitVocab(config.vocab_size, config.hidden_size, dist)\n        self.pos_embedding = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.dropout = nn.Dropout(config.dropout)\n        self.blocks: utils.StaticModuleList[GPT2Block_TensorParallel] = utils.StaticModuleList(\n            [\n                GPT2Block_TensorParallel(\n                    config.hidden_size,\n                    config.num_heads,\n                    config.dropout,\n                    config.layer_norm_epsilon,\n                    config.activation_function,\n                    dist,\n                )\n                for _ in range(config.num_layers)\n            ]\n        )\n        self.ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n\n    def forward(self, x: t.Tensor, cache=None) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, seq), dtype t.int64 - the token ids\n\n        Return: shape (batch, seq, vocab_size), dtype t.float32- the output logits\n        \"\"\"\n        pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Load with OPT\n\nThe provided `fast_load_gpt` does weight loading, and the provided `child_run_opt` runs the model in multiple threads.\n\nModify these two functions if necessary, and then test your OPT on successively more realistic cases:\n\n- `FakeDistributed` and CPU\n- `FakeDistributed` and multiple GPUs\n- Real `torch.distributed`\n\nPlay around and have fun with your OPT! You should be able to use your sampling code from GPT day, though it may create tensors on the wrong devices.\n\n- For models that do fit on one GPU, do you see a benefit in forward pass speed? What if we have larger batch sizes like a beam search?\n- How big a model can you load now? `w3d1_preload` should have prepared weight files for various model sizes. You can also try loading even larger models by modifying and re-running `w3d1_preload`.\n- How does speed compare between using multiple processes with `torch.distributed` compared to multiple threads with `FakeDistributed`?\n- Did your cache implementation \"just work\" or did it require modifications?\n\n\n<details>\n\n<summary>I'm getting 'RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half''</summary>\n\nHalf means float16, and this means on your installation, LayerNorm doesn't support float16 on whatever device you're using. If you're testing on CPU, you can cast your model to float32.\n\n</details>\n\n<details>\n\n<summary>What is this meta device thing? Do I need to understand it?</summary>\n\nYou don't have to understand it, but it's an interesting and underdocumented feature of PyTorch that has a few use cases.\n\nWhen a tensor has a device of \"meta\", it still has a shape, but no underlying storage. Operations on a storage are defined to produce another meta tensor with the correct output shape, but are otherwise a no-op.\n\nThe reason we're using these today is that by default, running the constructor of PyTorch's built-in `nn.Module` subclasses will cause random weight initialization to occur, and this ends up being surprisingly expensive on larger models and making the model take a long time to load.\n\nIf you're trying to load a model using `from_pretrained` and see your CPU pegged at 100% and your disk idling, this is probably what is happening.\n\nBy initializing the module on the meta device instead, the RNG operations become a no-op and we avoid spending a bunch of time generating random initial weights that have no purpose (since we immediately overwrite them with our pretrained weights).\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def fast_load_gpt(gpt: GPT2_TensorParallel, folder: str, device=t.device(\"cuda:0\"), dist=None) -> GPT2_TensorParallel:\n    \"\"\"Rapidly load a OPT model from parameters saved with w3d1_utils.save_state_dict.\"\"\"\n    if dist is None:\n        rank = 0\n        world_size = 1\n        print(\"Loading GPT with dist = None\")\n    else:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n        dist.logger.info(f\"{rank}: loading GPT with world_size {world_size} and dist = {dist}\")\n    gpt.requires_grad_(False)\n\n    def load_weight_bias(dest: Union[nn.Linear, nn.Embedding, nn.LayerNorm], param_prefix: str) -> None:\n        \"\"\"Load weights and biases for a module that has identical copies on each rank (not split).\"\"\"\n        src_weight = mmap_parameter(folder, f\"{param_prefix}.weight\")\n        if src_weight.shape != dest.weight.shape:\n            raise ValueError(f\"Src: {src_weight.shape}, Dest: {dest.weight.shape}\")\n        init_on_device(dest, \"weight\", src_weight.to(device))\n        bias = getattr(dest, \"bias\", None)\n        if bias is not None:\n            src_bias = mmap_parameter(folder, f\"{param_prefix}.bias\")\n            if bias.shape != src_bias.shape:\n                raise ValueError(f\"Src: {src_bias.shape}, Dest: {bias.shape}\")\n            init_on_device(dest, \"bias\", src_bias.to(device))\n\n    def load_embedding_split_vocab(dest: EmbeddingSplitVocab, param_prefix: str) -> None:\n        src_weight = mmap_parameter(folder, f\"{param_prefix}.weight\")\n        split_weight = embedding_split_vocab(src_weight, rank, world_size)\n        init_on_device(dest, \"weight\", split_weight.to(device))\n\n    def load_fc_columns(dest: LinearSplitColumns, param_prefix: str) -> None:\n        src_weight = mmap_parameter(folder, f\"{param_prefix}.weight\")\n        src_bias = mmap_parameter(folder, f\"{param_prefix}.bias\").to(device)\n        (split_weight, split_bias) = linear_split_columns(src_weight, src_bias, rank, world_size)\n        init_on_device(dest, \"weight\", split_weight.to(device))\n        init_on_device(dest, \"bias\", split_bias.to(device))\n\n    def load_attn_split_heads(dest: UnidirectionalAttention, param_prefix: str) -> None:\n        src_qkv_weight = mmap_parameter(folder, f\"{param_prefix}.self_attn_qkv.weight\")\n        src_o_weight = mmap_parameter(folder, f\"{param_prefix}.self_attn.out_proj.weight\")\n        src_qkv_bias = mmap_parameter(folder, f\"{param_prefix}.self_attn_qkv.bias\")\n        src_o_bias = mmap_parameter(folder, f\"{param_prefix}.self_attn.out_proj.bias\")\n        src_weights = UniAttnWeights(\n            gpt.config.num_heads, gpt.config.hidden_size, src_qkv_weight, src_qkv_bias, src_o_weight, src_o_bias\n        )\n        split_weights = uni_split_heads(src_weights, rank, world_size)\n        init_on_device(dest.qkv_proj, \"weight\", split_weights.qkv_weight.to(device))\n        if split_weights.qkv_bias is not None:\n            init_on_device(dest.qkv_proj, \"bias\", split_weights.qkv_bias.to(device))\n        init_on_device(dest.output_proj, \"weight\", split_weights.out_proj_weight.to(device))\n        if dest.output_proj is not None:\n            if split_weights.out_proj_bias is not None:\n                init_on_device(dest.output_proj, \"bias\", split_weights.out_proj_bias.to(device))\n            else:\n                dest.output_proj.register_parameter(\"bias\", None)\n\n    load_embedding_split_vocab(gpt.token_embedding, \"embed_tokens\")\n    load_weight_bias(gpt.pos_embedding, \"embed_positions\")\n    load_weight_bias(gpt.ln, \"final_layer_norm\")\n    for (i, my_block) in enumerate(gpt.blocks):\n        load_weight_bias(my_block.ln1, f\"layers.{i}.self_attn_layer_norm\")\n        load_attn_split_heads(my_block.attn.inner, f\"layers.{i}\")\n        load_weight_bias(my_block.ln2, f\"layers.{i}.final_layer_norm\")\n        load_fc_columns(my_block.linear1, f\"layers.{i}.fc1\")\n        load_fc_columns(my_block.linear2, f\"layers.{i}.fc2\")\n    for (name, p) in gpt.named_parameters():\n        if p.device == \"meta\":\n            print(\"WARNING: still on meta device: \", name)\n        if p.device != device:\n            print(\"WARNING: Did not send to device: \", name)\n    return gpt\n\n\ndef child_run_opt(dist: FakeDistributed, devices: list[str], opt_name: str, prompt: str) -> None:\n    \"\"\"Load OPT in multiple threads and run inference.\"\"\"\n    config = CONFIGS[opt_name]\n    rank = dist.get_rank()\n    local_device = t.device(devices[rank])\n    folder = w3d1_utils.mlab_weight_dir(opt_name)\n    with w3d1_utils.init_on_meta_device():\n        model = GPT2_TensorParallel(config, dist)\n    fast_load_gpt(model, folder, device=local_device, dist=dist)\n    if local_device.type == \"cpu\":\n        print(\"Running on CPU: converting to float32 to avoid LayerNormKernelImpl not implemented for Half error\")\n        model = model.float()\n    tokenizer = w3d1_utils.get_tokenizer(opt_name)\n    local_device = next(model.parameters()).device\n    out = w2d3_part2_sampling_solution.sample_tokens(model, tokenizer, prompt, temperature=0)\n    dist.logger.info(f\"{rank}: {opt_name} said: {out}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nFor reference, when prompted with w3d1_utils.DEFAULT_PROMPT, my OPT-125m said:\n\n\"I'm not conscious. I'm just trying to get a better understanding of what's going on.\nI'm sorry, but I'm not\"\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def run_opt(devices: list[str]) -> None:\n    num_threads = len(devices)\n    local_dist = FakeDistributed(world_size=num_threads)\n    local_dist.logger.setLevel(logging.INFO)\n    local_dist.logger.info(f\"Testing OPT with {opt_name} and {num_threads} ranks...\")\n    target = partial(child_run_opt, devices=devices, opt_name=opt_name, prompt=w3d1_utils.DEFAULT_PROMPT)\n    launch_threads(target, num_threads, local_dist)\n\n\nif MAIN:\n    print(\"Models available: \")\n    pprint.pprint(CONFIGS)\n    opt_name = list(CONFIGS)[0]\n    print(\"Testing FakeDistributed on 1 CPU\")\n    run_opt([\"cpu\"])\nif MAIN:\n    print(\"Testing FakeDistributed on the below devices: \")\n    devices = [\"cuda:0\", \"cuda:0\", \"cuda:0\", \"cuda:0\"]\n    run_opt(devices)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n## Bonus\n\nCongratulations on completing the day's main content!\n\n### Float16 for LinearSplitRows\n\nGo back and try to get your LinearSplitRows implementation to produce the same (or at least closer) results when float16 is used for the weights and inputs.\n\n### Optimizing the MLP\n\nIn the MLP, we have the sequence `Linear` -> `GELU` or `RELU` -> `Linear`. Replacing these with our tensor parallel `Linear`, we have a `all_reduce` or `all_gather` as part of each `Linear`.\n\nFind a way to eliminate the first communication completely and only sync at the end of the MLP.\n\n<details>\n\n<summary>Solution - Optimal Splits</summary>\n\nIf we split on columns first, then each device has a subset of complete outputs. Instead of communicating, we can just apply GELU to the complete outputs and feed the resulting subset directly into a LinearSplitRows, which is capable of accepting a subset as input.\n\nIf your LinearSplitRows isn't numerically stable enough, you can work in float32 or float64 for this.\n\n</details>\n\n### Optimizing the Embedding\n\nExplore optimizing the partition along the vocabulary dimension, or different approaches like partitioning along the embedding dimension. What's the fastest way to do it?\n\n### Tensor Parallel Training\n\nWe focused on inference today and left out some details that would be required to do training with tensor parallel. Primarily. we didn't implement backpropagation across GPUs.\n\nIn the backwards pass, you'll need to communicate the gradients to the appropriate places. One way to do this is by writing a [torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function).\n\n\n### 2D Parallel Training (challenging)\n\nYou can \"nest\" tensor parallel inside of data parallel to combine their benefits. Suppose each tensor was split up on 2 GPUs with tensor parallel - then from the perspective of data parallel, that 2-GPU unit is just a single logical device.\n\nThe implementation is tricky:\n\n- Instead of just using the rank, derive a \"dp_rank\" in {0, 1} and \"tp_rank\" in {0, 1} from the rank.\n- Communications aren't all-to-all anymore, as the tensor parallel communications only need to happen within the pair. You can do this in various ways, and may want to use [process groups](https://pytorch.org/docs/stable/distributed.html#groups), though FakeDistributed doesn't implement these yet.\n- It's often helpful when testing parallelism to generate a small set of random training data and see if you can get the model to memorize the labels.\n\n### Uneven Size Partitions\n\nFor simplicity, we assumed that all our partitions were of equal size. Usually, we try to make our model dimensions and number of devices such that this works out, but it's not always possible.\n\nGo through and test if your code works if the partitions are unequal. We can always partition so that only the last partition might be smaller. For example, with 3 GPUs and 10 rows, the partitions would be [4, 4, 2].\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}