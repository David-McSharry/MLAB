{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D1 - Parallelism, Part 2\n\nBelow is a training loop for a ResNet variant on CIFAR10. It's already been optimized with some fancy tricks and on my machine trains to 94% accuracy in 30s (or around 18 seconds on an A100). Let's see how much faster we can get it by using multiple GPUs!\n\n## Table of Contents\n\n- [Parallelizing the Training Loop](#parallelizing-the-training-loop)\n- [Onto Part 3](#onto-part-)\n- [Bonus](#bonus)\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import collections\nimport copy\nimport os\nimport sys\nimport time\nfrom dataclasses import asdict, dataclass\nimport torch\nimport torch as t\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport w3d1_model\nimport wandb\nfrom w3d1_utils import CustomOptimizer, data_augmentation, load_cifar10, update_ema, validation\n\nIS_CI = os.getenv(\"IS_CI\")\nif IS_CI:\n    sys.exit(0)\n\n\n@dataclass(frozen=True)\nclass Config:\n    epochs: int\n    batch_size: int\n    momentum: float\n    weight_decay: float\n    weight_decay_bias: float\n    ema_update_freq: int\n\n\nDEFAULT_CONFIG = Config(\n    epochs=10, batch_size=256, momentum=0.9, weight_decay=0.256, weight_decay_bias=0.004, ema_update_freq=5\n)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Parallelizing the Training Loop\n\nModify the training loop so it can either run using `dist` or without it. The function `dist.is_initialized()` can be used to tell you if you're using `dist`.\n\nYou shouldn't need to modify `w3d1_utils` or `w3d1_model`, just the `train` and `rank_process` functions. Suggested order of operations:\n\n- Implement `rank_process` to perform the necessary `dist` initialization boilerplate before calling train().\n- Handle the data loading in a manner of your choice. The current implementation loads the entire training set to GPU memory, which saves time transferring it on every epoch. Your solution should be at least as performant.\n    - A simple starting point is: each rank loads all the data to CPU, preprocesses it, and then puts a slice of it in GPU memory where the slice depends on the rank. Then each rank can shuffle and iterate through its own slice without ever having to communicate training data.\n    - Better is to preprocess once and save the slices in separate files, so each process can load only the file(s) that contain data for its slice.\n- Ensure the models start out in an identical state on each rank.\n    - Method 1: compute the initial state on rank 0 and broadcast the parameters to all the others.\n    - Method 2: seed the RNG the same on each rank, so that the random initialization is the same. In this case, you'll need to take care of the `first_layer_weights` parameter whose initialization is based on training data statistics.\n- Add `dist.all_reduce` after `backward` to sum up the gradients.\n- Ensure your code is clear about what is a batch versus a minibatch.\n\nA common gotcha is that if different processes have a different number of minibatches due to unequal division of the training data, you'll get out of sync at the end of the first epoch. Watch out for this and consider logging the data size and number of minibatches on each process.\n\nNote that the best hyperparameters depends on the number of GPUs. For example, I found it beneficial to increase the batch size with the GPU count.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def train(config: Config, is_leader=True, seed=0, wandb_mode=\"disabled\") -> None:\n    batch_size = config.batch_size\n    rank = 0\n    \"TODO: YOUR CODE HERE\"\n    if is_leader:\n        wandb.init(project=\"w1d4\", config=asdict(config), mode=wandb_mode)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dtype = torch.float16 if device.type != \"cpu\" else torch.float32\n    start_time = time.perf_counter()\n    torch.manual_seed(seed + rank)\n    torch.backends.cudnn.benchmark = True\n    (train_data, train_targets, valid_data, valid_targets) = load_cifar10(device, dtype)\n    first_layer_weights = w3d1_model.patch_whitening(train_data[:10000, :, 4:-4, 4:-4])\n    train_model = w3d1_model.ResNetBagOfTricks(first_layer_weights, c_in=3, c_out=10, scale_out=0.125)\n    train_model.to(dtype)\n    for module in train_model.modules():\n        if isinstance(module, nn.BatchNorm2d):\n            module.float()\n    train_model.train(True)\n    train_model.to(device)\n    if is_leader:\n        valid_model = copy.deepcopy(train_model)\n    \"TODO: YOUR CODE HERE\"\n    optimizer = CustomOptimizer(\n        train_model.parameters(), config.momentum, config.weight_decay, config.weight_decay_bias\n    )\n    if is_leader:\n        print(f\"Preprocessing: {time.perf_counter() - start_time:.2f} seconds\")\n    train_time = 0.0\n    batch_count = 0\n    valid_acc = 0.0\n    \"TODO: YOUR CODE HERE\"\n    if is_leader:\n        print(\"\\nepoch    batch    train time [sec]    validation accuracy\")\n    for epoch in range(1, config.epochs + 1):\n        start_time = time.perf_counter()\n        indices = torch.randperm(len(train_data), device=device)\n        data = data_augmentation(train_data[indices], batch_size)\n        targets = train_targets[indices]\n        for i in range(0, len(data), batch_size):\n            if i + batch_size > len(data):\n                break\n            inputs = data[i : i + batch_size]\n            target = targets[i : i + batch_size]\n            batch_count += 1\n            train_model.zero_grad()\n            logits = train_model(inputs)\n            loss = w3d1_model.label_smoothing_loss(logits, target, alpha=0.2)\n            loss_sum = loss.sum()\n            loss_sum.backward()\n            \"TODO: YOUR CODE HERE\"\n            optimizer.step()\n            if is_leader and i // batch_size % config.ema_update_freq == 0:\n                update_ema(train_model, valid_model, 0.99**config.ema_update_freq)\n        train_time += time.perf_counter() - start_time\n        if is_leader:\n            valid_acc = validation(valid_model, valid_data, valid_targets, batch_size, epoch, batch_count, train_time)\n            wandb.log(\n                dict(\n                    elapsed=train_time,\n                    train_loss=loss_sum,\n                    val_acc=valid_acc,\n                    epoch=epoch,\n                    examples_seen=batch_count * batch_size,\n                )\n            )\n    if is_leader:\n        print(\"Final validation accuracy: \", valid_acc)\n        wandb.finish()\n\n\ndef rank_process(rank: int, world_size: int):\n    \"\"\"Initialize the store, process group, and call train with the default config.\"\"\"\n    pass\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"n_gpu\", type=int, nargs=\"?\", default=1)\n    args = parser.parse_args()\n    world_size = args.n_gpu\n    if world_size == 1:\n        train(DEFAULT_CONFIG, is_leader=True)\n    else:\n        mp.spawn(rank_process, args=(world_size,), nprocs=world_size, join=True)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Onto Part 3\n\nCongratulations! At this point you have a correct implementation of data parallelism that provides some speedup in wall-clock time over a single GPU, without having to touch the model at all.\n\nMy un-optimized solution was able to improve from 30 seconds on 1 V100 GPU to 22 seconds on 2 V100 GPU, but 4 V100 still took 22 seconds, which indicates that there wasn't enough work to saturate 4 GPUs. However, I was able reach 11 seconds on 4 V100 by doubling the batch size without significantly reducing accuracy. A 2.7x speedup on 4 GPU is respectable, but it's possible to do substantially better.\n\nI recommend moving onto Part 3 now, but if you have time at the end of the day, come back to this part and see how much you can improve!\n\n## Bonus\n\n<details>\n\n<summary>Bonus - Ideas for Optimization</summary>\n\nTo go faster, one optimization is to start communicating gradients as soon as they're available, without waiting for `backward` to complete. This interleaving can hide some of the communication latency. You could also perform the optimizer step for a parameter as soon as the all_reduce is done, instead of waiting for all the grads to be available.\n\nAnother optimization is that many small calls to `all_reduce` have substantial overhead. You can try `all_reduce_coalesced` instead.\n\nAs well as improving wall clock time, you can improve memory usage. Read about the [ZeroRedundancyOptimizer](https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html) and try to do the same sharding of the optimizer states.\n\nAnother cool idea is to use lossy compression on the gradients to reduce the amount of data that needs to be communicated. The [PowerSGD](https://arxiv.org/pdf/1905.13727.pdf) paper explores this.\n\n</details>\n\n<details>\n\n<summary>Bonus - DistributedDataParallel API</summary>\n\nIn the actual PyTorch, the class [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) (DDP for short) encapsulates all the distributed code to further reduce the changes needed in the user's training loop. In the simplest case, you can just wrap the model object in a DDP object and that's it.\n\nThis is intended to make things very easy for the user, but is actually sometimes slower than a more intrusive solution. Refer to the optional reading in Part 1 and get an idea of how DDP is implemented. If it sounds interesting, try implementing parts of it yourself.\n\n</details>\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}