{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# Diffusion Models For Image Generation\n\nToday you're going to implement and train a tiny diffusion model from scratch on FashionMNIST. Specifically, we'll be following the [2020 paper **Denoising Diffusion Probabilistic Models**](https://arxiv.org/pdf/2006.11239.pdf), which was an influential early paper in the field of realistic image generation. Understanding this paper will give you a solid foundation to understand state of the art diffusion models. I personally believe that diffusion models are an exciting research area and will make other methods like GANs obsolete in the coming years. To get a sense of how diffusion works, you'll first implement and train an even tinier model to generate images of color gradients.\n\nThe material is divided into three parts. In Part 1 we'll implement the actual equations for diffusion and train a basic model to generate color gradient images. In Part 2 we'll implement the U-Net architecture, which is a spicy mix of convolutions, MLPs, and attention all in one network. In Part 3, we'll train the U-Net architecture on FashionMNIST.\n\nYou're getting to be experts at implementing things, so today will be a less guided experience where you'll have to refer to the paper as you go. Don't worry about following all the math - the math might look intimidating but it's actually either less complicated than it sounds, or can safely be skipped over for the time being.\n\nDiffusion models are an area of active research with rapid developments occurring - our goal today is to conceptually understand all the moving parts and how they fit together into a working system. It's also to understand all the different names for things - part of the difficulty is that diffusion models can be understood mathematically from different perspectives and since these are relatively new, different people use different terminology to refer to the same thing.\n\nOnce you understand today's material, you'll have a solid foundation for understanding state of the art systems involving diffusion models like:\n\n- [GLIDE](https://arxiv.org/abs/2112.10741)\n- [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI\n- [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg\n- [ImageGen](https://imagen.research.google/) by Google Brain.\n\n## Table of Contents\n\n- [What Even Is Diffusion?](#what-even-is-diffusion)\n- [Today's Plan:](#todays-plan)\n    - [Image Processing](#image-processing)\n    - [Normalization](#normalization)\n    - [Variance Schedule](#variance-schedule)\n    - [Forward (q) function - Equation 2](#forward-q-function---equation-)\n    - [Forward (q) function - Equation 4](#forward-q-function---equation-)\n    - [Training Loop](#training-loop)\n- [Sampling from the Model](#sampling-from-the-model)\n\n## What Even Is Diffusion?\n\nWe're going to start by thinking about the input distribution of images. [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) is a dataset of 60K training examples and 10K test examples that belong to one of 10 different classes like \"t-shirt\" or \"sandal\". Each image is 28x28 pixels and in 8-bit grayscale. We think of those dataset examples as being samples drawn IID from some larger input distribution \"the set of all FashionMNIST images\".\n\nOne way to think about the input distribution is a mapping from each of the $256^{28*28}$ grayscale images to the probability that the image would be collected if we collected more training examples via an identical process as was used to obtain the 60K training examples.\n\nOur goal in generative modeling is to take this input distribution and learn a very rough estimate of the probability in various regions. It should be near zero for images that look like random noise, and also near zero for a picture of a truck since that isn't part of the concept of \"the set of all FashionMNIST images\".\n\nFor our training examples, the fact that they were already sampled is evidence that their probability should be pretty high, but we only have information on 60K examples which is really not a lot compared to $256^{28*28}$. To have any hope of mapping out this space, we need to make some assumptions.\n\nThe assumption behind the forward process is that if we add Gaussian noise to an image from the distribution, on average this makes the noised image less likely to belong to the distribution. This isn't guaranteed - there exists some random noise that you could sample with positive probability that is exactly what's needed to turn your sandal into a stylish t-shirt.\n\nThe claim is that this is an empirical fact about the way the human visual system perceives objects - a sandal with a small splotch on it still looks like a sandal to us. As long as this holds most of the time, then we've successfully generated an additional training example. In addition, we know something about how the new example relates to the original example.\n\nNote that this is similar but not the same as data augmentation in traditional supervised learning. In that setup, we make a perturbation to the original image and claim that the class label is preserved - that is, we would tell the model via the loss function that our noised sandal is exactly as much a sandal as the original sandal is, for any level of noise up to some arbitrary maximum. In today's setup, we're claiming that the noised sandal is less of a FashionMNIST member in proportion to the amount of noise involved.\n\nNow that we know how to generate as much low-probability data as we want, in theory we could learn a reverse function that takes an image and returns one that is *more* likely to belong to the distribution.\n\nThen we could just repeatedly apply the reverse function to \"hill climb\" and end up with a final image that has a relatively large probability. We know that deep neural networks are a good way to learn complicated functions, if you can define a loss function suitable for gradient descent and if you can find a suitable parameterization so that learning is smooth.\n\n## Today's Plan:\n- Implement the forward process to add noise to images\n- Implement the loss function\n- Train a toy diffusion model to reconstruct images of color gradients\n- Implement the UNet neural network architecture\n- Train the network from scratch on FashionMNIST\n\nEnsure you have [the paper](https://arxiv.org/pdf/2006.11239.pdf) open to section 2 (Background). I find it helpful when facing a pile of notation to go through each symbol and try to make each one as concrete as possible. In particular, its useful to clearly distinguish between \"a probability distribution\" and \"a sample from a probability distribution\".\n\nTo start, we have $x_0$ which is a sample of data. $q(x_0)$ on the other hand is probability distribution - it's a function that you can pass any image to and get out a number telling you how likely that image is to occur in the input distribution.\n\nSpecifically, $x_0$ is an image treated as a flattened vector of some length `k = image width * image height * num color channels`. So one element of this vector is the intensity of some color at some position in the image. $x_1$ up to $x_T$ are also images, with some number of steps of noise added and capital $T$ is the maximum number of noise steps we're considering.\n\nThe notation $p(x_T) = N(x_T; 0, I)$ was new to me. The function $p$ refers to the reverse process, and in general $p$ is defined through Equation 1. But for the specific scenario that the input to $p$ is some image with T steps of noise added, the output of $p$ is a standard Gaussian. The 0 is a vector of `k` zeros, and the `I` is the identity matrix of shape `(k, k)` which means that each color component at each position is independent of everything else in the image. They dropped the subscript $\\theta$ because $\\theta$ refers to the parameters of a neural network, but in this specific case, the neural network isn't needed. The semicolon is just separating the left part $x_T$ which is an input to the function from the right part, which is constant.\n\nEquation 1 then defines what $p$ means for all the other possible inputs. It means you start with that standard k-dimensional Gaussian and multiplying by a bunch of other Gaussians of the same dimensionality. Geometrically, you can think of your blob of probability slowly shifting around until we end up with a posterior distribution.\n\nIn the actual code, we will work with samples instead: starting with a sample from $p(x_T)$ (which to reiterate is just a standard k-dimensional Gaussian), we repeatedly pass the sample into the neural network to get a mean and variance term, which define a Gaussian that we can sample from to obtain a new sample from $p(x_t-1)$. Once we reach a sample from $p(x_0)$, we'll have an image that looks like a real $x_0$, if our neural network was giving us the right outputs.\n\nExercise: work through Equation 2 and explain what each symbol means in plain English. The specific values of $\\beta$ are defined in Section 4. Try to explain why the square root term is there. If you feel stuck or intimidated, don't hesitate to look at the solution.\n\n<details>\n\n<summary>Solution - Equation 2</summary>\n\nWe're going to apply noise in a number of small steps, where each $\\beta_t$ is a scalar controlling the variance of the Gaussian. Because we take the square root of $1 - \\beta_t$, each $\\beta_t$ must be less than or equal to 1.\n\nSection 4 says that the noise schedule is fixed ahead of time and isn't learned during training.\n\nIn terms of distributions, the forward process is also a product of Gaussians just like the reverse process. The Gaussians here don't depend on the neural network - to obtain a distribution $q(x_t)$ all we need is the previous $x_{t-1}$ and the appropriate $\\beta$. Again, all pixels and all color channels are noised independently.\n\nIn terms of samples, if we take a sample $x_0$ and repeatedly sample from the Gaussians, we'll end up with a sample $q(x_t)$ after $t$ steps. The square root term isn't justified, but I think the idea is to prevent the norm of $q(x_t)$ from growing too much.\n\n</details>\n\nLet's implement the forward process using what we know so far!\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import os\nimport time\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom functools import reduce\nfrom operator import mul\nfrom typing import Any, Optional, Union\nimport matplotlib.pyplot as plt\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\nimport utils\nimport wandb\n\nMAIN = __name__ == \"__main__\"\nIS_CI = os.getenv(\"IS_CI\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Image Processing\n\nWe'll first generate a toy dataset of random color gradients, and train the model to be able to recover them. This should be an easy task because the structure in the data is simple.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def gradient_images(n_images: int, img_size: tuple[int, int, int]) -> t.Tensor:\n    \"\"\"\n    Generate n_images of img_size, each a color gradient\n    \"\"\"\n    (C, H, W) = img_size\n    corners = t.randint(0, 255, (2, n_images, C))\n    xs = t.linspace(0, W / (W + H), W)\n    ys = t.linspace(0, H / (W + H), H)\n    (x, y) = t.meshgrid(xs, ys, indexing=\"xy\")\n    grid = x + y\n    grid = grid / grid[-1, -1]\n    grid = repeat(grid, \"h w -> b c h w\", b=n_images, c=C)\n    base = repeat(corners[0], \"n c -> n c h w\", h=H, w=W)\n    ranges = repeat(corners[1] - corners[0], \"n c -> n c h w\", h=H, w=W)\n    gradients = base + grid * ranges\n    assert gradients.shape == (n_images, C, H, W)\n    return gradients / 255\n\n\ndef plot_img(img: t.Tensor, title: Optional[str] = None) -> None:\n    if IS_CI:\n        return\n    img = rearrange(img, \"c h w -> h w c\")\n    plt.imshow(img.numpy())\n    if title:\n        plt.title(title)\n    plt.show()\n\n\nif MAIN:\n    print(\"A few samples from the input distribution: \")\n    img_shape = (3, 16, 16)\n    n_images = 5\n    imgs = gradient_images(n_images, img_shape)\n    for i in range(n_images):\n        plot_img(imgs[i])\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Normalization\n\nEach input value is from [0, 1] right now, but it's easier for the neural network to learn if we center them so they have mean 0. Here are some helper functions to do that, and to recover our original image.\n\nAll our computations will operate on normalized images, and we'll denormalize whenever we want to plot the result.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def normalize_img(img: t.Tensor) -> t.Tensor:\n    return img * 2 - 1\n\n\ndef denormalize_img(img: t.Tensor) -> t.Tensor:\n    return ((img + 1) / 2).clamp(0, 1)\n\n\nif MAIN:\n    plot_img(imgs[0], \"Original\")\n    plot_img(normalize_img(imgs[0]), \"Normalized\")\n    plot_img(denormalize_img(normalize_img(imgs[0])), \"Denormalized\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Variance Schedule\n\nThe amount of noise to add at each step is called $\\beta$.\n\nCompute the vector of $\\beta$ according to Section 4 of the paper. They use 1000 steps, but when reproducing a paper it's a good idea to start by making everything smaller in order to have faster feedback cycles, so we're using only 200 steps here.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def linear_schedule(max_steps: int, min_noise: float = 0.0001, max_noise: float = 0.02) -> t.Tensor:\n    \"\"\"Return the forward process variances as in the paper.\n\n    max_steps: total number of steps of noise addition\n    out: shape (step=max_steps, ) the amount of noise at each step\n    \"\"\"\n    pass\n\n\nif MAIN:\n    betas = linear_schedule(max_steps=200)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Forward (q) function - Equation 2\n\nImplement Equation 2 in code. Literally use a for loop to implement the q function iteratively and visualize your results. After 50 steps, you should barely be able to make out the colors of the gradient. After 200 steps it should look just like random Gaussian noise.\n\nHint: you can use `torch.normal` or `torch.randn`.\n\n<details>\n\n<summary>I can still see the gradient pretty well after 200 steps.</summary>\n\nThe beta value indicates the variance of the normal distribution - did you forget a square root to convert it to standard deviation?\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def q_eq2(x: t.Tensor, num_steps: int, betas: t.Tensor) -> t.Tensor:\n    \"\"\"Return the input image with num_steps iterations of noise added according to schedule.\n    x: shape (channels, height, width)\n    schedule: shape (T, ) with T >= num_steps\n\n    out: shape (channels, height, width)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    x = normalize_img(gradient_images(1, (3, 16, 16))[0])\n    for n in [1, 10, 50, 200]:\n        xt = q_eq2(x, n, betas)\n        plot_img(denormalize_img(xt), f\"Equation 2 after {n} step(s)\")\n    plot_img(denormalize_img(t.randn_like(xt)), \"Random Gaussian noise\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Forward (q) function - Equation 4\n\nEquation 2 is very slow and would be even slower if we went to 1000 steps. Conveniently, the authors chose to use Gaussian noise and a nice closed form expression exists to go directly to step t without needing a for loop. Implement Equation 4 and verify it looks visually similar to Equation 2.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def q_eq4(x: t.Tensor, num_steps: int, betas: t.Tensor) -> t.Tensor:\n    \"\"\"Equivalent to Equation 2 but without a for loop.\"\"\"\n    pass\n\n\nif MAIN:\n    for n in [1, 10, 50, 200]:\n        xt = q_eq4(x, n, betas)\n        plot_img(denormalize_img(xt), f\"Equation 4 after {n} steps\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nOur image reconstruction process will depend on the noise schedule we use during training. So that we can save our noise schedule with our model later, we'll define a `NoiseSchedule` class that subclasses `nn.Module`.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class NoiseSchedule(nn.Module):\n    betas: t.Tensor\n    alphas: t.Tensor\n    alpha_bars: t.Tensor\n\n    def __init__(self, max_steps: int, device: Union[t.device, str]) -> None:\n        super().__init__()\n        self.max_steps = max_steps\n        self.device = device\n        pass\n\n    @t.inference_mode()\n    def beta(self, num_steps: Union[int, t.Tensor]) -> t.Tensor:\n        \"\"\"\n        Returns the beta(s) corresponding to a given number of noise steps\n        num_steps: int or int tensor of shape (batch_size,)\n        Returns a tensor of shape (batch_size,), where batch_size is one if num_steps is an int\n        \"\"\"\n        pass\n\n    @t.inference_mode()\n    def alpha(self, num_steps: Union[int, t.Tensor]) -> t.Tensor:\n        \"\"\"\n        Returns the alphas(s) corresponding to a given number of noise steps\n        num_steps: int or int tensor of shape (batch_size,)\n        Returns a tensor of shape (batch_size,), where batch_size is one if num_steps is an int\n        \"\"\"\n        pass\n\n    @t.inference_mode()\n    def alpha_bar(self, num_steps: Union[int, t.Tensor]) -> t.Tensor:\n        \"\"\"\n        Returns the alpha_bar(s) corresponding to a given number of noise steps\n        num_steps: int or int tensor of shape (batch_size,)\n        Returns a tensor of shape (batch_size,), where batch_size is one if num_steps is an int\n        \"\"\"\n        pass\n\n    def __len__(self) -> int:\n        return self.max_steps\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNow we'll use this noise schedule to apply noise to our generated images. This will be the batched version of `q_eq4`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def noise_img(\n    img: t.Tensor, noise_schedule: NoiseSchedule, max_steps: Optional[int] = None\n) -> tuple[t.Tensor, t.Tensor, t.Tensor]:\n    \"\"\"\n    Adds a random number of steps of noise to each image in img.\n\n    img: An image tensor of shape (B, C, H, W)\n    noise_schedule: The NoiseSchedule to follow\n    max_steps: if provided, only perform the first max_steps of the schedule\n\n    Returns a tuple composed of:\n    num_steps: an int tensor of shape (B,) of the number of steps of noise added to each image\n    noise: the unscaled, standard Gaussian noise added to each image, a tensor of shape (B, C, H, W)\n    noised: the final noised image, a tensor of shape (B, C, H, W)\n    \"\"\"\n    (B, C, H, W) = img.shape\n    pass\n\n\nif MAIN:\n    noise_schedule = NoiseSchedule(max_steps=200, device=\"cpu\")\n    img = gradient_images(1, (3, 16, 16))\n    (num_steps, noise, noised) = noise_img(normalize_img(img), noise_schedule, max_steps=10)\n    plot_img(img[0], \"Gradient\")\n    plot_img(noise[0], \"Applied Unscaled Noise\")\n    plot_img(denormalize_img(noised[0]), \"Gradient with Noise Applied\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nLater, we'd like to reconstruct images for logging purposes. If we pass the true noise to this function, it will compute the inverse of `noise_img()` above.\n\nDuring training, we'll pass the predicted noise and we'll be able to visually see how close the prediction is.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def reconstruct(noisy_img: t.Tensor, noise: t.Tensor, num_steps: t.Tensor, noise_schedule: NoiseSchedule) -> t.Tensor:\n    \"\"\"\n    Subtract the scaled noise from noisy_img to recover the original image. We'll later use this with the model's output to log reconstructions during training. We'll use a different method to sample images once the model is trained.\n\n    Returns img, a tensor with shape (B, C, H, W)\n    \"\"\"\n    (B, C, H, W) = noisy_img.shape\n    pass\n\n\nif MAIN:\n    reconstructed = reconstruct(noised, noise, num_steps, noise_schedule)\n    denorm = denormalize_img(reconstructed)\n    plot_img(img[0], \"Original Gradient\")\n    plot_img(denorm[0], \"Reconstruction\")\n    utils.allclose(denorm, img)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNow, we'll create a tiny model to use as our diffusion model. We'll use a simple two-layer MLP.\n\nNote that we setup our `DiffusionModel` class to subclass `nn.Module` and the abstract base class (ABC). All ABC does for us is raise an error if subclasses forget to implement the abstract method `forward`. Later, we can write our training loop to work with any `DiffusionModel` subclass.\n\n<details>\n<summary>How should we handle num_steps in the forward pass?</summary>\nYou can scale num_steps down to [0, 1] and concatenate it to the flattened image.\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class DiffusionModel(nn.Module, ABC):\n    img_shape: tuple[int, ...]\n    noise_schedule: Optional[NoiseSchedule]\n\n    @abstractmethod\n    def forward(self, images: t.Tensor, num_steps: t.Tensor) -> t.Tensor:\n        ...\n\n\n@dataclass(frozen=True)\nclass TinyDiffuserConfig:\n    img_shape: tuple[int, ...]\n    hidden_size: int\n    max_steps: int\n\n\nclass TinyDiffuser(DiffusionModel):\n    def __init__(self, config: TinyDiffuserConfig):\n        \"\"\"\n        A toy diffusion model composed of an MLP (Linear, ReLU, Linear)\n        \"\"\"\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.img_shape = config.img_shape\n        self.noise_schedule = None\n        self.max_steps = config.max_steps\n        pass\n\n    def forward(self, images: t.Tensor, num_steps: t.Tensor) -> t.Tensor:\n        \"\"\"\n        Given a batch of images and noise steps applied, attempt to predict the noise that was applied.\n        images: tensor of shape (B, C, H, W)\n        num_steps: tensor of shape (B,)\n\n        Returns\n        noise_pred: tensor of shape (B, C, H, W)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    img_shape = (3, 4, 5)\n    n_images = 5\n    imgs = gradient_images(n_images, img_shape)\n    n_steps = t.zeros(imgs.size(0))\n    model_config = TinyDiffuserConfig(img_shape, 16, 100)\n    model = TinyDiffuser(model_config)\n    out = model(imgs, n_steps)\n    plot_img(out[0].detach(), \"Noise prediction of untrained model\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Training Loop\n\nAfter a pile of math, the authors arrive at Equation 14 for the loss function and Algorithm 1 for the training procedure. We're going to skip over the derivation for now and implement the training loop at the top of Page 4.\n\nExercise: go through each line of Algorithm 1, explain it in plain English, and describe the shapes of each thing.\n\n<details>\n\n<summary>Solution - Line 2</summary>\n\nThe $x_0$ is just the original training data distribution, so we're just going to draw a minibatch from the training data of shape (batch, channels, height, width).\n\n</details>\n\n<details>\n\n<summary>Solution - Line 3</summary>\n\nWe need to draw the number of steps of noise to add for each element of the batch, so the $t$ here will have shape (batch,) and be an integer tensor. Both 1 and T are inclusive here. Each element gets a different number of steps of noise added.\n\n</details>\n\n<details>\n\n<summary>Solution - Line 4</summary>\n\n$\\epsilon$ is the sampled noise, not scaled by anything. It's going to add to the image, so its shape also has to be (batch, channel, height, width).\n\n</details>\n\n<details>\n\n<summary>Solution - Line 5</summary>\n\n$\\epsilon_\\theta$ is our neural network. It takes two arguments: the image with noise applied in one step of (batch, channel, height, width), and the number of steps (batch, ), normalized to the range [0, 1].\n\n</details>\n\nIn Line 6 - it's unspecified how we know if the network is converged. We're just going to go until the loss seems to stop decreasing.\n\nNow implement the training loop on minibatches of examples, using Adam as the optimizer. Log your results to Weights and Biases.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def log_images(\n    img: t.Tensor, noised: t.Tensor, noise: t.Tensor, noise_pred: t.Tensor, reconstructed: t.Tensor, num_images: int = 3\n) -> list[wandb.Image]:\n    \"\"\"\n    Convert tensors to a format suitable for logging to Weights and Biases. Returns an image with the ground truth in the upper row, and model reconstruction on the bottom row. Left is the noised image, middle is noise, and reconstructed image is in the rightmost column.\n    \"\"\"\n    actual = t.cat((noised, noise, img), dim=-1)\n    pred = t.cat((noised, noise_pred, reconstructed), dim=-1)\n    log_img = t.cat((actual, pred), dim=-2)\n    images = [wandb.Image(i) for i in log_img[:num_images]]\n    return images\n\n\ndef train(\n    model: DiffusionModel, config_dict: dict[str, Any], trainset: TensorDataset, testset: Optional[TensorDataset] = None\n) -> DiffusionModel:\n    wandb.init(project=\"diffusion_models\", config=config_dict, mode=\"disabled\" if IS_CI else \"enabled\")\n    config = wandb.config\n    print(f\"Training with config: {config}\")\n    pass\n\n\nif MAIN:\n    config: dict[str, Any] = dict(\n        lr=0.001,\n        image_shape=(3, 4, 5),\n        hidden_size=128,\n        epochs=20 if not IS_CI else 1,\n        max_steps=100,\n        batch_size=128,\n        img_log_interval=200,\n        n_images_to_log=3,\n        n_images=50000 if not IS_CI else 10,\n        n_eval_images=1000 if not IS_CI else 10,\n        device=t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\"),\n    )\n    images = normalize_img(gradient_images(config[\"n_images\"], config[\"image_shape\"]))\n    train_set = TensorDataset(images)\n    images = normalize_img(gradient_images(config[\"n_eval_images\"], config[\"image_shape\"]))\n    test_set = TensorDataset(images)\n    model_config = TinyDiffuserConfig(config[\"image_shape\"], config[\"hidden_size\"], config[\"max_steps\"])\n    model = TinyDiffuser(model_config).to(config[\"device\"])\n    model = train(model, config, train_set, test_set)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Sampling from the Model\n\nOur training loss went down, so maybe our model learned something. Implement sampling from the model according to Algorithm 2 so we can see what the images look like.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def sample(model: DiffusionModel, n_samples: int, return_all_steps: bool = False) -> Union[t.Tensor, list[t.Tensor]]:\n    \"\"\"\n    Sample, following Algorithm 2 in the DDPM paper\n\n    model: The trained noise-predictor\n    n_samples: The number of samples to generate\n    return_all_steps: if true, return a list of the reconstructed tensors generated at each step, rather than just the final reconstructed image tensor.\n\n    out: shape (B, C, H, W), the denoised images\n    \"\"\"\n    schedule = model.noise_schedule\n    assert schedule is not None\n    pass\n\n\nif MAIN:\n    print(\"Generating multiple images\")\n    assert isinstance(model, DiffusionModel)\n    with t.inference_mode():\n        samples = sample(model, 5)\n    for s in samples:\n        plot_img(denormalize_img(s).cpu())\nif MAIN:\n    print(\"Printing sequential denoising\")\n    assert isinstance(model, DiffusionModel)\n    with t.inference_mode():\n        samples = sample(model, 1, return_all_steps=True)\n    for (i, s) in enumerate(samples):\n        if i % (len(samples) // 20) == 0:\n            plot_img(denormalize_img(s[0]), f\"Step {i}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNow that we've got the training working, on to part 2!\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}