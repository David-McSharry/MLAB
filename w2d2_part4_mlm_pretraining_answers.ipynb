{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W2D2 Part 4 - Pretraining\n\nWrite a training loop that loads the data you saved in Part 3 and makes use of your implementations of `random_mask` and `cross_entropy_selected`. (You'll need to change the import to point to your code, and change the path to point to your saved data).\n\nWe can't exactly follow the hyperparameters from section A.2 of the paper because they are training a much larger model on a much larger dataset using multiple GPUs.\n\nDon't expect too much out of WikiText-2: deep learning is extremely sample inefficient, and we are orders of magnitude away from the amount of compute needed to pretrain something as good as the real BERT.\n\nFor comparison, the [Induction Heads paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#results-loss) observed induction heads forming around 2 billion tokens seen, which would require 1000 epochs on our 2 million token dataset. To train a model like PaLM, they used 780 billion tokens for one epoch.\n\nToday, if your model can beat the baseline of just predicting the token frequencies, you can consider it a success. Training the model to do better than predicting token frequencies may take longer than you have available. For reference, to see whether your training run is off-track, here is a plot of the loss for a successful training run:\n\n<p align=\"center\">\n    <img src=\"w2d2_train_loss.png\"/>\n</p>\n\n## Table of Contents\n\n- [DataLoaders](#dataloaders)\n- [Learning Rate Schedule](#learning-rate-schedule)\n- [Weight Decay](#weight-decay)\n- [Training Loop](#training-loop)\n- [Model Evaluation](#model-evaluation)\n- [Bonus](#bonus)\n    - [Modifying Sequence Length During Training](#modifying-sequence-length-during-training)\n    - [Improved Versions of BERT](#improved-versions-of-bert)\n    - [Next Sentence Prediction](#next-sentence-prediction)\n    - [Applying Scaling Laws](#applying-scaling-laws)\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import os\nimport sys\nimport torch as t\nimport transformers\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import TensorDataset\nfrom tqdm.auto import tqdm\nimport wandb\nfrom w2d1_solution import BertConfig, BertLanguageModel, predict\nfrom w2d2_part3_wikitext_solution import cross_entropy_selected, random_mask\n\nMAIN = __name__ == \"__main__\"\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nIS_CI = os.getenv(\"IS_CI\")\nif IS_CI:\n    sys.exit(0)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## DataLoaders\n\nAdjust the path as necessary.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n    hidden_size = 512\n    assert hidden_size % 64 == 0\n    bert_config_tiny = BertConfig(\n        max_position_embeddings=128,\n        hidden_size=hidden_size,\n        intermediate_size=4 * hidden_size,\n        num_layers=8,\n        num_heads=hidden_size // 64,\n    )\n    config_dict = dict(\n        filename=\"./data/w2d2/bert_lm.pt\",\n        lr=0.0002,\n        epochs=40,\n        batch_size=128,\n        weight_decay=0.01,\n        mask_token_id=tokenizer.mask_token_id,\n        warmup_step_frac=0.01,\n        eps=1e-06,\n        max_grad_norm=None,\n    )\n    (train_data, val_data, test_data) = t.load(\"./data/w2d2/wikitext_tokens_103.pt\")\n    print(\"Training data size: \", train_data.shape)\n    train_loader = DataLoader(\n        TensorDataset(train_data), shuffle=True, batch_size=config_dict[\"batch_size\"], drop_last=True\n    )\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Learning Rate Schedule\n\nThe authors used learning rate warmup from an unspecified value and an unspecified shape to a maximum of 1e-4 for the first 10,000 steps out of 1 million, and then linearly decayed to an unspecified value.\n\nIt's very common that authors will leave details out of the paper in the interests of space, and the only way to figure it out is hope that they published source code. The source code doesn't always match the actual experimental results, but it's the best you can do other than trying to contact the authors.\n\nFrom the repo, we can see in [optimization.py](https://github.com/google-research/bert/blob/master/optimization.py) that AdamW is used for the optimizer, that the warmup is linear and that the epsilon used for AdamW is 1e-6.\n\nAssume that the initial learning rate and the final learning rate are both 1/10th of the maximum, and that we want to warm-up for 1% of the total number of steps.\n\n<details>\n\n<summary>Expected LR Schedule</summary>\n\n<p align=\"center\">\n    <img src=\"w2d2_mlm_schedule.png\"/>\n</p>\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def lr_for_step(step: int, max_step: int, max_lr: float, warmup_step_frac: float):\n    \"\"\"Return the learning rate for use at this step of training.\"\"\"\n    pass\n\n\nif MAIN:\n    max_step = int(len(train_loader) * config_dict[\"epochs\"])\n    lrs = [\n        lr_for_step(step, max_step, max_lr=config_dict[\"lr\"], warmup_step_frac=config_dict[\"warmup_step_frac\"])\n        for step in range(max_step)\n    ]\n    (fig, ax) = plt.subplots(figsize=(12, 4))\n    ax.plot(lrs)\n    ax.set(xlabel=\"Step\", ylabel=\"Learning Rate\", title=\"Learning Rate Schedule\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Weight Decay\n\nThe BERT paper specifies that a \"L2 weight decay\" of 0.01 is used, but leaves unspecified exactly which parameters have weight decay applied. Recall that weight decay is an inductive bias, which in the case of linear regression is exactly equivalent to a prior that each weight is Gaussian distributed.\n\nFor modern deep learning models, weight decay is much harder to analyze, having interactions with adaptive learning rate methods and normalization layers. Papers on weight decay feature phrases like \"The effect of weight decay remains poorly understood\" [1](https://arxiv.org/pdf/1810.12281.pdf) or \"despite its ubiquity, its behavior is still an area of active research\" [2](https://www.cs.cornell.edu/gomes/pdf/2021_bjorck_aaai_wd.pdf), and you'll see different implementations do different things.\n\nToday we're going to use weight decay conservatively, and only apply it to the weight (and not the bias) of each `Linear` layer. I didn't find much effect, but it provides an opportunity to learn how to use [parameter groups](https://pytorch.org/docs/stable/optim.html#per-parameter-options) in the optimizer.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def make_optimizer(model: BertLanguageModel, config_dict: dict) -> t.optim.AdamW:\n    \"\"\"\n    Loop over model parameters and form two parameter groups:\n\n    - The first group includes the weights of each Linear layer and uses the weight decay in config_dict\n    - The second has all other parameters and uses weight decay of 0\n    \"\"\"\n    pass\n\n\nif MAIN:\n    test_config = BertConfig(\n        max_position_embeddings=4, hidden_size=1, intermediate_size=4, num_layers=3, num_heads=1, head_size=1\n    )\n    optimizer_test_model = BertLanguageModel(test_config)\n    opt = make_optimizer(optimizer_test_model, dict(weight_decay=0.1, lr=0.0001, eps=1e-06))\n    expected_num_with_weight_decay = test_config.num_layers * 6 + 1\n    wd_group = opt.param_groups[0]\n    actual = len(wd_group[\"params\"])\n    assert (\n        actual == expected_num_with_weight_decay\n    ), f\"Expected 6 linear weights per layer (4 attn, 2 MLP) plus the final lm_linear weight to have weight decay, got {actual}\"\n    all_params = set()\n    for group in opt.param_groups:\n        all_params.update(group[\"params\"])\n    assert all_params == set(optimizer_test_model.parameters()), \"Not all parameters were passed to optimizer!\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Training Loop\n\nWrite your training loop here, logging to Weights and Biases. Tips:\n\n- Log your training loss and sanity check that it looks reasonable. At initialization it should be around the value of random prediction; if it's much higher, then you probably have a bug in your weight initialization code.\n- By 10M tokens, if your model hasn't basically figured out the unigram frequencies then you probably have a bug, or you changed your hyperparameters to bad ones.\n- Log your learning rate as well, as it's easy to not apply the learning rate schedule properly.\n- If you use gradient clipping, note that `clip_grad_norm_` returns the norm of the gradients. It's useful to know if your clipping is actually doing anything.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def bert_mlm_pretrain(model: BertLanguageModel, config_dict: dict, train_loader: DataLoader) -> None:\n    \"\"\"Train using masked language modelling.\"\"\"\n    pass\n\n\nif MAIN:\n    model = BertLanguageModel(bert_config_tiny)\n    num_params = sum((p.nelement() for p in model.parameters()))\n    print(\"Number of model parameters: \", num_params)\n    bert_mlm_pretrain(model, config_dict, train_loader)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Model Evaluation\n\nYou can test the model's predictions, but they're going to be underwhelming given our limited computational budget.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    model = BertLanguageModel(bert_config_tiny)\n    model.load_state_dict(t.load(config_dict[\"filename\"]))\n    your_text = \"The Answer to the Ultimate Question of Life, The Universe, and Everything is [MASK].\"\n    predictions = predict(model, tokenizer, your_text)\n    print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Bonus\n\nCongratulations, you've finished the main content for today!\n\nYou can do whatever bonus section(s) seem interesting to you.\n\n### Modifying Sequence Length During Training\n\nThe authors pretrained using a sequence length of just 128 for 90% of the steps, then completed training with sequence length of 512. Try this and see what happens.\n\n### Improved Versions of BERT\n\nRead about one of the improved versions of BERT and try to replicate it. Some good ones to try:\n\n- [ELECTRA](https://arxiv.org/pdf/2003.10555.pdf%27)\n- [DEBERTA](https://arxiv.org/pdf/2006.03654.pdf)\n- [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf)\n\n### Next Sentence Prediction\n\nTry to pretrain on the next sentence prediction task.\n\n### Applying Scaling Laws\n\nThe paper [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf) attempts to find the optimal model and dataset sizes for a given amount of compute. If you were to extrapolate their curves down to our very limited amount of compute, what does the paper suggest would be the ideal model and dataset size?\n\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}