{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D5 - Part 2 - Stable Diffusion\n\n## Setting up your instance\n\nIn order to download the stable-diffusion models from HuggingFace, there is some setup required:\n\n1. Make a [HuggingFace account](https://huggingface.co/join) and confirm your email address.\n1. Visit [here](https://huggingface.co/CompVis/stable-diffusion-v1-4) and click `yes` to the terms and conditions (after thoroughly reading them, of course) and then click `access repository`.\n1. Generate a [HuggingFace token](https://huggingface.co/settings/tokens) with a `read` role.\n1. Run `huggingface-cli login` in your VSCode terminal and paste the token you generated above (ignore the warning text). This will allow the Python module to download the pretrained models we will be using.\n\nYou should now be able to load the pretrained models in this notebook.\n\n## Introducing the model\n\nBefore moving on to integrate CLIP into the Stable Diffusion (SD) model, it's worth briefly reviewing what we've built in Part 1. CLIP provides a text encoder and image encoder that are trained together to minimize contrastive loss, and therefore allows for embedding arbitrary text sequences in a latent space that has some relevance to images.\n\nNow, we will introduce the Stable Diffusion model, a state-of-the-art architecture that integrates the text encoder from CLIP (although other text encoders can be used) into a modified diffusion model which is similar to your work from yesterday, W3D4. The primary differences between an \"ordinary\" diffusion model and the Stable Diffusion model:\n\n* Text encoding is done using a frozen CLIP text encoder. By frozen, we mean the encoder was pretrained separately using the contrastive loss as described yesterday, and not modified at all during the training of SD. The vision transformer part of CLIP is not used in SD.\n* U-Net operates in a latent space which has a lower spatial dimensionality than the pixel space of the input. The schematic below describes \"LDM-8\" which means that the spatial dimension is shrunk by a factor of 8 in width and height. More downsampling makes everything faster, but reduces perceptual quality.\n* The encoding to and decoding from the latent space are done using a Variational Autoencoder (VAE), which is trained on the reconstruction error after compressing images into the latent space and then decompressing them again. At inference time, we have no need of the encoder portion because we start with random latents, not pixels. We only need to make one call to the VAE decoder at the very end to turn our latents back into pixels.\n\n## Schematic\n\n<p align=\"center\">\n    <img src=\"w3d5_stablediff_schematic.png\" width=\"500\"/><br>\n    Source: <a href=\"https://huggingface.co/blog/stable_diffusion\">https://huggingface.co/blog/stable_diffusion</a>\n</p>\n\n## A final product\n\nBefore getting into it, here's a *very rough* example of the sort of interpolation-based animations that you will (hopefully) be generating by the end of the day with only a few minutes of runtime! Ideally, yours will be smoother and more creative given longer inference time to generate more frames. :)\n\n<p align=\"center\">\n    <img src=\"w3d5_stable_diffusion_animation.gif\" width=\"500\"/><br/>\n</p>\n\n## Table of Contents\n\n- [Setting up your instance](#setting-up-your-instance)\n- [Introducing the model](#introducing-the-model)\n- [Schematic](#schematic)\n- [A final product](#a-final-product)\n- [References (optional) for Part 2](#references-optional-for-part-)\n- [Preparation](#preparation)\n- [Text encoder](#text-encoder)\n- [Getting pretrained models](#getting-pretrained-models)\n- [Tokenization](#tokenization)\n- [Assembling the inference pipeline](#assembling-the-inference-pipeline)\n- [Trying it out!](#trying-it-out)\n- [Implementing interpolation](#implementing-interpolation)\n- [Prompt Interpolation](#prompt-interpolation)\n    - [Saving a GIF](#saving-a-gif)\n- [Bonus](#bonus)\n    - [Speeding up interpolation](#speeding-up-interpolation)\n    - [Multiple prompt image generation](#multiple-prompt-image-generation)\n- [Acknowledgements](#acknowledgements)\n\n## References (optional) for Part 2\n\n- [Stable Diffusion Paper](https://arxiv.org/abs/2112.10752)\n- [HuggingFace implementation](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n- [HuggingFace diffusers module](https://github.com/huggingface/diffusers/tree/71ba8aec55b52a7ba5a1ff1db1265ffdd3c65ea2)\n- [Classifier-Free Diffusion Guidance paper](https://arxiv.org/abs/2207.12598)\n\n\n\n# Implementation\n\nNow, we will work on implementing Stable Diffusion according to the above schematic as each of the parts have already been implemented. Furthermore, due to the significant training time of a model, we will use pretrained models from HuggingFace: [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4). This pretrained Stable Diffusion pipeline includes weights for the text encoder, tokenizer, variational autoencoder, and U-Net that have been trained together (again, with a fixed pretrained text encoder).\n\n## Preparation\n\nFirst, we import the necessary libraries, define a config class, and provide a helper function to assist you in your implementation. This function gets the pretrained models for the tokenizer, text encoder, VAE, and U-Net. As always, it is worth reading through this code to make sure you understand what it does.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import os\nimport sys\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Callable, Generic, TypeVar, Union, cast\nimport numpy as np\nimport torch as t\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers.schedulers.scheduling_lms_discrete import LMSDiscreteScheduler\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom transformers.models.clip import modeling_clip\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom w3d5_globals import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\nfrom w3d5_part1_clip_solution import CLIPModel\n\nMAIN = __name__ == \"__main__\"\nDEVICE = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nIS_CI = os.getenv(\"IS_CI\")\nif IS_CI:\n    sys.exit(0)\n\n\n@dataclass\nclass StableDiffusionConfig:\n    \"\"\"\n    Default configuration for Stable Diffusion.\n\n    guidance_scale is used for classifier-free guidance.\n\n    The sched_ parameters are specific to LMSDiscreteScheduler.\n    \"\"\"\n\n    height = 512\n    width = 512\n    num_inference_steps = 100\n    guidance_scale = 7.5\n    sched_beta_start = 0.00085\n    sched_beta_end = 0.012\n    sched_beta_schedule = \"scaled_linear\"\n    sched_num_train_timesteps = 1000\n\n    def __init__(self, generator: t.Generator):\n        self.generator = generator\n\n\nT = TypeVar(\"T\", CLIPTokenizer, CLIPTextModel, AutoencoderKL, UNet2DConditionModel)\n\n\ndef load_model(cls: type[T], subfolder: str) -> T:\n    model = cls.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=subfolder, use_auth_token=True)\n    return cast(T, model)\n\n\ndef load_tokenizer() -> CLIPTokenizer:\n    return load_model(CLIPTokenizer, \"tokenizer\")\n\n\ndef load_text_encoder() -> CLIPTextModel:\n    return load_model(CLIPTextModel, \"text_encoder\").to(DEVICE)\n\n\ndef load_vae() -> AutoencoderKL:\n    return load_model(AutoencoderKL, \"vae\").to(DEVICE)\n\n\ndef load_unet() -> UNet2DConditionModel:\n    return load_model(UNet2DConditionModel, \"unet\").to(DEVICE)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNow that the pretrained models we will be using are in an accessible format, try printing one of them out to examine its architecture. For example:\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    vae = load_vae()\n    print(vae)\n    del vae\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Text encoder\n\nNext, we provide a function to initialize a text encoder model from our implementation of CLIP and load the weights from the pretrained CLIP text encoder. This uses the `load_state_dict` function, which loads the variables from an `OrderedDict` that maps parameter names to their values (typically tensors) into another model with identically named parameters.\n\nIn this case, the pretrained `state_dict` of the `CLIPTextModel` instance contains keys prepended with `text_model.`, as the `CLIPTextModel` encapsulates the `CLIPTextTransformer` model, i.e. `type(CLIPTextModel.text_model) == CLIPTextTransformer`. Therefore, to match the input dictionary keys to the parameter names in our `CLIPModel.text_model` class, we need to modify the dictionary from `pretrained.state_dict()` to remove the `text_model.` from each key.\n\n**Note:** You may have noticed that by using the `text_model` member of our `CLIPModel` class from Part 1, we depend on the implementation of `CLIPTextTransformer` imported from `modeling_clip`. This is the same class as that used by the pretrained text model in `CLIPTextModel`. Therefore, this function effectively initializes a `CLIPTextTransformer` class with the pretrained weights just to copy its weights to a second `CLIPTextTransformer` class. However, if we later choose to modify or re-implement the `text_model` in our `CLIPModel` class, maintaining the same parameter names, this function will serve to initialize its weights using the pretrained model weights.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def clip_text_encoder(pretrained: CLIPTextModel) -> modeling_clip.CLIPTextTransformer:\n    pretrained_text_state_dict = OrderedDict([(k[11:], v) for (k, v) in pretrained.state_dict().items()])\n    clip_config = CLIPConfig(CLIPVisionConfig(), CLIPTextConfig())\n    clip_text_encoder = CLIPModel(clip_config).text_model\n    clip_text_encoder.to(DEVICE)\n    clip_text_encoder.load_state_dict(pretrained_text_state_dict)\n    return clip_text_encoder\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Getting pretrained models\n\nNow, we're ready to start building the model. There are only a few parts to instantiate and connect into a pipeline that can transform our text prompt into an image. First, we initialize the pretrained models as well as our `CLIPModel.text_model` with pretrained text encoder weights.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@dataclass\nclass Pretrained:\n    tokenizer = load_tokenizer()\n    vae = load_vae()\n    unet = load_unet()\n    pretrained_text_encoder = load_text_encoder()\n    text_encoder = clip_text_encoder(pretrained_text_encoder)\n\n\nif MAIN:\n    pretrained = Pretrained()\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Tokenization\n\nWe provide part of a helper function that uses our `PreTrainedTokenizer` to tokenize prompt strings, embed the tokens, and concatenate embeddings for the empty padding token for \"classifier-free guidance\" ([see paper for details](https://arxiv.org/abs/2207.12598)).\n\nPlease implement the `uncond_embeddings` used for classifier-free guidance below, based on the format of `text_embeddings`. Note that `uncond_embeddings` should be of the same shape as `text_embeddings`, and `max_length` has already been assigned for you. Return the concatenated tensor with `uncond_embeddings` and `text_embeddings`, in that order.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def tokenize(pretrained: Pretrained, prompt: list[str]) -> t.Tensor:\n    text_input = pretrained.tokenizer(\n        prompt,\n        padding=\"max_length\",\n        max_length=pretrained.tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    text_embeddings = pretrained.text_encoder(text_input.input_ids.to(DEVICE))[0]\n    max_length = text_input.input_ids.shape[-1]\n    pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Assembling the inference pipeline\n\nUsing the scheduler parameters defined in the config at the beginning (`sched_`), instantiate and return the `LMSDiscreteScheduler` in `get_scheduler()`. The scheduler defines the noise schedule during training and/or inference, and will be used later in our inference process.\n\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def get_scheduler(config: StableDiffusionConfig) -> LMSDiscreteScheduler:\n    pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nNow, we will implement the missing parts of the inference pipeline in `stable_diffusion_inference()` below. The intended behavior of this function is as follows:\n\n1. Initialize the scheduler, batch_size, multiply latent random Gaussian noise by initial scheduler noise term $\\sigma_0$\n2. If prompt strings are provided, compute text embeddings\n3. In the inference loop, for each timestep defined by the scheduler:\n    1. Expand/repeat latent embeddings by 2 for classifier-free guidance, divide the result by $\\sqrt{\\sigma_i^2 + 1}$ using $\\sigma_i$ from the scheduler\n    2. Compute concatenated noise prediction using U-Net, feeding in latent input, timestep, and text embeddings\n    3. Split concatenated noise prediction $N_c = [N_u, N_t]$ into the unconditional $N_u$ and text $N_t$ portion. You can use the `torch.Tensor.chunk()` function for this.\n    4. Compute the total noise prediction $N$ with respect to the guidance scale factor $g$: $N = N_u + g * (N_t - N_u)$\n    5. Step to the previous timestep using the scheduler to get the next latent input\n4. Rescale latent embedding and decode into image space using VAE decoder\n5. Rescale resulting image into RGB space\n6. Permute dimensions and convert to `PIL.Image.Image` objects for viewing/saving\n\nExamine the existing implementation, identify which parts are missing, and implement these by referring to the surrounding code and module implementations as necessary.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def stable_diffusion_inference(\n    pretrained: Pretrained, config: StableDiffusionConfig, prompt: Union[list[str], t.Tensor], latents: t.Tensor\n) -> list[Image.Image]:\n    scheduler = get_scheduler(config)\n    if isinstance(prompt, list):\n        text_embeddings = None\n        text_embeddings = tokenize(pretrained, prompt)\n    elif isinstance(prompt, t.Tensor):\n        text_embeddings = prompt\n    scheduler.set_timesteps(config.num_inference_steps)\n    latents = latents * scheduler.sigmas[0]\n    with t.autocast(\"cuda\"):\n        for (i, ts) in enumerate(scheduler.timesteps):\n            latent_input = None\n            \"TODO: YOUR CODE HERE\"\n            with t.no_grad():\n                \"TODO: YOUR CODE HERE\"\n            \"TODO: YOUR CODE HERE\"\n            latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"]\n    images = pretrained.vae.decode(latents / 0.18215)\n    images = (images * 255 / 2 + 255 / 2).clamp(0, 255)\n    images = images.detach().cpu().permute(0, 2, 3, 1).numpy().round().astype(\"uint8\")\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Trying it out!\n\nFinally, after implementing a function to compute our latent noise (provided for you below), we can use our Stable Diffusion inference pipeline by passing in the pretrained models, config, and a prompt of strings.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def latent_sample(config: StableDiffusionConfig, batch_size: int) -> t.Tensor:\n    latents = t.randn(\n        (batch_size, cast(int, pretrained.unet.in_channels), config.height // 8, config.width // 8),\n        generator=config.generator,\n    ).to(DEVICE)\n    return latents\n\n\nif MAIN:\n    SEED = 1\n    config = StableDiffusionConfig(t.manual_seed(SEED))\n    prompt = [\"A digital illustration of a medieval town\"]\n    latents = latent_sample(config, len(prompt))\n    images = stable_diffusion_inference(pretrained, config, prompt, latents)\n    images[0].save(\"./w3d5_image.png\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n# Fun with animations!\n\nFinally, let's close off MLAB with some interpolation-based animation fun. The idea is relatively straightforward: the continuous text embedding space from which our Stable Diffusion pipeline generates an image means that we can interpolate between two or more text embeddings to build a set of images generated from the interpolation path. In other words, this means that we can generate a relatively sensible (to the extent that the denoising model works as we expect) image \"between\" any other two images.\n\n## Implementing interpolation\n\nGiven that we've already built our Stable Diffusion inference pipeline, the only thing we need to add is interpolation. Here, we create a function to handle the interpolation of tensors, `interpolate_embeddings()`, and use this function in `run_interpolation()` to loop over each embedded prompt, feeding it into the Stable Diffusion inference pipeline.\n\nPlease complete the implementation of `interpolate_embeddings()` as described. However, as this is the last day of MLAB content, if you would prefer to play around with generating images/animations feel free to use the solution code implementation.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def interpolate_embeddings(concat_embeddings: t.Tensor, scale_factor: int) -> t.Tensor:\n    \"\"\"\n    Returns a tensor with `scale_factor`-many interpolated tensors between each pair of adjacent\n    embeddings.\n    concat_embeddings: t.Tensor - Contains uncond_embeddings and text_embeddings concatenated together\n    scale_factor: int - Number of interpolations between pairs of points\n    out: t.Tensor - shape: [2 * scale_factor * (concat_embeddings.shape[0]/2 - 1), *concat_embeddings.shape[1:]]\n    \"\"\"\n    \"TODO: YOUR CODE HERE\"\n    assert out.shape == (2 * scale_factor * (num_prompts - 1), *text_embeddings.shape[1:])\n    return out\n\n\ndef run_interpolation(prompts: list[str], scale_factor: int, batch_size: int, latent_fn: Callable) -> list[Image.Image]:\n    SEED = 1\n    config = StableDiffusionConfig(t.manual_seed(SEED))\n    concat_embeddings = tokenize(pretrained, prompts)\n    (uncond_interp, text_interp) = interpolate_embeddings(concat_embeddings, scale_factor).chunk(2)\n    split_interp_emb = t.split(text_interp, batch_size, dim=0)\n    interpolated_images = []\n    for t_emb in tqdm(split_interp_emb):\n        concat_split = t.concat([uncond_interp[: t_emb.shape[0]], t_emb])\n        config = StableDiffusionConfig(t.manual_seed(SEED))\n        latents = latent_fn(config, t_emb.shape[0])\n        interpolated_images += stable_diffusion_inference(pretrained, config, concat_split, latents)\n    return interpolated_images\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Prompt Interpolation\n\nFinally, if you've implemented Stable Diffusion correctly, you're ready to play with prompt interpolation. Go ahead and fiddle with the prompts and interpolation scaling factor below, and be sure to share your favorite results on Slack!\n\n`scale_factor` indicates the number of images between each consecutive prompt.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    prompts = [\n        \"a photograph of a cat on a lawn\",\n        \"a photograph of a dog on a lawn\",\n        \"a photograph of a bunny on a lawn\",\n    ]\n    interpolated_images = run_interpolation(prompts, scale_factor=2, batch_size=1, latent_fn=latent_sample)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Saving a GIF\n\nSave your list of images as a GIF by running the following:\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def save_gif(images: list[Image.Image], filename):\n    images[0].save(filename, save_all=True, append_images=images[1:], duration=100, loop=0)\n\n\nif MAIN:\n    save_gif(interpolated_images, \"w3d5_animation1.gif\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Speeding up interpolation\n\nConsider how you might speed up the interpolation inference process above. Note that batching multiple prompts (making sure to concatenate their correspondings unconditional embeddings as expected) tends to speed up the per-prompt generation time. However, this also affects the random generation of Gaussian noise fed into the U-Net as the noise is different for each sample, which in practice tends to result in images that don't always \"fit\" together or play smoothly in an animation. Think about how you can modify the latent noise generation step to batch prompts without affecting the randomness relative to individually feeding prompts into the model, and try implementing this change.\n\nHere, a new function `latent_sample_same()` is created which uses the same inputs as `latent_sample()` and is intended to output the same noise for a batch size of 1. For larger batches, it should use the same noise for each image in the batch. Implement this quick change, looking back at `latent_sample()` if needed, and try testing whether a larger interpolation batch size with this sampling function improves performance on your system. This will depend on your maximum batch size usually constrained by GPU memory size as well as other minor factors.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def latent_sample_same(config: StableDiffusionConfig, batch_size: int) -> t.Tensor:\n    \"\"\"TODO: YOUR CODE HERE\"\"\"\n    return latents\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nFor example, here is a call to `run_interpolation()` that uses a batch size of 2 and passes in your modified `latent_sample_same()` function to generate random noise.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    prompts = [\n        \"a photograph of a cat on a lawn\",\n        \"a photograph of a dog on a lawn\",\n        \"a photograph of a bunny on a lawn\",\n    ]\n    interpolated_images = run_interpolation(prompts, scale_factor=2, batch_size=2, latent_fn=latent_sample_same)\n    save_gif(interpolated_images, \"w3d5_animation2.gif\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n\nIf you've gotten this far, you're all done with today's content as well as the standard MLAB content. Congratulations!\n\n## Bonus\n\nHere are a few bonus tasks as inspiration. However, feel free to play with the Stable Diffusion model to find your own idea!\n\n### Multiple prompt image generation\n\nWhat does it mean to combine prompts within a single image? Can this be done by modifying the Stable Diffusion inference process to condition on two or more text embeddings, or for parts of an image to condition on different embeddings?\n\n### Stylistic changes\n\nTry to identify changes in prompts that induce stylistic changes in the resulting image. For example, a painting as opposed to a photograph, or a greyscale photograph as opposed to a color photograph.\n\n## Acknowledgements\n\n- [HuggingFace blog post on Stable Diffusion](https://huggingface.co/blog/stable_diffusion>https://huggingface.co/blog/stable_diffusion), a great resource for introducing the model and implementing an inference pipeline.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}