{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# W2D2 - BERT Inference and Training\n","\n","Yesterday, we wrote the code for BERT's architecture and were able to do inference using an already trained model. Today's material has four parts:\n","\n","- Part 1: prepare a dataset for fine-tuning\n","- Part 2: fine-tune a pretrained BERT to do sentiment classification\n","- Part 3: prepare a dataset for pretraining\n","- Part 4: Train from scratch a tiny BERT on the masked language modeling task.\n","\n","## Table of Contents\n","\n","- [Readings](#readings)\n","- [HuggingFace Tokenizer Warning](#huggingface-tokenizer-warning)\n","- [Fine-Tuning BERT](#fine-tuning-bert)\n","    - [IMDB Dataset](#imdb-dataset)\n","- [Data Visualization](#data-visualization)\n","    - [Basic Inspection](#basic-inspection)\n","    - [Detailed Inspection](#detailed-inspection)\n","- [Tokenization](#tokenization)\n","- [Bonus](#bonus)\n","    - [Better Truncation](#better-truncation)\n","    - [Better Data Cleaning](#better-data-cleaning)\n","\n","## Readings\n","\n","- [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf) - focus on the details of pretraining, found primarily in Section 3.1 and Appendix A.\n","\n","## HuggingFace Tokenizer Warning\n","\n","You might see the warning \"The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks\" when re-running cells. Nothing bad will happen if you ignore this warning, other than that your tokenizer may run more slowly without the parallelism.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting matplotlib\n","  Using cached matplotlib-3.7.1-cp310-cp310-macosx_11_0_arm64.whl (7.3 MB)\n","Requirement already satisfied: numpy>=1.20 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from matplotlib) (1.24.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n","Collecting contourpy>=1.0.1\n","  Downloading contourpy-1.1.0-cp310-cp310-macosx_11_0_arm64.whl (229 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.4/229.4 kB\u001b[0m \u001b[31m336.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting cycler>=0.10\n","  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Requirement already satisfied: packaging>=20.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from matplotlib) (23.0)\n","Collecting pillow>=6.2.0\n","  Using cached Pillow-9.5.0-cp310-cp310-macosx_11_0_arm64.whl (3.1 MB)\n","Collecting kiwisolver>=1.0.1\n","  Using cached kiwisolver-1.4.4-cp310-cp310-macosx_11_0_arm64.whl (63 kB)\n","Collecting fonttools>=4.22.0\n","  Downloading fonttools-4.40.0-cp310-cp310-macosx_10_9_universal2.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m140.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting pyparsing>=2.3.1\n","  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","Requirement already satisfied: six>=1.5 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n","Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.40.0 kiwisolver-1.4.4 matplotlib-3.7.1 pillow-9.5.0 pyparsing-3.0.9\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install matplotlib\n","\n","import hashlib\n","import os\n","import re\n","import sys\n","import tarfile\n","from dataclasses import dataclass\n","import requests\n","import torch as t\n","import transformers\n","from matplotlib import pyplot as plt\n","from torch.utils.data import TensorDataset\n","from tqdm.auto import tqdm\n","\n","MAIN = __name__ == \"__main__\"\n","IMDB_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n","DATA_FOLDER = \"./data/w2d2/\"\n","IMDB_PATH = os.path.join(DATA_FOLDER, \"acllmdb_v1.tar.gz\")\n","SAVED_TOKENS_PATH = os.path.join(DATA_FOLDER, \"tokens.pt\")\n","device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n","IS_CI = os.getenv(\"IS_CI\")\n","if IS_CI:\n","    sys.exit(0)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Fine-Tuning BERT\n","\n","Fine-tuning a pretrained model is awesome - it typically is much faster and more accurate than directly training on the task you care about, especially if your task has relatively few labels available. In our case we will be using the [IMDB Sentiment Classification Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n","\n","It's traditional to treat this as a binary classification task where each review is positive or negative. Today we're also going to predict the star rating review from 1 to 10, inclusive.\n","\n","It's a bit redundant to train with both the star rating and the positive/negative labels as targets (you could just use the star rating), but we'll do it anyway to practice having multiple terms in the loss function.\n","\n","There are a few ways to treat the star rating. One way is to have each rating be a category and use the regular cross entropy loss.\n","\n","Exercise: what are the disadvantages of doing this?\n","\n","<details>\n","\n","<summary>Solution - disadvantages of cross-entropy for star rating</summary>\n","\n","Cross entropy doesn't capture the intuition that the classes are ordered. Intuitively, we should penalize our model more for predicting 10 stars for a 1-star movie than predicting 2 stars.\n","\n","</details>\n","\n","Another way would be to treat the star rating as a continuous value, and use mean squared or mean absolute error. We'll do this today because it's simple and works well, but note that a more sophisticated approach like [ordinal regression](https://en.wikipedia.org/wiki/Ordinal_regression) could also be used.\n","\n","### IMDB Dataset\n","\n","Previously, we've used the `torchvision` package to download CIFAR10 for us. Today we'll load and process the training data ourselves to get an idea of what's involved.\n","\n","Use [requests.get](https://requests.readthedocs.io/en/latest/user/quickstart/) to fetch the data and then write the `content` field of the response to disk. It's 82MB, so may take a few seconds depending on your connection. On future calls to the function, if the file already exists, your function should just read the local file instead of downloading the data again.\n","\n","We've provided code that hashes the data using `hashlib.md5` and verifies that it matches a known good reference. Why is this a good practice?\n","\n","<details>\n","\n","<summary>Solution - Why Hash the Data?</summary>\n","\n","We don't expect the file pointed to by the URL to change, and if it does we would like things to break loudly. At the very least, our training pipeline would no longer be reproducible, and it's possible that an adversary compromised the website and is supplying malicious data of some form. It's also possible that hardware errors could corrupt the data either on our local disk or the remote machine.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def maybe_download(url: str, path: str) -> None:\n","    \"\"\"Download the file from url and save it to path. If path already exists, do nothing.\"\"\"\n","    if not os.path.exists(path):\n","        with requests.get(url, stream=True) as r:\n","            r.raise_for_status()\n","            with open(path, \"wb\") as f:\n","                for chunk in r.iter_content(chunk_size=8192):\n","                    f.write(chunk)\n","\n","if MAIN:\n","    os.makedirs(DATA_FOLDER, exist_ok=True)\n","    expected_hexdigest = \"7c2ac02c03563afcf9b574c7e56c153a\"\n","    maybe_download(IMDB_URL, IMDB_PATH)\n","    with open(IMDB_PATH, \"rb\") as f:\n","        actual_hexdigest = hashlib.md5(f.read()).hexdigest()\n","        assert actual_hexdigest == expected_hexdigest\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","Now we have a tar archive, which we can read using the standard library module [tarfile](https://docs.python.org/3/library/tarfile.html). Note the warning about extracting archives from untrusted sources.\n","\n","Open the archive with `tarfile.open`, loop over the entries returned by `getmembers()` and use the `extractfile` method as appropriate to create a list of `Review`. A filename like `aclImdb/test/neg/127_3.txt` means it belongs to the test set, has a negative sentiment, has an id of 127 (we will ignore this), and was rated 3/10 stars.\n","\n","You should have 25000 train and 25000 test entries - ignore the unlabeled folder.\n","\n","This should take less than 10 seconds, but it's good practice to use tqdm to monitor your progress as most datasets will be much larger than this one.\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0867138e1e62447a8a3f18d779502c81","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/100019 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["@dataclass(frozen=True)\n","class Review:\n","    split: str\n","    is_positive: bool\n","    stars: int\n","    text: str\n","\n","\n","def load_reviews(path: str) -> list[Review]:\n","    reviews = []\n","    i = 0\n","    with tarfile.open(path) as t:\n","        for member in tqdm(t.getmembers()):\n","            if re.match(r\"aclImdb/(train|test)/(pos|neg)/\\d+_\\d+.txt\", member.name):\n","                reviews.append(\n","                    Review(\n","                        split=member.name.split(\"/\")[1],\n","                        is_positive=member.name.split(\"/\")[2] == \"pos\",\n","                        stars=int(member.name.split(\"_\")[1].split(\".\")[0]),\n","                        text=t.extractfile(member).read().decode(\"utf-8\"),\n","                    )\n","                )\n","    return reviews\n","\n","reviews = []\n","if MAIN:\n","    reviews = load_reviews(IMDB_PATH)\n","    assert sum((r.split == \"train\" for r in reviews)) == 25000\n","    assert sum((r.split == \"test\" for r in reviews)) == 25000\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Data Visualization\n","\n","Charles Babbage, the inventor of the first mechanical computer, was famously asked \"Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?\"\n","\n","200 years later, if you put wrong figures into the machine, the right answers still do not come out.\n","\n","Inspecting the data before training can be tedious, but will catch many errors, either in your code or upstream, and allow you to validate or refute assumptions you've made about the data. Remember: \"Garbage In, Garbage Out\".\n","\n","### Basic Inspection\n","\n","Take some time now to do a basic inspection of your data. This should at minimum include:\n","\n","- Plot the distribution of review lengths in characters.\n","    - Our BERT was only trained to handle 512 tokens maximum, so if we assume that a token is roughly 4 characters, we will have to truncate reviews that are longer than around 2048 characters.\n","    - Are positive and negative reviews different in length on average? If so, truncating would differentially affect the longer reviews.\n","- Plot the distribution of star ratings. Is it what you expected?\n","\n","<details>\n","<summary>Star ratings distribution</summary>\n","That's right, there are no five or six star reviews in the dataset.\n","</details>\n","\n","### Detailed Inspection\n","\n","Either now, or later while your model is training, it's a worthwhile and underrated activity to do a more in-depth inspection. For a language dataset, some things I would want to know are:\n","\n","- What is the distribution over languages? Many purportedly English datasets in fact have some of the data in other natural languages like Spanish, and computer languages like HTML tags.\n","    - This can cause bias in the results. Suppose that a small fraction of reviews were in Spanish, and purely by chance they have more positive/negative sentiment than the base rate. Our classifier would then incorrectly learn that Spanish words inherently have positive/negative sentiment.\n","    - Libraries like [Lingua](https://github.com/pemistahl/lingua-py) can (imperfectly) check for this.\n","- How are non-ASCII characters handled?\n","    - The answer is often \"poorly\". A large number of things can go wrong around quoting, escaping, and various text encodings. Spending a lot of time trying to figure out why there are way too many backslashes in front of your quotation marks is an Authentic ML Experience. Libraries like [`ftfy`](https://pypi.org/project/ftfy/) can be useful here.\n","- What data can you imagine existing that is NOT part of the dataset? Your neural network is not likely to generalize outside the specific distribution it was trained on. You need to understand the limitations of your trained classifier, and notice if you in fact need to collect different data to do the job properly:\n","    - What specific geographical area, time period, and demographic was the data sampled from? How does this compare to the deployment use case?\n","    - What filters were applied upstream that could leave \"holes\" in the distribution?\n","- What fraction of labels are objectively wrong?\n","    - Creating accurate labels is a laborious process and humans inevitably make mistakes. It's expensive to check and re-check labels, so most published datasets do contain incorrect labels.\n","    - Errors in training set labels can be mitigated through stronger regularization to prevent the model from memorizing the errors, or other techniques.\n","    - Most pernicious are errors in **test set** labels. Even a small percentage of these can cause us to select a model that outputs the (objectively mistaken) label over one that does the objectively right thing. The paper [Pervasive Label Errors in Test Sets\n","Destabilize Machine Learning Benchmarks](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/f2217062e9a397a1dca429e7d70bc6ca-Paper-round1.pdf) shows that these are more common than you might expect, and describes implications of this in more detail.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["230\n","False\n","test\n","I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n"]}],"source":["if MAIN:\n","    # plot the distribution of stars in the training set in blue and distribution of stars in the test set in red\n","    # plt.hist([r.stars for r in reviews if r.split == \"train\"], bins=10, alpha=0.5, label=\"train\")\n","    # plt.hist([r.stars for r in reviews if r.split == \"test\"], bins=10, alpha=0.5, label=\"test\")\n","    # plt.show()\n","\n","    # plot the character count of the reviews in the training set in blue and the test set in red\n","    # plt.hist([len(r.text) for r in reviews if r.split == \"train\"], bins=100, alpha=0.5, label=\"train\")\n","    # plt.hist([len(r.text) for r in reviews if r.split == \"test\"], bins=100, alpha=0.5, label=\"test\")\n","\n","    # count the number of reviews that contain common non-english in spanish, french, german, and italian\n","    print(sum((bool(re.search(r\"[áíóúñ]\", r.text)) for r in reviews)))\n","    print(reviews[0].is_positive)\n","    print(reviews[0].split)\n","    \n","    print(reviews[0].text)\n","\n","\n","    # print the first 10 reviews that contain non-english characters\n","    # for r in reviews:\n","    #     if re.search(r\"[áíóúñ]\", r.text):\n","    #         print(r.text)\n","    # result: all english?\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Tokenization\n","\n","When fine-tuning, you need to use the same tokenizer as was used for pre-training. The tokenizer already knows about the 512 token maximum, so it will take care of the truncation if we pass `truncation=True`. We also need to pad out the short reviews with a special padding token. In this case, `tokenizer.pad_token_id` is 0, but it's good practice to not assume this in our code.\n","\n","It's most convenient to tokenize the dataset once and store the preprocessed data. Roughly, how large will our preprocessed dataset be?\n","\n","<details>\n","\n","<summary>Solution - Train Data Size</summary>\n","\n","Naively, using int64 the tokens are 25,000 reviews * 512 tokens/review * 8 bytes/token or approximately 104MB. The labels are negligible.\n","\n","We know the maximum token value is `tokenizer.vocab_size` or 28996, so it's safe to use int16 instead, decreasing our storage requirements by a factor of 4 to approximately 25MB. We would also expect this data to be very compressible, since there are long strings of padding tokens.\n","\n","</details>\n","\n","Implement `to_dataset`. Calling this function could take a minute, as tokenization requires a lot of CPU even with the efficient Rust implementation provided by HuggingFace. We aren't writing our own tokenizer because it would be extremely slow to do it in pure Python.\n","\n","Note that you really don't want to have to do long-running tasks like this repeatedly. It's always a good idea to store the preprocessed data on disk and load that on future runs. Then you only have to re-run it if you want to preprocess the data in some different way.\n","\n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`token_type_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[39m=\u001b[39m as_tensor(value)\n\u001b[1;32m    719\u001b[0m     \u001b[39m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[39m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[39m#     tensor = tensor[None, :]\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[39], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m MAIN:\n\u001b[1;32m     27\u001b[0m     tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     train_data \u001b[39m=\u001b[39m to_dataset(tokenizer, [r \u001b[39mfor\u001b[39;49;00m r \u001b[39min\u001b[39;49;00m reviews \u001b[39mif\u001b[39;49;00m r\u001b[39m.\u001b[39;49msplit \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     29\u001b[0m     test_data \u001b[39m=\u001b[39m to_dataset(tokenizer, [r \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m reviews \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39msplit \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m     t\u001b[39m.\u001b[39msave((train_data, test_data), SAVED_TOKENS_PATH)\n","Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mto_dataset\u001b[0;34m(tokenizer, reviews)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_dataset\u001b[39m(tokenizer, reviews: \u001b[39mlist\u001b[39m[Review]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorDataset:\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tokenize the reviews (which should all belong to the same split) and bundle into a TensorDataset.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    The tensors in the TensorDataset should be (in this exact order):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m    star_labels: shape (batch, ), dtype int\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     encoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m     12\u001b[0m         [r\u001b[39m.\u001b[39;49mtext \u001b[39mfor\u001b[39;49;00m r \u001b[39min\u001b[39;49;00m reviews],\n\u001b[1;32m     13\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m         truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     input_ids \u001b[39m=\u001b[39m encoded[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m     attention_mask \u001b[39m=\u001b[39m encoded[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2800\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2790\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2791\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2792\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2793\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2797\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2798\u001b[0m )\n\u001b[0;32m-> 2800\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2801\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2802\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2803\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2804\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2805\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2806\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2807\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2808\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2809\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2810\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2811\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2812\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2813\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2814\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2815\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2816\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2817\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2818\u001b[0m )\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:477\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 477\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n","File \u001b[0;32m~/.pyenv/versions/3.10.7/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39moverflowing_tokens\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             )\n\u001b[0;32m--> 733\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mpadding=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncation=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m features (`\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`token_type_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."]}],"source":["def to_dataset(tokenizer, reviews: list[Review]) -> TensorDataset:\n","    \"\"\"Tokenize the reviews (which should all belong to the same split) and bundle into a TensorDataset.\n","\n","    The tensors in the TensorDataset should be (in this exact order):\n","\n","    input_ids: shape (batch, sequence length), dtype int64\n","    attention_mask: shape (batch, sequence_length), dtype int\n","    sentiment_labels: shape (batch, ), dtype int\n","    star_labels: shape (batch, ), dtype int\n","    \"\"\"\n","    encoded = tokenizer.batch_encode_plus(\n","        [r.text for r in reviews],\n","        return_tensors='pt',\n","        padding=True,\n","        truncation=True\n","    )\n","    input_ids = encoded['input_ids']\n","    attention_mask = encoded['attention_mask']\n","    sentiment_labels = t.tensor([int(r.is_positive) for r in reviews])\n","    star_labels = t.tensor([r.stars for r in reviews])\n","    return TensorDataset(input_ids, attention_mask, sentiment_labels, star_labels)        \n","                \n","\n","\n","\n","if MAIN:\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","    train_data = to_dataset(tokenizer, [r for r in reviews if r.split == \"train\"])\n","    test_data = to_dataset(tokenizer, [r for r in reviews if r.split == \"test\"])\n","    t.save((train_data, test_data), SAVED_TOKENS_PATH)\n","\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS] Hello world! [SEP]\n"]}],"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","print(tokenizer.decode(tokenizer.encode(\"Hello world!\")))\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Bonus\n","\n","Go on to Step 2, but if you have time at the end, you can come back and try the bonus exercise for this part.\n","\n","### Better Truncation\n","\n","We arbitrarily kept the first `max_length` tokens and truncated the rest. Is this strategy optimal? If you read some of the reviews, a common pattern is to sum up and conclude the review in the final 1-2 sentences.\n","\n","This suggests that we might do better by keeping some number of tokens at the end and truncating the middle instead. Implement this strategy and see if you can measure a difference in accuracy.\n","\n","### Better Data Cleaning\n","\n","You may have noticed that paragraph breaks are denoted by the string \"< br / > < br / >\". We might suppose that these are not very informative, and it would be better to strip them out. Particularly, if we are truncating our reviews then we would rather have 10 tokens of text than this in the truncated sequence. Replace these with the empty string (in all splits of the data) and see if you can measure a difference in accuracy.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":4}
