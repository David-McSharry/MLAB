{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D2 - Part 2 - DQN Algorithm\n\nIn this section, you'll implement Deep Q-Learning, often referred to as DQN for \"Deep Q-Network\". This was used in a landmark paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).\n\nAt the time, I was very excited about this paper - the idea that convolutional neural networks could look at Atari game pixels and \"see\" gameplay-relevant features like a Space Invader was new and noteworthy. In 2022, we take for granted that convnets work, so we're going to focus on the RL aspect and not the vision aspect today.\n\n## Fast Feedback Loops\n\nWe want to have faster feedback loops, and learning from Atari pixels doesn't achieve that. It might take 15 minutes per training run to get an agent to do well on Breakout, and that's if your implementation is relatively optimized. Even waiting 5 minutes to learn Pong from pixels will limit your ability to iterate relative to environments that are as simple as possible.\n\n## CartPole\n\nThe classic environment \"CartPole-v0\" is simple to understand, yet hard enough for a RL agent to be interesting. [Here's a video of a RL agent playing CartPole](https://www.youtube.com/watch?v=46wjA6dqxOM); by the end of the day your agent will be able to do this and more!\n\nYou can see the source for CartPole [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py); don't worry about the implementation but do read the documentation to understand the format of the actions and observations.\n\nThe simple physics involved would be very easy for a model-based algorithm to fit, but today we're doing it model-free: your agent has no idea that these observations represent positions or velocities, and it has no idea what the laws of physics are.\n\nEach environment can have different versions registered to it. By consulting [the Gym source](https://github.com/openai/gym/blob/master/gym/envs/__init__.py) you can see that CartPole-v0 and CartPole-v1 are the same environment, except that v1 has longer episodes. Again, a minor change like this can affect what algorithms score well; an agent might consistently survive for 200 steps but not be stable out to 500 steps.\n\n## Beyond CartPole\n\nIf things go well and your agent masters CartPole, the next harder challenges are [Acrobot-v1](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py), and [MountainCar-v0](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py). These also have discrete action spaces, which are the only type we're dealing with today. Feel free to Google for appropriate hyperparameters for these other problems - in a real RL problem you would have to run a hyperparameter search using the techniques we learned on a previous day because bad hyperparameters in RL often completely fail to learn, even if the algorithm is perfectly correct.\n\nThere are many more exciting environments to play in, but generally they're going to require more compute and more optimization than we have time for today. If you finish the main material, some ones I like are:\n\n- [Minimalistic Gridworld Environments](https://github.com/Farama-Foundation/gym-minigrid) - a fast gridworld environment for experiments with sparse rewards and natural language instruction.\n- [microRTS](https://github.com/santiontanon/microrts) - a small real-time strategy game suitable for experimentation.\n- [Megastep](https://andyljones.com/megastep/) - RL environment that runs fully on the GPU (fast!)\n\n## Outline of the Exercises\n\n- Implement the Q-network that maps a state to an estimated value for each action.\n- Implement the policy which chooses actions based on the Q-network, plus epsilon greedy randomness just like in the bandit case.\n- Implement a replay buffer to the collected (state, action, reward, next_state) tuples, which are called experiences for short.\n- Piece everything together into a training loop and train your agent.\n\n## Interesting Resources (not required reading)\n\n- [An Outsider's Tour of Reinforcement Learning](http://www.argmin.net/2018/06/25/outsider-rl/) - comparison of RL techniques with the engineering discipline of control theory.\n- [Towards Characterizing Divergence in Deep Q-Learning](https://arxiv.org/pdf/1903.08894.pdf) - analysis of what causes learning to diverge.\n- [Divergence in Deep Q-Learning: Tips and Tricks](https://amanhussain.com/post/divergence-deep-q-learning/) - includes some plots of average returns for comparison.\n- [Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures) - 2017 bootcamp with video and slides. Good if you like videos.\n- [DQN debugging using OpenAI gym Cartpole](https://adgefficiency.com/dqn-debugging/) - random dude's adventures in trying to get it to work.\n- [CleanRL DQN](https://github.com/vwxyzjn/cleanrl) - single file implementations of RL algorithms. Your starter code today is based on this; try not to spoiler yourself by looking at the solutions too early!\n- [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html) - 2018 article describing difficulties preventing industrial adoption of RL.\n- [Deep Reinforcement Learning Works - Now What?](https://tesslerc.github.io/posts/drl_works_now_what/) - 2020 response to the previous article highlighting recent progress.\n- [Seed RL](https://github.com/google-research/seed_rl) - example of distributed RL using Docker and GCP.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import argparse\nimport os\nimport random\nimport sys\nimport time\nimport dataclasses\nfrom dataclasses import dataclass\nfrom distutils.util import strtobool\nfrom typing import Any, Iterable, List, Optional, Tuple, Union\nimport gym\nimport gym.envs.registration\nimport numpy as np\nimport torch\nimport torch as t\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom einops import rearrange\nfrom gym.spaces import Box, Discrete\nfrom matplotlib import pyplot as plt\nfrom numpy.random import Generator\nfrom torch.utils.tensorboard import SummaryWriter\nimport w3d2_test\nfrom w3d2_utils import make_env\n\nMAIN = __name__ == \"__main__\"\nIS_CI = os.getenv(\"IS_CI\")\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## The Q-network\n\nThe Q-network takes in an observation and outputs a number for each available action that predicts how good it is. For best results, the architecture of the Q-network can be customized to each particular problem. For example, [the architecture of OpenAI Five](https://cdn.openai.com/research-covers/openai-five/network-architecture.pdf) used to play DOTA 2 is quite complex and involves LSTMs.\n\nFor learning from pixels, a simple convolutional network and some fully connected layers does quite well. Where we have already processed features here, it's even easier - an MLP of this size should be plenty large for any environment today. Your code should support running the network on either the GPU or CPU, but for CartPole it was actually faster to use the CPU on my hardware.\n\nImplement the Q-network using alternating Linear and ReLU layers. Should there be a ReLU at the very end? Why or why not?\n\n<details>\n\n<summary>Solution - ReLU at the end</summary>\n\nIf you end with a ReLU, then your network can only predict 0 or positive rewards. This will cause problems as soon as you encounter an environment with negative rewards or you try to do some scaling of the rewards.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class QNetwork(nn.Module):\n    def __init__(self, num_observations: int, num_actions: int, hidden_sizes: list[int] = [120, 84]):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        pass\n\n\nif MAIN:\n    net = QNetwork(num_observations=4, num_actions=2)\n    n_params = sum((p.nelement() for p in net.parameters()))\n    print(net)\n    print(f\"Total number of parameters: {n_params}\")\n    assert n_params == 10934\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Replay Buffer\n\nIn the supervised learning setting, training examples should be drawn i.i.d. from some distribution, and we hope to generalize to future examples from that distribution.\n\nIn RL, the distribution of experiences is a moving target that changes every time the policy changes. This is why the training loss curve isn't going to have a nice steady decrease like in supervised learning.\n\nAnother problem is that states within an episode are correlated and not i.i.d. at all. For example, in a chess game if you happen to be far ahead in pieces, for the rest of the game most actions will have a much higher than average expected reward. Even bad moves will have a high reward if they aren't bad enough to cause you to actually lose. If a batch of these actions was presented to the network, it would not have a representative sample of what chess is like.\n\n### Replay buffer with uniform sampling\n\nA simple strategy that works decently well to combat these problems is to store the agent's \"experiences\" in a buffer, which is then sampled from to train the policy further. This works as follows:\n1. We start with a policy which we run in the environment for some number of steps.\n2. At each step we take the (`observation`, `action`, `reward`, `done`, `next_observation`) information that we got from that step and add it to the buffer.\n3. After we've done this for enough steps, we can randomly sample a batch of experience tuples from the buffer to train the policy further.\n\nIntuitively, if we want the policy to play well in all sorts of states, the sampled batch should be a representative sample of all the diverse scenarios that can happen in the environment.\n\nFor complex environments, this implies a very large batch size (or doing something better than uniform sampling). [OpenAI Five](https://cdn.openai.com/dota-2.pdf), used batch sizes of over 2 million experiences for Dota 2.\n\nThe capacity of the replay buffer is yet another hyperparameter; if it's too small then it's just going to be full of recent and correlated examples. But if it's too large, we pay an increasing cost in memory usage and the information may be too old to be relevant.\n\nImplement `ReplayBuffer`. It only needs to handle a discrete action space, and you can assume observations are some shape of dtype `np.float32`.\n\n### Vectorized Environments\n\nOur agent won't interact directly with an instance of the CartPole `Env`, but with a wrapper class `gym.vector.SyncVectorEnv` which behaves like an array of `Env` instances. This doesn't actually make things any faster because it's just a regular `for` loop inside, but it means you can conveniently call `step` with an array of actions (one for each instance) and get back arrays of observations, rewards, etc.\n\nWe're introducing this today because PPO will use it tomorrow with multiple instances inside, but DQN will only use it with a single instance.\n\nIn general, don't worry about execution speed today. It's okay to use for loops and accept that not everything can be efficiently vectorized. If we actually wanted to go fast, we would likely ditch Python entirely and write everything in C++.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@dataclass\nclass ReplayBufferSamples:\n    \"\"\"\n    Samples from the replay buffer, converted to PyTorch for use in neural network training.\n\n    obs: shape (sample_size, *observation_shape), dtype t.float\n    actions: shape (sample_size, ) dtype t.int\n    rewards: shape (sample_size, ), dtype t.float\n    dones: shape (sample_size, ), dtype t.bool\n    next_observations: shape (sample_size, *observation_shape), dtype t.float\n    \"\"\"\n\n    observations: t.Tensor\n    actions: t.Tensor\n    rewards: t.Tensor\n    dones: t.Tensor\n    next_observations: t.Tensor\n\n\nclass ReplayBuffer:\n    rng: Generator\n\n    def __init__(self, buffer_size: int, num_actions: int, observation_shape: tuple, num_environments: int, seed: int):\n        assert num_environments == 1, \"This buffer only supports SyncVectorEnv with 1 environment inside.\"\n        pass\n\n    def add(\n        self, obs: np.ndarray, actions: np.ndarray, rewards: np.ndarray, dones: np.ndarray, next_obs: np.ndarray\n    ) -> None:\n        \"\"\"\n        obs: shape (num_environments, *observation_shape) Observation before the action\n        actions: shape (num_environments, ) the action chosen by the agent\n        rewards: shape (num_environments, ) the reward after the after\n        dones: shape (num_environments, ) if True, the episode ended and was reset automatically\n        next_obs: shape (num_environments, *observation_shape) Observation after the action. If done is True, this should be the terminal observation, NOT the first observation of the next episode.\n        \"\"\"\n        pass\n\n    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:\n        \"\"\"Uniformly sample sample_size entries from the buffer and convert them to PyTorch tensors on device.\n\n        Sampling is with replacement, and sample_size may be larger than the buffer size.\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w3d2_test.test_replay_buffer_single(ReplayBuffer)\n    w3d2_test.test_replay_buffer_deterministic(ReplayBuffer)\n    w3d2_test.test_replay_buffer_wraparound(ReplayBuffer)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Environment Resets\n\nThere's a subtlety to the Gym API around what happens when the agent fails and the episode is terminated. Our environment is set up to automatically reset at the end of an episode, but when this happens the `next_obs` returned from `step` is actually the initial observation of the new episode.\n\nWhat we want to store in the replay buffer is the final observation of the old episode. The code to do this is shown below.\n\n- Run the code and inspect the replay buffer contents. Referring back to the [CartPole source](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py), do the numbers make sense?\n- Look at the sample, and see if it looks properly shuffled.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    rb = ReplayBuffer(buffer_size=256, num_actions=2, observation_shape=(4,), num_environments=1, seed=0)\n    envs = gym.vector.SyncVectorEnv([make_env(\"CartPole-v0\", 0, 0, False, \"test\")])\n    obs = envs.reset()\n    for i in range(512):\n        actions = np.array([0])\n        (next_obs, rewards, dones, infos) = envs.step(actions)\n        real_next_obs = next_obs.copy()\n        for (i, done) in enumerate(dones):\n            if done:\n                real_next_obs[i] = infos[i][\"terminal_observation\"]\n        rb.add(obs, actions, rewards, dones, real_next_obs)\n        obs = next_obs\n    sample = rb.sample(128, t.device(\"cpu\"))\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Epsilon Greedy Policy\n\nIn later methods like policy gradient, we'll explicitly represent the policy as its own neural network. In DQN, the policy is implicitly defined by the Q-network: we take the action with the maximum predicted reward. This means we'll tend to choose actions which the Q-network is overly optimistic about, and then our agent will get less reward than expected.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def epsilon_greedy_policy(\n    envs: gym.vector.SyncVectorEnv, q_network: QNetwork, rng: Generator, obs: t.Tensor, epsilon: float\n) -> np.ndarray:\n    \"\"\"With probability epsilon, take a random action. Otherwise, take a greedy action according to the q_network.\n\n    cartpole: obs shape (envs, obs space), action: (shape, )\n    probs: obs shape (envs,)\n\n    Return: (n_environments, ) the sampled action for each environment.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w3d2_test.test_epsilon_greedy_policy(epsilon_greedy_policy)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Exploration\n\nDQN makes no attempt to explore intelligently. The exploration strategy is the same as the basic approach from Chapter 2 of Sutton and Barto: agents take a random action with probability `epsilon`, and `epsilon` is gradually decreased. The Q-network is also randomly initialized, so its predictions of what is the best action to take are also pretty random to start.\n\nSome games like Montezuma's Revenge have sparse rewards that require more advanced exploration methods to obtain. The player is required to collect specific keys to unlock specific doors, but unlike humans, DQN has no prior knowledge about what a key or a door is, and it turns out that bumbling around randomly has too low of a probability of correctly matching a key to its door. Even if the agent does manage to do this, the long separation between finding the key and going to the door makes it hard to learn that picking the key up was important.\n\nAs a result, DQN scored an embarrassing 0% of average human performance on this game.\n\n### Reward Shaping\n\nOne solution to sparse rewards is to use human knowledge to define auxillary reward functions that are more dense and make the problem easier. In this specific example, we could give it a reward for picking up a key. What could possibly go wrong?\n\n### Reward Hacking\n\nOne time this was tried, the reward was given slightly too early and the agent learned it could go close to the key without quite picking it up, obtain the auxillary reward, and then back up and repeat. See [here](https://www.youtube.com/watch?v=_sFp1ffKIc8&list=PLehfUY5AEKX-g-QNM7FsxRHgiTOCl-1hv&index=4) for a video of this happening.\n\nMore examples of unintended agent behavior are [here](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml).\n\n### Advanced Exploration\n\nIt would be better if the agent didn't require these auxillary rewards, and could rely on other signals in the environment that a state might be worth exploring. One idea is that a state which is \"surprising\" or \"unpredictable\" in some sense might be valuable. In 2018, OpenAI released [Random Network Distillation](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/) which made progress in operationalizing this notion. In 2019, an excellent paper [First return, then explore](https://arxiv.org/pdf/2004.12919v6.pdf) found an even better approach.\n\nFor now, implement the basic linearly decreasing exploration schedule. Plot it and check that it looks right.\n\n<details>\n\n<summary>Solution - Plot of the Intended Schedule</summary>\n\n<p align=\"center\">\n    <img src=\"w3d2_epsilon_schedule.png\"/>\n</p>\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def linear_schedule(\n    current_step: int, start_e: float, end_e: float, exploration_fraction: float, total_timesteps: int\n) -> float:\n    \"\"\"Return the appropriate epsilon for the current step.\n\n    Epsilon should be start_e at step 0 and decrease to end_e at step (exploration_fraction * total_timesteps).\n    \"\"\"\n    pass\n\n\nif MAIN:\n    epsilons = [\n        linear_schedule(step, start_e=1.0, end_e=0.05, exploration_fraction=0.5, total_timesteps=500)\n        for step in range(500)\n    ]\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Probe Environments\n\nExtremely simple probe environments are a great way to debug your algorithm. Implement the first probe environment below.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "ObsType = np.ndarray\nActType = int\n\n\nclass Probe1(gym.Env):\n    \"\"\"One action, observation of [0.0], one timestep long, +1 reward.\n\n    We expect the agent to rapidly learn that the value of the constant [0.0] observation is +1.0. Note we're using a continuous observation space for consistency with CartPole.\n    \"\"\"\n\n    action_space: Discrete\n    observation_space: Box\n\n    def __init__(self):\n        pass\n\n    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n        pass\n\n    def reset(\n        self, seed: Optional[int] = None, return_info=False, options=None\n    ) -> Union[ObsType, tuple[ObsType, dict]]:\n        pass\n\n\ngym.envs.registration.register(id=\"Probe1-v0\", entry_point=Probe1)\nif MAIN:\n    env = gym.make(\"Probe1-v0\")\n    assert env.observation_space.shape == (1,)\n    assert env.action_space.shape == ()\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Additional Probe Environments\n\nFeel free to skip ahead for now, and implement these as needed to debug your model.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class Probe2(gym.Env):\n    \"\"\"One action, observation of [-1.0] or [+1.0], one timestep long, reward equals observation.\n\n    We expect the agent to rapidly learn the value of each observation is equal to the observation.\n    \"\"\"\n\n    action_space: Discrete\n    observation_space: Box\n\n    def __init__(self):\n        pass\n\n    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n        pass\n\n    def reset(\n        self, seed: Optional[int] = None, return_info=False, options=None\n    ) -> Union[ObsType, tuple[ObsType, dict]]:\n        pass\n\n\ngym.envs.registration.register(id=\"Probe2-v0\", entry_point=Probe2)\n\n\nclass Probe3(gym.Env):\n    \"\"\"One action, [0.0] then [1.0] observation, two timesteps, +1 reward at the end.\n\n    We expect the agent to rapidly learn the discounted value of the initial observation.\n    \"\"\"\n\n    action_space: Discrete\n    observation_space: Box\n\n    def __init__(self):\n        pass\n\n    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n        pass\n\n    def reset(\n        self, seed: Optional[int] = None, return_info=False, options=None\n    ) -> Union[ObsType, tuple[ObsType, dict]]:\n        pass\n\n\ngym.envs.registration.register(id=\"Probe3-v0\", entry_point=Probe3)\n\n\nclass Probe4(gym.Env):\n    \"\"\"Two actions, [0.0] observation, one timestep, reward is -1.0 or +1.0 dependent on the action.\n\n    We expect the agent to learn to choose the +1.0 action.\n    \"\"\"\n\n    action_space: Discrete\n    observation_space: Box\n\n    def __init__(self):\n        pass\n\n    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n        pass\n\n    def reset(\n        self, seed: Optional[int] = None, return_info=False, options=None\n    ) -> Union[ObsType, tuple[ObsType, dict]]:\n        pass\n\n\ngym.envs.registration.register(id=\"Probe4-v0\", entry_point=Probe4)\n\n\nclass Probe5(gym.Env):\n    \"\"\"Two actions, random 0/1 observation, one timestep, reward is 1 if action equals observation otherwise -1.\n\n    We expect the agent to learn to match its action to the observation.\n    \"\"\"\n\n    action_space: Discrete\n    observation_space: Box\n\n    def __init__(self):\n        pass\n\n    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n        pass\n\n    def reset(\n        self, seed: Optional[int] = None, return_info=False, options=None\n    ) -> Union[ObsType, tuple[ObsType, dict]]:\n        pass\n\n\ngym.envs.registration.register(id=\"Probe5-v0\", entry_point=Probe5)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Main DQN Algorithm\n\nAll the boilerplate code is provided for you. You just need to fill in the three placeholders as indicated, referring to Algorithm 1 in the [DQN paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) (or the [DQN step-by-step blog post](https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b)). Note that since you aren't preprocessing pixels, your version will just directly store states.\n\nA sample invocation that will let you see your agent's progress in Weights and Biases, including videos of it playing episodes would look like:\n\n`python your_script.py --track --capture-video`\n\nI recommend that you start by running statements line by line in the REPL and verifying that things are as you expect, then pasting them into the function when they work.\n\nDon't be discouraged if it's not working - it's normal for debugging RL to take longer than you would expect. Add asserts or your own tests, implement an appropriate probe environment, try anything in the Andy Jones post that sounds promising, and try to notice confusion.\n\nOne more tip: when gathering experiences, make sure you have a line `obs = next_obs` at an appropriate location, or you'll just keep passing in the same observation on every iteration. I've already forgotten this line twice.\n\n### Logging Metrics\n\nYou can view your metrics either in the IDE using Tensorboard (VS Code has built-in Tensorboard support) or remotely on Weights and Biases. Some of the logs won't work properly until you define a variable with the expected name.\n\n### Expected Behavior of the Loss\n\nIn supervised learning, we want our loss to be decreasing and it's a bad sign if it's actually increasing. In RL, it's the reward that should be (noisily) increasing over time.\n\nOur agent's loss function just reflects how close together the Q-network's estimates are to the experiences currently sampled from the replay buffer.\n\nThis means that once the agent starts to learn something and do better at the problem, it's expected for the loss to increase.\n\nFor example, the Q-network initially learned some state was bad, because an agent that reached them was just derping around randomly and died shortly after. But now it's getting evidence that the same state is good, now that the agent that reached the state has a better idea what to do next. A higher loss is thus actually a good sign that learning is actively occurring if it's correlated with an increase in average reward obtained.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "@dataclass\nclass DQNArgs:\n    exp_name: str = os.path.basename(globals().get(\"__file__\", \"DQN_implementation\").rstrip(\".py\"))\n    seed: int = 1\n    torch_deterministic: bool = True\n    cuda: bool = True\n    track: bool = False\n    wandb_project_name: str = \"w3d2\"\n    wandb_entity: Optional[str] = None\n    capture_video: bool = False\n    env_id: str = \"CartPole-v0\"\n    total_timesteps: int = 500000\n    learning_rate: float = 0.00025\n    buffer_size: int = 10000\n    gamma: float = 0.99\n    target_network_frequency: int = 500\n    batch_size: int = 128\n    start_e: float = 1.0\n    end_e: float = 0.05\n    exploration_fraction: float = 0.5\n    learning_starts: int = 10000\n    train_frequency: int = 10\n    use_target_network: bool = False\n\n\narg_help_strings = {\n    \"exp_name\": \"the name of this experiment\",\n    \"seed\": \"seed of the experiment\",\n    \"torch_deterministic\": \"if toggled, `torch.backends.cudnn.deterministic=False`\",\n    \"cuda\": \"if toggled, cuda will be enabled by default\",\n    \"track\": \"if toggled, this experiment will be tracked with Weights and Biases\",\n    \"wandb_project_name\": \"the wandb's project name\",\n    \"wandb_entity\": \"the entity (team) of wandb's project\",\n    \"capture_video\": \"whether to capture videos of the agent performances (check out `videos` folder)\",\n    \"env_id\": \"the id of the environment\",\n    \"total_timesteps\": \"total timesteps of the experiments\",\n    \"learning_rate\": \"the learning rate of the optimizer\",\n    \"buffer_size\": \"the replay memory buffer size\",\n    \"gamma\": \"the discount factor gamma\",\n    \"target_network_frequency\": \"the timesteps it takes to update the target network\",\n    \"batch_size\": \"the batch size of samples from the replay memory\",\n    \"start_e\": \"the starting epsilon for exploration\",\n    \"end_e\": \"the ending epsilon for exploration\",\n    \"exploration_fraction\": \"the fraction of `total-timesteps` it takes from start-e to go end-e\",\n    \"learning_starts\": \"timestep to start learning\",\n    \"train_frequency\": \"the frequency of training\",\n    \"use_target_network\": \"If True, use a target network.\",\n}\ntoggles = [\"torch_deterministic\", \"cuda\", \"track\", \"capture_video\"]\n\n\ndef parse_args(arg_help_strings=arg_help_strings, toggles=toggles) -> DQNArgs:\n    parser = argparse.ArgumentParser()\n    for field in dataclasses.fields(DQNArgs):\n        flag = \"--\" + field.name.replace(\"_\", \"-\")\n        type_function = field.type if field.type != bool else lambda x: bool(strtobool(x))\n        toggle_kwargs = {\"nargs\": \"?\", \"const\": True} if field.name in toggles else {}\n        parser.add_argument(\n            flag, type=type_function, default=field.default, help=arg_help_strings[field.name], **toggle_kwargs\n        )\n    return DQNArgs(**vars(parser.parse_args()))\n\n\ndef setup(args: DQNArgs) -> Tuple[str, SummaryWriter, np.random.Generator, t.device, gym.vector.SyncVectorEnv]:\n    \"\"\"Helper function to set up useful variables for the DQN implementation\"\"\"\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n    )\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n    rng = np.random.default_rng(args.seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n    envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n    assert isinstance(envs.single_action_space, Discrete), \"only discrete action space is supported\"\n    return (run_name, writer, rng, device, envs)\n\n\ndef log(\n    writer: SummaryWriter,\n    start_time: float,\n    step: int,\n    predicted_q_vals: t.Tensor,\n    loss: Union[float, t.Tensor],\n    infos: Iterable[dict],\n    epsilon: float,\n):\n    \"\"\"Helper function to write relevant info to TensorBoard logs, and print some things to stdout\"\"\"\n    if step % 100 == 0:\n        writer.add_scalar(\"losses/td_loss\", loss, step)\n        writer.add_scalar(\"losses/q_values\", predicted_q_vals.mean().item(), step)\n        writer.add_scalar(\"charts/SPS\", int(step / (time.time() - start_time)), step)\n        if step % 10000 == 0:\n            print(\"SPS:\", int(step / (time.time() - start_time)))\n    for info in infos:\n        if \"episode\" in info.keys():\n            print(f\"global_step={step}, episodic_return={info['episode']['r']}\")\n            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], step)\n            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], step)\n            writer.add_scalar(\"charts/epsilon\", epsilon, step)\n            break\n\n\ndef train_dqn(args: DQNArgs):\n    (run_name, writer, rng, device, envs) = setup(args)\n    \"YOUR CODE: Create your Q-network, Adam optimizer, and replay buffer here.\"\n    start_time = time.time()\n    obs = envs.reset()\n    for step in range(args.total_timesteps):\n        \"YOUR CODE: Sample actions, step the environment, and record the results in the replay buffer here.\"\n        if step > args.learning_starts and step % args.train_frequency == 0:\n            \"YOUR CODE: Sample from the replay buffer, compute the TD target, compute TD loss, and perform an optimizer step.\"\n            log(writer, start_time, step, predicted_q_vals, loss, infos, epsilon)\n    envs.close()\n    writer.close()\n\n\nif MAIN and (not IS_CI):\n    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n        filename = globals().get(\"__file__\", \"<filename of this script>\")\n        print(f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\")\n        args = DQNArgs()\n    else:\n        args = parse_args()\n    train_dqn(args)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Bonus\n\n### Target Network\n\nWhen `--use-target-network` is provided, create a copy of the Q-network called the target network, and use that instead of the Q-network to compute the target for the loss.\n\nThe target network is never trained with gradient descent; every `target_network_frequency` steps, overwrite the target network with the Q-network's weights.\n\nDoes this improve performance? Try to come up with a robust way to measure if this is beneficial or not.\n\n### Dueling DQN\n\nImplement dueling DQN according to [the paper](https://arxiv.org/pdf/1511.06581.pdf) and compare its performance.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}