{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# W1D3 - Build Your Own Backpropagation Framework\n","\n","Today you're going to build your very own system that can run the backpropagation algorithm in essentially the same way as PyTorch does. By the end of the day, you'll be able to train a multi-layer perceptron neural network using your own backprop system!\n","\n","The main differences between the full PyTorch and our version are:\n","\n","- We will focus on CPU only, as all the ideas are the same on GPU.\n","- We will use NumPy arrays internally instead of ATen, the C++ array type used by PyTorch. Backpropagation works independently of the array type.\n","- A real `torch.Tensor` has about 700 fields and methods. We will only implement a subset that are particularly instructional and/or necessary to train the MLP.\n","\n","## Table of Contents\n","\n","- [Readings](#readings)\n","- [Computing Gradients with Backpropagation](#computing-gradients-with-backpropagation)\n","    - [Backward Functions](#backward-functions)\n","    - [Backpropagation](#backpropagation)\n","- [Static Typing In Python Crash Course](#static-typing-in-python-crash-course)\n","- [Backward functions](#backward-functions)\n","    - [Backward function of log](#backward-function-of-log)\n","- [Backwards Functions of Two Tensors](#backwards-functions-of-two-tensors)\n","    - [Broadcasting Rules](#broadcasting-rules)\n","    - [Backward Function for Elementwise Multiply](#backward-function-for-elementwise-multiply)\n","- [Autograd](#autograd)\n","    - [Wrapping Arrays (Tensor)](#wrapping-arrays-tensor)\n","    - [Recipe](#recipe)\n","- [Registering backwards functions](#registering-backwards-functions)\n","- [Tensors](#tensors)\n","    - [requires_grad](#requiresgrad)\n","- [Forward Pass: Building the Computational Graph](#forward-pass-building-the-computational-graph)\n","- [Forward Pass - Generic Version](#forward-pass---generic-version)\n","- [Backpropagation](#backpropagation-)\n","    - [Topological Sort](#topological-sort)\n","    - [The Actual Backprop Function](#the-actual-backprop-function)\n","- [Filling out the Tensor class with forward and backward methods](#filling-out-the-tensor-class-with-forward-and-backward-methods)\n","- [Non-Differentiable Functions](#non-differentiable-functions)\n","- [Implementing `negative`](#implementing-negative)\n","- [Implementing `exp`](#implementing-exp)\n","- [Reshape](#reshape)\n","- [Permute](#permute)\n","- [Expand](#expand)\n","- [Backwards Pass - Sum](#backwards-pass---sum)\n","- [Backwards Pass - Indexing](#backwards-pass---indexing)\n","- [Elementwise addition, subtract, true_divide](#elementwise-addition-subtract-truedivide)\n","- [In-Place Operations](#in-place-operations)\n","- [Mixed scalar-tensor operations](#mixed-scalar-tensor-operations)\n","- [Splitting Gradients - elementwise maximum](#splitting-gradients---elementwise-maximum)\n","- [Functional ReLU](#functional-relu)\n","- [2D Matrix Multiply](#d-matrix-multiply)\n","- [Build Your Own `nn.Parameter`](#build-your-own-nnparameter)\n","- [Build Your Own `nn.Module`](#build-your-own-nnmodule)\n","- [Build Your Own Linear Layer](#build-your-own-linear-layer)\n","- [Build Your Own Cross-Entropy Loss](#build-your-own-cross-entropy-loss)\n","- [Build Your Own `no_grad`](#build-your-own-nograd)\n","- [Training Your Network](#training-your-network)\n","    - [Training Loop](#training-loop)\n","- [Bonus](#bonus)\n","    - [In-Place Operation Warnings](#in-place-operation-warnings)\n","    - [In-Place ReLU](#in-place-relu)\n","    - [Backward for einsum](#backward-for-einsum)\n","    - [Reuse of Module during forward](#reuse-of-module-during-forward)\n","    - [ResNet Support](#resnet-support)\n","    - [Central Difference Checking](#central-difference-checking)\n","    - [Non-Differentiable Function Support](#non-differentiable-function-support)\n","    - [Differentiation wrt Keyword Arguments](#differentiation-wrt-keyword-arguments)\n","    - [torch.stack](#torchstack)\n","\n","## Readings\n","\n","- [Max's Extension of Chris Olah's \"Calculus on Computational Graphs: Backpropagation\"](https://witty-mirror-0a0.notion.site/Automatic-differentiation-f5f45f26ec47431fadbd67f831700d5c)\n","\n","## Computing Gradients with Backpropagation\n","\n","This section will briefly review the backpropagation algorithm, but focus mainly on the concrete implementation in software.\n","\n","To train a neural network, we want to know how the loss would change if we slightly adjust one of the learnable parameters.\n","\n","One obvious and straightforward way to do this would be just to add a small value $\\epsilon$ to the parameter, and run the forward pass again. This is called *finite differences*, and the main issue is we need to run a forward pass for every single parameter that we want to adjust. This method is infeasible for large networks, but it's important to know as a way of sanity checking other methods.\n","\n","A second obvious way is to write out the function for the entire network, and then symbolically take the gradient to obtain a symbolic expression for the gradient. This also works and is another thing to check against, but the expression gets quite complicated.\n","\n","Suppose that you have some computation graph, and you want to determine the derivative of the some scalar loss L with respect to NumPy arrays a, b, and c:\n","\n","```mermaid\n","\n","graph LR\n","    a & b --> Mul1{Mul}\n","    b & c --> Add1{Add}\n","    Mul1 --> d --> Add2{Add}\n","    Add1 --> e --> Add2\n","    Add2 --> L\n","```\n","\n","This graph corresponds to the following Python:\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","The goal of our system is that users can write ordinary looking Python code like this and have all the book-keeping needed to perform backpropagation happen behind the scenes. To do this, we're going to wrap each array and each function in special objects from our library that do the usual thing plus build up this graph structure that we need.\n","\n","### Backward Functions\n","\n","We've drawn our computation graph from left to right and the arrows pointing to the right, so that in the forward pass, boxes to the right depend on boxes to the left. In the backwards pass, the opposite is true: the gradient of boxes on the left depends on the gradient of boxes on the right.\n","\n","We're going to forbid any cycles in the graph to guarantee it's possible to do what we want, which is traverse the graph once from right to left. This means no in-place operations that modify memory. We can still write code that reuses variable names like `a = a + b`, but it's important to understand that the `a` on the left hand side is a separate instance of `Tensor` than the one on the right hand side.\n","\n","Each time we encounter an instance of function application, we can use the chain rule from calculus to proceed one step further to the left. For example, if we have `d = a * b`, then:\n","\n","$$\\frac{dL}{da} = \\frac{dL}{dd}\\frac{dd}{da} = \\frac{dL}{dd}b$$\n","$$\\frac{dL}{db} = \\frac{dL}{dd}\\frac{dd}{db} = \\frac{dL}{dd}a$$\n","\n","In other words, if we already know $\\frac{dL}{dd}$ and want to obtain $\\frac{dL}{da}$, then the function $f(x) = b * x$ will do the job for us. This function $f$ is telling us how \"sensitive\" $\\frac{dL}{da}$ is to small changes in `a`. We will write one of these \"backwards functions\" for each function and each input argument we want to support. To keep things simple for now, we will assume that the input arguments can be numbered by their position.\n","\n","### Backpropagation\n","\n","After all this setup, the actual backpropagation process becomes straightforward. First sort the nodes into a reverse [topological ordering](https://en.wikipedia.org/wiki/Topological_sorting) and then iterate over them and call each backward function exactly once. Because we visit the nodes in this special ordering, we've guaranteed that the argument $\\frac{dL}{d_{out}}$ that our function needs has been computed already and is available for use.\n","\n","At the first (rightmost) node we visit, the argument to `backward()` will be just $\\frac{dL}{dL} = 1$.\n","\n","It's important that the grads be accumulated instead of overwritten in a case like value $b$ which has two outgoing edges, since $dL/db$ will then be the sum of two terms. Since addition is commutative it doesn't matter whether we `backward()` the Mul or the Add that depend on $b$ first.\n","\n","Accumulating grads is also important for the case where you want to make multiple forward and backward calls before adjusting the weights. For example, if you want to have a batch size of 32 but only 8 inputs fit on your GPU, you can run 4 forward and backward calls before each call to the optimizer.\n","\n","## Static Typing In Python Crash Course\n","\n","Static typing is optional in Python and represents a tradeoff: you spend more time writing type annotations, but then you can catch errors in your IDE instead of having to wait for tests to fail or a runtime exception.\n","\n","Today it's going to be particularly beneficial to you to use type annotations, because your code will have a variety of types and it's easy to mix them up. It's also good practice to annotate APIs that other developers will work with as a form of documentation.\n","\n","For the most part, you can figure type annotations out just by looking at the provided example code. Here are some additional tips:\n","\n","- You can enable strict type checking in `.vscode/settings.json` by setting `\"python.analysis.typeCheckingMode\"` to `basic` or `strict`.\n","- VS Code shows type errors in the \"Problems\" tab.\n","- Hover over a symbol with the mouse in VS Code to see what type the checker thinks something is. If it already inferred the type correctly, there's probably no benefit to adding an annotation.\n","- If you want to name a type that hasn't been defined yet, just write the type's name as a string. For example list['CustomType'] works even if CustomType is defined later in the file.\n","- Since Python 3.9, you can call the type of a dict with s tring keys and float values `dict[str, float]`.\n","- Similarly `list[int]` is the type of a list of int, or `tuple[float, float]` for a 2-tuple of floats.\n","- The standard library `typing` module has some useful types:\n","    - `Union[type1, type2]` means either type1 or type2.\n","    - `Optional[type1]` is another way to write `Union[type1, None]`\n","    - `Any` means the object can legally have any methods or fields. This is different from `object`, which is the base class for everything and has no methods or fields.\n","    - `Callable[[argtype1, argtype2], returntype]` is useful if you want to pass a function as an argument.\n","    - If you want to type a variable number of arguments, just write the type of one of them: `def forward(self, *args: float)` takes zero or more floats as arguments.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import time\n","from collections import defaultdict\n","from dataclasses import dataclass\n","from typing import Any, Callable, Iterable, Iterator, Optional, Protocol, Union\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import w1d3_test\n","import w1d3_utils\n","from einops import rearrange, repeat, reduce\n","\n","MAIN = __name__ == \"__main__\"\n","IS_CI = os.getenv(\"IS_CI\")\n","Arr = np.ndarray\n","grad_tracking_enabled = True\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Backward functions\n","\n","During backpropagation, for each forward function in our computational graph we need to find the partial derivative of the output with respect to each of its inputs. Each partial is then multiplied by the gradient of the loss with respect to the forward functions output (`grad_out`) to find the gradient of the loss with respect to each input. We'll handle these calculations using backward functions.\n","\n","### Backward function of log\n","\n","Consider this computation graph where L is the loss:\n","\n","```mermaid\n","\n","graph LR\n","    a --> Log1 --> b --> Log2 --> c --> Sum --> L\n","```\n","\n","`log_back` will be called twice during backpropagation:\n","\n","- $log\\_back(\\partial L / \\partial c, c, b)$ is the first call at node Log2 and should return $\\partial L / \\partial b$\n","- $log\\_back(\\partial L / \\partial b, b, a)$ is the second call at node Log1 and should return $\\partial L / \\partial a$\n","\n","Implement `log_back` so it does the right thing.\n","\n","<details>\n","\n","<summary>Solution - log_back</summary>\n","\n","Consider the first call and the first scalar element of `c`, which is equal to the log of the first scalar element of `b`. By the chain rule:\n","\n","$$ \\frac{dL}{db} = \\frac{dL}{dc} \\frac{dc}{db} = \\frac{dL}{dc} \\frac{d(log(b))}{db} = \\frac{dL}{dc} \\frac{1}{b}$$\n","\n","Since `log` operates elementwise, the gradient is just the same as doing this derivative elementwise. All we need to do is divide `grad_out` by the input argument `x`.\n","\n","</details>\n","\n","Note in this case you don't need to use the `out` argument - we'll see examples where it's useful later. Also don't worry about division by zero or other edge cases - the goal here is just to see how the pieces of the system fit together.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_log_back passed in 0.00s.\n"]}],"source":["def log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n","    \"\"\"Backwards function for f(x) = log(x)\n","\n","    grad_out: Gradient of some loss wrt out\n","    out: the output of np.log(x). Provided as an optimization in case it's cheaper to express the gradient in terms of the output.\n","    x: the input of np.log.\n","\n","    Return: gradient of the given loss wrt x\n","    \"\"\"\n","    return grad_out * (1 / x)\n","\n","\n","if MAIN:\n","    w1d3_test.test_log_back(log_back)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Backwards Functions of Two Tensors\n","\n","Now we'll implement backward functions for multiple tensors. To do so, we first need to understand broadcasting.\n","\n","### Broadcasting Rules\n","\n","Both NumPy and PyTorch have the same rules for broadcasting:\n","\n","- Dimensions of length 1 can be expanded to any length.\n","- Additional dimensions of any length can be prepended to the front of the shape.\n","\n","We will handle two situations where broadcasting can occur:\n","\n","- A binary operation between arrays of two different shapes: both are broadcasted to a common shape.\n","- An explicit call to an expansion function. We will be implementing the functionality of `torch.expand` using `np.broadcast_to` as the underlying function.\n","\n","In either case, backpropagation has to do the same thing: the grad with respect to an input element has to be the sum of each position where that input element was used in the output.\n","\n","Implement the `unbroadcast` helper function.\n","\n","<details>\n","\n","<summary>I'm confused about implementing unbroadcast!</summary>\n","\n","Unbroadcast has to do two things:\n","\n","- Sum and remove dimensions that were prepended to the front of the original shape.\n","- Sum dimensions that were originally 1 back to the size 1 (using keepdims=True).\n","\n","It's easiest to do the two tasks in that order, since after removing the additional dimensions, the shapes will have an equal number of dimensions.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_unbroadcast passed in 0.00s.\n"]}],"source":["def unbroadcast(broadcasted: Arr, original: Arr) -> Arr:\n","    \"\"\"Sum 'broadcasted' until it has the shape of 'original'.\n","\n","    broadcasted: An array that was formerly of the same shape of 'original' and was expanded by broadcasting rules.\n","    \"\"\"\n","    num_dims_to_sum = len(broadcasted.shape) - len(original.shape) \n","    short_broadcasted = np.sum(broadcasted, axis=tuple(range(num_dims_to_sum)))\n","\n","    singular_dims = tuple(np.where(np.array(original.shape) == 1)[0])\n","    short_broadcasted = np.sum(short_broadcasted, axis=singular_dims, keepdims=True)\n","    return short_broadcasted\n","    \n","    \n","if MAIN:\n","    w1d3_test.test_unbroadcast(unbroadcast)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Backward Function for Elementwise Multiply\n","\n","Functions that are differentiable with respect to more than one input tensor are straightforward given that we already know how to handle broadcasting.\n","\n","- We're going to have two backwards functions, one for each input argument.\n","- If the input arguments were broadcasted together to create a larger output, the incoming `grad_out` will be of the larger common broadcasted shape and we need to make use of `unbroadcast` from earlier to match the shape to the appropriate input argument.\n","- We'll want our backward function to work when one of the inputs is an float. We won't need to calculate the grad_in with respect to floats, so we only need to consider when y is an float for `multiply_back0` and when x is an float for `multiplyback1`.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_multiply_back passed in 0.00s.\n","w1d3_test.test_multiply_back_float passed in 0.00s.\n"]}],"source":["def multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Union[Arr, float]) -> Arr:\n","    \"\"\"Backwards function for x * y wrt argument 0 aka x.\"\"\"\n","    return unbroadcast(grad_out * np.array(y), x)\n","\n","def multiply_back1(grad_out: Arr, out: Arr, x: Union[Arr, float], y: Arr) -> Arr:\n","    \"\"\"Backwards function for x * y wrt argument 1 aka y.\"\"\"\n","    return unbroadcast(grad_out * np.array(x), y)\n","\n","\n","if MAIN:\n","    w1d3_test.test_multiply_back(multiply_back0, multiply_back1)\n","    w1d3_test.test_multiply_back_float(multiply_back0, multiply_back1)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","Now we'll use our backward functions to do backpropagation manually, for the following computational graph:\n","\n","\n","```mermaid\n","\n","graph LR\n","    a --> prod1[*] --> d --> prod2[*] --> f --> log1[Log] --> g\n","    b --> prod1\n","    c --> log2[Log] --> e --> prod2\n","```\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_forward_and_back passed in 0.00s.\n"]}],"source":["def forward_and_back(a: Arr, b: Arr, c: Arr) -> tuple[Arr, Arr, Arr]:\n","    \"\"\"\n","    Calculates the output of the computational graph above (g), then backpropogates the gradients and returns dg/da, dg/db, and dg/dc\n","    \"\"\"\n","    d = a * b\n","    e = np.log(c)\n","    f = d * e\n","    g = np.log(f)\n","    dg_df = log_back(1, g, f)\n","    \n","    dg_de = multiply_back1(dg_df, f, d, e)\n","    dg_dd = multiply_back0(dg_df, f, d, e)\n","\n","    dg_dc = log_back(dg_de, e, c)\n","    dg_db = multiply_back1(dg_dd, d, a, b)\n","    dg_da = multiply_back0(dg_dd, d, a, b)\n","    return dg_da, dg_db, dg_dc\n","\n","    \n","if MAIN:\n","    w1d3_test.test_forward_and_back(forward_and_back)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Autograd\n","\n","Now, rather than figuring out which backward functions to call, in what order, and what their inputs should be, we'll write code that takes care of that for us. We'll implement this with a few major components:\n","\n","- Tensor\n","- Recipe\n","- wrap_forward_fn\n","\n","### Wrapping Arrays (Tensor)\n","\n","We're going to wrap each array with a wrapper object from our library which we'll call `Tensor` because it's going to behave similarly to a `torch.Tensor`.\n","\n","Each Tensor that is created by one of our forward functions will have a `Recipe`, which tracks the extra information need to run backpropagation.\n","\n","`wrap_forward_fn` will take a forward function and return a new forward function that does the same thing while recording the info we need to do backprop in the `Recipe`.\n","\n","### Recipe\n","\n","Let's start by taking a look at `Recipe`.\n","\n","`@dataclass` is a handy class decorator that sets up an `__init__` function for the class that takes the provided attributes as arguments and sets them as you'd expect.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["@dataclass(frozen=True)\n","class Recipe:\n","    \"\"\"Extra information necessary to run backpropagation. You don't need to modify this.\"\"\"\n","\n","    func: Callable\n","    \"The 'inner' NumPy function that does the actual forward computation.\"\n","    args: tuple\n","    \"The input arguments passed to func.\"\n","    kwargs: dict[str, Any]\n","    \"Keyword arguments passed to func. To keep things simple today, we aren't going to backpropagate with respect to these.\"\n","    parents: dict[int, \"Tensor\"]\n","    \"Map from positional argument index to the Tensor at that position, in order to be able to pass gradients back along the computational graph.\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Registering backwards functions\n","\n","The `Recipe` takes care of tracking the forward functions in our computational graph, but we still need a way to find the backward function corresponding to a given forward function when we do backprop.\n","\n","The implementation today can be done very simply. We won't support backprop wrt keyword arguments and will raise an exception if the user tries to pass a Tensor by keyword. You can remove this limitation later if you have time.\n","\n","We do need to support functions with multiple positional arguments like multiplication so we'll also provide the positional argument index when setting and getting back_fns.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_back_func_lookup passed in 0.00s.\n"]}],"source":["class BackwardFuncLookup:\n","    def __init__(self) -> None:\n","        self.back_funcs = {}\n","\n","    def add_back_func(self, forward_fn: Callable, arg_position: int, back_fn: Callable) -> None:\n","        # for eg, for out = (pos 0) * (pos 1) function, we know we need a multiply_back0or1 function, pos arg tells us which arg we diff wrt\n","        self.back_funcs[(forward_fn, arg_position)] = back_fn\n","\n","    def get_back_func(self, forward_fn: Callable, arg_position: int) -> Callable:\n","        return self.back_funcs[(forward_fn, arg_position)]\n","\n","\n","if MAIN:\n","    w1d3_test.test_back_func_lookup(BackwardFuncLookup)\n","BACK_FUNCS = BackwardFuncLookup()\n","BACK_FUNCS.add_back_func(np.log, 0, log_back)\n","BACK_FUNCS.add_back_func(np.multiply, 0, multiply_back0)\n","BACK_FUNCS.add_back_func(np.multiply, 1, multiply_back1)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Tensors\n","\n","Our Tensor object has these fields:\n","\n","- An `array` field of type `np.ndarray`.\n","- A `requires_grad` field of type `bool`.\n","- A `grad` field of the same size and type as the value.\n","- A `recipe` field, as we've already seen.\n","\n","### requires_grad\n","The meaning of `requires_grad` is that when doing operations using this tensor, the recipe will be stored and it and any descendents will be included in the computational graph.\n","\n","Note that `requires_grad` does not mean that we will save the accumulated gradients to this tensor's `.grad` parameter when doing backprop: we will follow pytorch's implementation of backprop and only save gradients to leaf tensors (see `Tensor.is_leaf`, below).\n","\n","There is a lot of repetitive boilerplate involved which we have done for you. You don't need to modify anything in this class: the methods here will delegate to functions that you will implement throughout the day.\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class Tensor:\n","    \"\"\"\n","    A drop-in replacement for torch.Tensor supporting a subset of features.\n","    \"\"\"\n","\n","    array: Arr\n","    \"The underlying array. Can be shared between multiple Tensors.\"\n","    requires_grad: bool\n","    \"If True, calling functions or methods on this tensor will track relevant data for backprop.\"\n","    grad: Optional[\"Tensor\"]\n","    \"Backpropagation will accumulate gradients into this field.\"\n","    recipe: Optional[Recipe]\n","    \"Extra information necessary to run backpropagation.\"\n","\n","    def __init__(self, array: Union[Arr, list], requires_grad=False):\n","        self.array = array if isinstance(array, Arr) else np.array(array)\n","        self.requires_grad = requires_grad\n","        self.grad = None\n","        self.recipe = None\n","        \"If not None, this tensor's array was created via recipe.func(*recipe.args, **recipe.kwargs).\"\n","\n","    def __neg__(self) -> \"Tensor\":\n","        return negative(self)\n","\n","    def __add__(self, other) -> \"Tensor\":\n","        return add(self, other)\n","\n","    def __radd__(self, other) -> \"Tensor\":\n","        return add(other, self)\n","\n","    def __sub__(self, other) -> \"Tensor\":\n","        return subtract(self, other)\n","\n","    def __rsub__(self, other):\n","        return subtract(other, self)\n","\n","    def __mul__(self, other) -> \"Tensor\":\n","        return multiply(self, other)\n","\n","    def __rmul__(self, other):\n","        return multiply(other, self)\n","\n","    def __truediv__(self, other):\n","        return true_divide(self, other)\n","\n","    def __rtruediv__(self, other):\n","        return true_divide(self, other)\n","\n","    def __matmul__(self, other):\n","        return matmul(self, other)\n","\n","    def __rmatmul__(self, other):\n","        return matmul(other, self)\n","\n","    def __eq__(self, other):\n","        return eq(self, other)\n","\n","    def __repr__(self) -> str:\n","        return f\"Tensor({repr(self.array)}, requires_grad={self.requires_grad})\"\n","\n","    def __len__(self) -> int:\n","        if self.array.ndim == 0:\n","            raise TypeError\n","        return self.array.shape[0]\n","\n","    def __hash__(self) -> int:\n","        return id(self)\n","\n","    def __getitem__(self, index) -> \"Tensor\":\n","        return getitem(self, index)\n","\n","    def add_(self, other: \"Tensor\", alpha: float = 1.0) -> \"Tensor\":\n","        add_(self, other, alpha=alpha)\n","        return self\n","\n","    @property\n","    def T(self) -> \"Tensor\":\n","        return permute(self)\n","\n","    def item(self):\n","        return self.array.item()\n","\n","    def sum(self, dim=None, keepdim=False):\n","        return sum(self, dim=dim, keepdim=keepdim)\n","\n","    def log(self):\n","        return log(self)\n","\n","    def exp(self):\n","        return exp(self)\n","\n","    def reshape(self, new_shape):\n","        return reshape(self, new_shape)\n","\n","    def expand(self, new_shape):\n","        return expand(self, new_shape)\n","\n","    def permute(self, dims):\n","        return permute(self, dims)\n","\n","    def maximum(self, other):\n","        return maximum(self, other)\n","\n","    def relu(self):\n","        return relu(self)\n","\n","    def argmax(self, dim=None, keepdim=False):\n","        return argmax(self, dim=dim, keepdim=keepdim)\n","\n","    def uniform_(self, low: float, high: float) -> \"Tensor\":\n","        self.array[:] = np.random.uniform(low, high, self.array.shape)\n","        return self\n","\n","    def backward(self, end_grad: Union[Arr, \"Tensor\", None] = None) -> None:\n","        if isinstance(end_grad, Arr):\n","            end_grad = Tensor(end_grad)\n","        return backprop(self, end_grad)\n","\n","    def size(self, dim: Optional[int] = None):\n","        if dim is None:\n","            return self.shape\n","        return self.shape[dim]\n","\n","    @property\n","    def shape(self):\n","        return self.array.shape\n","\n","    @property\n","    def ndim(self):\n","        return self.array.ndim\n","\n","    @property\n","    def is_leaf(self):\n","        \"\"\"Same as https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html\"\"\"\n","        if self.requires_grad and self.recipe and self.recipe.parents:\n","            return False\n","        return True\n","\n","    def __bool__(self):\n","        if np.array(self.shape).prod() != 1:\n","            raise RuntimeError(\"bool value of Tensor with more than one value is ambiguous\")\n","        return bool(self.item())\n","\n","\n","def empty(*shape: int) -> Tensor:\n","    \"\"\"Like torch.empty.\"\"\"\n","    return Tensor(np.empty(shape))\n","\n","\n","def zeros(*shape: int) -> Tensor:\n","    \"\"\"Like torch.zeros.\"\"\"\n","    return Tensor(np.zeros(shape))\n","\n","\n","def arange(start: int, end: int, step=1) -> Tensor:\n","    \"\"\"Like torch.arange(start, end).\"\"\"\n","    return Tensor(np.arange(start, end, step=step))\n","\n","\n","def tensor(array: Arr, requires_grad=False) -> Tensor:\n","    \"\"\"Like torch.tensor.\"\"\"\n","    return Tensor(array, requires_grad=requires_grad)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Forward Pass: Building the Computational Graph\n","\n","Let's start with a simple case: our `log` function. Our `log` function must do the following:\n","\n","- Call `np.log(tensor.array)` to obtain an output array.\n","- Create a new `Tensor` containing the output.\n","- If grad tracking is enabled globally AND (the input requires grad, OR has a recipe), then the output requires grad and we fill out the recipe of our output. Note that since log has a single argument, `recipe.parents` will be a dict with a single integer key `0`.\n","\n","Later we'll redo this in a generic and reusable way, but for now just get it working.\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_log passed in 0.00s.\n","w1d3_test.test_log_no_grad passed in 0.00s.\n"]}],"source":["def log_forward(x: Tensor) -> Tensor:\n","    \"\"\"The forward pass for log.\"\"\"\n","    \n","    log_x = Tensor(np.log(x.array))\n","\n","    if grad_tracking_enabled and (x.requires_grad or x.recipe) != None: \n","        log_x.requires_grad = True\n","        # the below line is instructional about the recipes+tensor classes\n","        log_x.recipe = Recipe(np.log, args = (x.array,), kwargs = {}, parents = {0: x})\n","    return log_x\n","\n","        \n","        \n","\n","\n","if MAIN:\n","    log = log_forward\n","    w1d3_test.test_log(Tensor, log_forward)\n","    w1d3_test.test_log_no_grad(Tensor, log_forward)\n","    a = Tensor([1], requires_grad=True)\n","    grad_tracking_enabled = False\n","    b = log_forward(a)\n","    grad_tracking_enabled = True\n","    assert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\n","    assert b.recipe is None, \"should not create recipe if grad tracking globally disabled\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","Now let's do the same for multiply, to see how to handle functions with multiple arguments. There are a few differences:\n","\n","- The actual function to be called\n","- The multiple positional input arguments that need to be recorded in the recipe as arguments and the Tensor args as parents\n","- The possibility that one of the inputs may be an int\n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_multiply passed in 0.00s.\n","w1d3_test.test_multiply_no_grad passed in 0.00s.\n","w1d3_test.test_multiply_float passed in 0.00s.\n"]}],"source":["def multiply_forward(a: Union[Tensor, int], b: Union[Tensor, int]) -> Tensor:\n","    assert isinstance(a, Tensor) or isinstance(b, Tensor)\n","    # it's not obvious, but do not inclue int's in parents since they are not tensors\n","    # args should be either np.ndarry or int, depending on the type of the input (they can't both be np.ndarray)\n","\n","    a_is_tensor = isinstance(a, Tensor)\n","    b_is_tensor = isinstance(b, Tensor)\n","\n","    if isinstance(b, int):\n","        b_arg = b\n","        a_array = a.array\n","        b = Tensor([b])\n","        b_array = b.array\n","        a_arg = a_array\n","    elif isinstance(a, int):\n","        a_arg = a\n","        b_array = b.array\n","        a = Tensor([a])\n","        a_array = a.array\n","        b_arg = b_array\n","    else:\n","        a_array = a.array\n","        b_array = b.array\n","        a_arg = a_array\n","        b_arg = b_array\n","\n","    out = Tensor(np.multiply(a_array, b_array))\n","\n","    if grad_tracking_enabled and (a.requires_grad or b.requires_grad or a.recipe or b.recipe) != None:\n","        out.requires_grad = True\n","        parents = {}\n","        if a_is_tensor:\n","            parents[0] = a\n","        if b_is_tensor:\n","            parents[1] = b\n","        out.recipe = Recipe(np.multiply, args = (a_arg, b_arg), kwargs = {}, parents = parents)\n","\n","    return out\n","        \n","if MAIN:\n","    multiply = multiply_forward\n","    w1d3_test.test_multiply(Tensor, multiply_forward)\n","    w1d3_test.test_multiply_no_grad(Tensor, multiply_forward)\n","    w1d3_test.test_multiply_float(Tensor, multiply_forward)\n","    a = Tensor([2], requires_grad=True)\n","    b = Tensor([3], requires_grad=True)\n","    grad_tracking_enabled = False\n","    b = multiply_forward(a, b)\n","    grad_tracking_enabled = True\n","    assert not b.requires_grad, \"should not require grad if grad tracking globally disabled\"\n","    assert b.recipe is None, \"should not create recipe if grad tracking globally disabled\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Forward Pass - Generic Version\n","\n","All our forward functions are going to look extremely similar to `log_forward` and `multiply_forward`.\n","\n","Implement the higher order function `wrap_forward_fn` that takes a `Arr -> Arr` function and returns a `Tensor -> Tensor` function. In other words, `wrap_forward_fn(np.log)` should evaluate to a callable that does the same thing as your `log_forward`.\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_log passed in 0.00s.\n","w1d3_test.test_log_no_grad passed in 0.00s.\n","w1d3_test.test_multiply passed in 0.00s.\n","w1d3_test.test_multiply_no_grad passed in 0.00s.\n","w1d3_test.test_multiply_float passed in 0.00s.\n","Got a nice exception as intended:\n","log() takes from 1 to 2 positional arguments but 0 were given\n"]}],"source":["def wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable:\n","    \"\"\"\n","    numpy_func: function. It takes any number of positional arguments, some of which may be NumPy arrays, \n","    and any number of keyword arguments which we aren't allowing to be NumPy arrays at present. It returns a single NumPy array.\n","\n","    is_differentiable: if True, numpy_func is differentiable with respect to some input argument, so we may need to track \n","    information in a Recipe. If False, we definitely don't need to track information.\n","\n","    Return: function. It has the same signature as numpy_func, except wherever there was a NumPy array, this has a Tensor instead.\n","    \"\"\"\n","\n","    def tensor_func(*args: Any, **kwargs: Any) -> Tensor:\n","        arg_arrays = []\n","        arg_args = []\n","        for arg in args:\n","            if isinstance(arg, Tensor):\n","                arg_arrays.append(arg.array)\n","                arg_args.append(arg.array)\n","            else:\n","                arg_arrays.append([arg])\n","                arg_args.append(arg)\n","        # arg_arrays to tuple\n","        arg_arrays = tuple(arg_arrays)\n","\n","        arr = numpy_func(*arg_arrays, **kwargs)\n","        # make a dict of the args that are tensors\n","        out = Tensor(arr)\n","        # user wants grads\n","        condition_1 = grad_tracking_enabled\n","        # one can actually differentiate the np function\n","        condition_2 = is_differentiable\n","        # Any grad is actually needed for the args, as long as they are tensors\n","        condition_3 = any([arg.requires_grad for arg in args if isinstance(arg, Tensor)])\n","\n","        if condition_1 and condition_2 and condition_3:\n","            parents = {i: arg for i, arg in enumerate(args) if isinstance(arg, Tensor)}\n","            out.requires_grad = True\n","            out.recipe = Recipe(numpy_func, args = arg_args, kwargs = kwargs, parents = parents)\n","        else:\n","            out = Tensor(arr)\n","        return out\n","\n","    return tensor_func\n","\n","log = wrap_forward_fn(np.log)\n","multiply = wrap_forward_fn(np.multiply)\n","if MAIN:\n","    w1d3_test.test_log(Tensor, log)\n","    w1d3_test.test_log_no_grad(Tensor, log)\n","    w1d3_test.test_multiply(Tensor, multiply)\n","    w1d3_test.test_multiply_no_grad(Tensor, multiply)\n","    w1d3_test.test_multiply_float(Tensor, multiply)\n","    try:\n","        log(x=Tensor([100]))\n","    except Exception as e:\n","        print(\"Got a nice exception as intended:\")\n","        print(e)\n","    else:\n","        assert False, \"Passing tensor by keyword should raise some informative exception.\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Backpropagation\n","\n","Now all the pieces are in place to implement backpropagation. We need to:\n","\n","- Loop over the nodes from right to left. At each node:\n","    - Call the backward function to transform the grad wrt output to the grad wrt input.\n","    - If the node is a leaf, write the grad to the grad field.\n","    - Otherwise, accumulate the grad into temporary storage.\n","\n","### Topological Sort\n","\n","As part of backprop, we need to sort the nodes of our graph so we can traverse the graph in the appropriate order.\n","\n","Write a general function `topological_sort` that return a list of node's descendants in topological order (beginning with the furthest descentents, ending with the starting node) using [depth-first search](https://en.wikipedia.org/wiki/Topological_sorting).\n","\n","First we'll define a couple protocol classes so that we can write `topological_sort` so that it generalize in a predictable way. We should write it in such a way that it can work with any pair of compatible Node/ChildrenGetters that follow the given protocol. You do not need to modify the Protocol classes.\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_topological_sort_linked_list passed in 0.00s.\n","w1d3_test.test_topological_sort_branching passed in 0.00s.\n","w1d3_test.test_topological_sort_rejoining passed in 0.00s.\n","w1d3_test.test_topological_sort_cyclic passed in 0.00s.\n"]}],"source":["class Node(Protocol):\n","    \"\"\"\n","    A protocol defining the Node's interface in topological sort. Any object will do!\n","    \"\"\"\n","\n","\n","class ChildrenGetter(Protocol):\n","    \"\"\"A protocol defining the get_children_fns passed to topological sort, to get the node's children\"\"\"\n","\n","    def __call__(self, node: Any) -> list[Any]:\n","        \"\"\"Get the given node's children, returning a list of nodes\"\"\"\n","        \n","\n","\n","def topological_sort(node: Node, get_children_fn: ChildrenGetter) -> list[Any]:\n","    \"\"\"\n","    Return a list of node's descendants in reverse topological order from future to past.\n","    \"\"\"\n","    visited = set()\n","    temp_mark = set()\n","    descendants = []\n","\n","    def depth_first_search(node):\n","        if node in temp_mark:\n","            raise ValueError(\"Cycle detected\")\n","        if node in visited:\n","            return\n","        temp_mark.add(node)\n","        for child in get_children_fn(node):\n","            depth_first_search(child)\n","        visited.add(node)    \n","        temp_mark.remove(node)\n","        descendants.append(node)\n","        return descendants\n","\n","    descendants = depth_first_search(node)\n","    return descendants        \n","    \n","    \n","\n","if MAIN:\n","    w1d3_test.test_topological_sort_linked_list(topological_sort)\n","    w1d3_test.test_topological_sort_branching(topological_sort)\n","    w1d3_test.test_topological_sort_rejoining(topological_sort)\n","    w1d3_test.test_topological_sort_cyclic(topological_sort)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","Now using your `topological_sort` write `sorted_computation_graph`, which returns the nodes in your computational graph in the order you need for backprop.\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['g', 'f', 'e', 'c', 'd', 'b', 'a']\n"]}],"source":["def sorted_computational_graph(node: Tensor) -> list[Tensor]:\n","    \"\"\"\n","    For a given tensor, return a list of Tensors that make up the nodes of the given Tensor's computational graph, in reverse topological order\n","    \"\"\"\n","    def get_children_fn(node):\n","        if node.recipe is None:\n","            return []\n","        else:\n","            return list(node.recipe.parents.values())\n","    return list(reversed(topological_sort(node, get_children_fn)))\n","\n","if MAIN:\n","    a = Tensor([1], requires_grad=True)\n","    b = Tensor([2], requires_grad=True)\n","    c = Tensor([3], requires_grad=True)\n","    d = a * b\n","    e = c.log()\n","    f = d * e\n","    g = f.log()\n","    name_lookup = {a: \"a\", b: \"b\", c: \"c\", d: \"d\", e: \"e\", f: \"f\", g: \"g\"}\n","    print([name_lookup[t] for t in sorted_computational_graph(g)])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### The Actual Backprop Function\n","\n","Now we're really ready for backprop!\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_backprop passed in 0.00s.\n","w1d3_test.test_backprop_branching passed in 0.00s.\n","w1d3_test.test_backprop_float_arg passed in 0.00s.\n","w1d3_test.test_backprop_requires_grad_false passed in 0.00s.\n"]}],"source":["def backprop(end_node: Tensor, end_grad: Optional[Tensor] = None) -> None:\n","    \"\"\"Accumulate gradients in the grad field of each leaf node.\n","\n","    tensor.backward() is equivalent to backprop(tensor).\n","\n","    end_node: the rightmost node in the computation graph. If it contains more than one element, end_grad must be provided.\n","    end_grad: A tensor of the same shape as end_node. Set to 1 if not specified and end_node has only one element.\n","    \"\"\"\n","    grads = {end_node: end_grad}\n","    graph = sorted_computational_graph(end_node)\n","    \n","    for node in graph:\n","\n","        if not node.recipe:\n","            continue\n","\n","        args, func = node.recipe.args, node.recipe.func\n","        \n","        for arg_position, parent in node.recipe.parents.items():\n","\n","            if not parent.requires_grad:\n","                continue\n","\n","            back = BACK_FUNCS.get_back_func(func, arg_position)\n","            grad = Tensor(back(grads[node].array, node.array, *args))\n","\n","            grads[parent] = grad\n","            \n","            # if parent.recipe does not exist, continue on to update parent.grad?? \n","            if parent.recipe:\n","                continue\n","\n","            # parent.grad += DOES NOT work? \n","            if parent.grad is not None:\n","                parent.grad += grad\n","            else:\n","                parent.grad = grad\n","        \n","\n","if MAIN:\n","    w1d3_test.test_backprop(Tensor)\n","    w1d3_test.test_backprop_branching(Tensor)\n","    w1d3_test.test_backprop_float_arg(Tensor)\n","    w1d3_test.test_backprop_requires_grad_false(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Filling out the Tensor class with forward and backward methods\n","\n","Congrats on implementing backprop! The next thing we'll do is write implement a bunch of backward functions that we need to train our model at the end of the day, as well as ones that cover interesting cases.\n","\n","## Non-Differentiable Functions\n","\n","For functions like `torch.argmax` or `torch.eq`, there's no sensible way to define gradients with respect to the input tensor. For these, we will still use `wrap_forward_fn` because we still need to unbox the arguments and box the result, but by passing `is_differentiable=False` we can avoid doing any unnecessary computation.\n","\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def _argmax(x: Arr, dim=None, keepdim=False):\n","    \"\"\"Like torch.argmax.\"\"\"\n","    return np.argmax(x, axis=dim, keepdims=keepdim)\n","\n","\n","argmax = wrap_forward_fn(_argmax, is_differentiable=False)\n","eq = wrap_forward_fn(np.equal, is_differentiable=False)\n","if MAIN:\n","    a = Tensor([1.0, 0.0, 3.0, 4.0], requires_grad=True)\n","    b = a.argmax()\n","    assert not b.requires_grad\n","    assert b.recipe is None\n","    assert b.item() == 3\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Implementing `negative`\n","\n","`torch.negative` just performs `-x` elementwise. Make your own version `negative` using `wrap_forward_fn`.\n","\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_negative_back passed in 0.00s.\n"]}],"source":["def negative_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n","    \"\"\"Backward function for f(x) = -x elementwise.\"\"\"\n","    return -grad_out\n","\n","\n","negative = wrap_forward_fn(np.negative)\n","BACK_FUNCS.add_back_func(np.negative, 0, negative_back)\n","if MAIN:\n","    w1d3_test.test_negative_back(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Implementing `exp`\n","\n","Make your own version of `torch.exp`. The backward function should express the result in terms of the `out` parameter - this more efficient than expressing it in terms of `x`.\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w1d3_test.test_exp_back passed in 0.00s.\n"]}],"source":["def exp_back(grad_out: Arr, out: Arr, x: Arr) -> Arr:\n","    return grad_out * out\n","\n","\n","exp = wrap_forward_fn(np.exp)\n","BACK_FUNCS.add_back_func(np.exp, 0, exp_back)\n","if MAIN:\n","    w1d3_test.test_exp_back(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Reshape\n","\n","`reshape` is a bit more complicated than the many functions we've dealt with so far: there is an additional positional argument `new_shape`. Since it's not a `Tensor`, we don't need to think about differentiating with respect to it.\n","\n","Depending how you wrote `wrap_forward_fn` and `backprop`, you might need to go back and adjust them to handle this. Or, you might just have to implement `reshape_back` and everything will work.\n","\n","Note that the output is a different shape than the input, but this doesn't introduce any additional complications.\n","\n","<details>\n","\n","<summary>I'm confused about the implementation of reshape_back!</summary>\n","\n","The invariant is that the `grad_out` should always be the same shape as `x`. Reshape is elementwise (none of the individual scalars interact with each other), so we just need to reshape `grad` into the same shape as `x` for everything to \"line up\".\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def reshape_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> None:\n","    return \n","\n","reshape = wrap_forward_fn(np.reshape)\n","BACK_FUNCS.add_back_func(np.reshape, 0, reshape_back)\n","if MAIN:\n","    #w1d3_test.test_reshape_back(Tensor)\n","    None\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Permute\n","\n","In NumPy, the equivalent of `torch.permute` is called `np.transpose`, so we will wrap that.\n","\n","<details>\n","\n","<summary>I'm confused about the implementation of permute_back!</summary>\n","\n","The gradients are in permuted positions, and we want to line them up with the original positions. This means we need to compute a permutation that is the inverse of the forward permutation.\n","\n","</details>\n","\n","<details>\n","\n","<summary>I'm still confused about permute_back!</summary>\n","\n","Compute the inverse permutation with `np.argsort(axes)` and then apply it with another `np.transpose`.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'tuple' object cannot be interpreted as an integer","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m~/Documents/ml-camp-0-participants/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m permute \u001b[39m=\u001b[39m wrap_forward_fn(np\u001b[39m.\u001b[39mtranspose)\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m MAIN:\n\u001b[0;32m----> 8\u001b[0m     w1d3_test\u001b[39m.\u001b[39;49mtest_permute_back(Tensor)\n","File \u001b[0;32m~/Documents/ml-camp-0-participants/utils.py:148\u001b[0m, in \u001b[0;36mreport.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@wraps\u001b[39m(test_func)\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m run_and_report(test_func, name, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Documents/ml-camp-0-participants/utils.py:155\u001b[0m, in \u001b[0;36mrun_and_report\u001b[0;34m(test_func, name, *test_func_args, **test_func_kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_and_report\u001b[39m(test_func: Callable, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39mtest_func_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest_func_kwargs):\n\u001b[1;32m    154\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 155\u001b[0m     out \u001b[39m=\u001b[39m test_func(\u001b[39m*\u001b[39;49mtest_func_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtest_func_kwargs)\n\u001b[1;32m    156\u001b[0m     elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n\u001b[1;32m    157\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m passed in \u001b[39m\u001b[39m{\u001b[39;00melapsed\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms.\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Documents/ml-camp-0-participants/w1d3_test.py:322\u001b[0m, in \u001b[0;36mtest_permute_back\u001b[0;34m(Tensor)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39m@report\u001b[39m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_permute_back\u001b[39m(Tensor):\n\u001b[1;32m    321\u001b[0m     a \u001b[39m=\u001b[39m Tensor(np\u001b[39m.\u001b[39marange(\u001b[39m24\u001b[39m)\u001b[39m.\u001b[39mreshape((\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m)), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 322\u001b[0m     out \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mpermute((\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    323\u001b[0m     out\u001b[39m.\u001b[39mbackward(np\u001b[39m.\u001b[39marange(\u001b[39m24\u001b[39m)\u001b[39m.\u001b[39mreshape((\u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)))\n\u001b[1;32m    325\u001b[0m     \u001b[39massert\u001b[39;00m a\u001b[39m.\u001b[39mgrad \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[9], line 99\u001b[0m, in \u001b[0;36mTensor.permute\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpermute\u001b[39m(\u001b[39mself\u001b[39m, dims):\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m permute(\u001b[39mself\u001b[39;49m, dims)\n","Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mwrap_forward_fn.<locals>.tensor_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# arg_arrays to tuple\u001b[39;00m\n\u001b[1;32m     23\u001b[0m arg_arrays \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(arg_arrays)\n\u001b[0;32m---> 25\u001b[0m arr \u001b[39m=\u001b[39m numpy_func(\u001b[39m*\u001b[39;49marg_arrays, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m \u001b[39m# make a dict of the args that are tensors\u001b[39;00m\n\u001b[1;32m     27\u001b[0m out \u001b[39m=\u001b[39m Tensor(arr)\n","File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m~/Documents/ml-camp-0-participants/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:668\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[1;32m    602\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose\u001b[39m(a, axes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[39m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m \n\u001b[1;32m    667\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mtranspose\u001b[39;49m\u001b[39m'\u001b[39;49m, axes)\n","File \u001b[0;32m~/Documents/ml-camp-0-participants/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n","File \u001b[0;32m~/Documents/ml-camp-0-participants/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n","\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"]}],"source":["def permute_back(grad_out: Arr, out: Arr, x: Arr, axes: tuple) -> None:\n","    return \n","\n","\n","BACK_FUNCS.add_back_func(np.transpose, 0, permute_back)\n","permute = wrap_forward_fn(np.transpose)\n","if MAIN:\n","    w1d3_test.test_permute_back(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Expand\n","\n","Implement your version of `torch.expand`. The backward function should just call `unbroadcast`.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def expand_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr:\n","    pass\n","\n","\n","def _expand(x: Arr, new_shape) -> Arr:\n","    \"\"\"Like torch.expand, calling np.broadcast_to internally.\n","\n","    Note torch.expand supports -1 for a dimension size meaning \"don't change the size\".\n","    np.broadcast_to does not natively support this.\n","    \"\"\"\n","    pass\n","\n","\n","expand = wrap_forward_fn(_expand)\n","BACK_FUNCS.add_back_func(_expand, 0, expand_back)\n","if MAIN:\n","    w1d3_test.test_expand(Tensor)\n","    w1d3_test.test_expand_negative_length(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Backwards Pass - Sum\n","\n","The output can also be smaller than the input, such as when calling `torch.sum`. Implement your own `torch.sum` and `sum_back`.\n","\n","<details>\n","\n","<summary>I'm confused about the implementation of sum_back!</summary>\n","\n","For a given element of `grad_out`, increasing any of the contributing inputs by `dx` also increases the sum by `dx` and increasing any other input doesn't affect the sum.\n","\n","So there's no multiplication required here, just reshaping and broadcasting `grad_out` so that each element of `grad_out` is matched up with every contributing input element.\n","\n","</details>\n","\n","<details>\n","\n","<summary>I'm still confused about the implementation of sum_back!</summary>\n","\n","Consider the keepdim=True case first. The shape of grad_out is the same as the shape of x, except it's 1 in each dim that was summed out. That 1-length dim just needs to be copied, which is just a broadcasting operation.\n","\n","Now consider the keepdim=False case. We can reshape grad_out to the shape it would've been if keepdim=True, and then we've simplified our problem to the first case. Note that our reshape has to be a literal `reshape` call and not a broadcast, because broadcast can only prepend new axes and our new axes can be anywhere in the shape.\n","</details>\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sum_back(grad_out: Arr, out: Arr, x: Arr, dim=None, keepdim=False):\n","    pass\n","\n","\n","def _sum(x: Arr, dim=None, keepdim=False) -> Arr:\n","    \"\"\"Like torch.sum, calling np.sum internally.\"\"\"\n","    pass\n","\n","\n","sum = wrap_forward_fn(_sum)\n","BACK_FUNCS.add_back_func(_sum, 0, sum_back)\n","if MAIN:\n","    w1d3_test.test_sum_keepdim_false(Tensor)\n","    w1d3_test.test_sum_keepdim_true(Tensor)\n","    w1d3_test.test_sum_dim_none(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Backwards Pass - Indexing\n","\n","In its full generality, indexing a `torch.Tensor` is really complicated and there are quite a few cases to handle separately.\n","\n","We only need two cases today:\n","\n","- The index is an integer or tuple of integers.\n","- The index is a tuple of (array or Tensor) representing coordinates. Each array is 1D and of equal length. Some coordinates may be repeated. This is [Integer array indexing](https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing).\n","    - For example, to select the five elements at (0, 0), (1,0), (0, 1), (1, 2), and (0, 0), the index would be the tuple `(np.array([0, 1, 0, 1, 0]), np.array([0, 0, 1, 2, 0]))`.\n","\n","<details>\n","\n","<summary>I'm confused about the implementation of getitem_back!</summary>\n","\n","If no coordinates were repeated, we could just assign the grad for each input element to be the grad at the corresponding output position, or 0 if that input element didn't appear.\n","\n","Because of the potential for repeat coordinates, we need to sum the grad from each corresponding output position.\n","\n","Initialize an array of zeros of the same shape as x, and then write in the appropriate elements using [np.add.at](https://numpy.org/doc/stable/reference/generated/numpy.ufunc.at.html)\n","\n","</details>\n","\n","<details>\n","\n","<summary>I'm confused about an IndexError I'm getting in forward!</summary>\n","\n","If you're seeing the message \"IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\", then possibly you're trying to index a NumPy array using a tuple of Tensors, which NumPy doesn't know how to do.\n","\n","Normally, the Tensor would have been unboxed already by the time `_getitem` is called, but if you did a check like `isinstance(arg, Tensor)` then a tuple[Tensor] doesn't meet this criteria.\n","\n","The simplest fix is just to unbox it manually within `_getitem` as this is a special case scenario anyway, but you could also modify your forward wrapper to handle this.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'getitem_back' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[250], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m x[index]\n\u001b[1;32m      8\u001b[0m getitem \u001b[39m=\u001b[39m wrap_forward_fn(_getitem)\n\u001b[0;32m----> 9\u001b[0m BACK_FUNCS\u001b[39m.\u001b[39madd_back_func(_getitem, \u001b[39m0\u001b[39m, getitem_back)\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m MAIN:\n\u001b[1;32m     11\u001b[0m     w1d3_test\u001b[39m.\u001b[39mtest_getitem_int(Tensor)\n","\u001b[0;31mNameError\u001b[0m: name 'getitem_back' is not defined"]}],"source":["Index = Union[int, tuple[int, ...], tuple[Arr], tuple[Tensor]]\n","\n","\n","def _getitem(x: Arr, index: Index) -> Arr:\n","    \"\"\"Like x[index] when x is a torch.Tensor.\"\"\"\n","    pass\n","\n","\n","def getitem_back(grad_out: Arr, out: Arr, x: Arr, index: Index):\n","    \"\"\"Backwards function for _getitem.\n","\n","    Hint: use np.add.at(a, indices, b)\n","    \"\"\"\n","    pass\n","\n","\n","getitem = wrap_forward_fn(_getitem)\n","BACK_FUNCS.add_back_func(_getitem, 0, getitem_back)\n","if MAIN:\n","    w1d3_test.test_getitem_int(Tensor)\n","    w1d3_test.test_getitem_tuple(Tensor)\n","    w1d3_test.test_getitem_integer_array(Tensor)\n","    w1d3_test.test_getitem_integer_tensor(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Elementwise addition, subtract, true_divide\n","\n","These are exactly analogous to the multiply case. Note that Python and NumPy have the notion of \"floor division\", which is a truncating integer division as in `7 // 3 = 2`. You can ignore floor division: - we only need the usual floating point division which is called \"true division\".\n","\n","Use lambda functions to define and register the backward functions each in one line.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["add = wrap_forward_fn(np.add)\n","subtract = wrap_forward_fn(np.subtract)\n","true_divide = wrap_forward_fn(np.true_divide)\n","\"TODO: YOUR CODE HERE\"\n","if MAIN:\n","    w1d3_test.test_add_broadcasted(Tensor)\n","    w1d3_test.test_subtract_broadcasted(Tensor)\n","    w1d3_test.test_truedivide_broadcasted(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## In-Place Operations\n","\n","Supporting in-place operations introduces substantial complexity and generally doesn't help performance that much. The problem is that if any of the inputs used in the backward function have been modified in-place since the forward pass, then the backward function will incorrectly calculate using the modified version.\n","\n","PyTorch will warn you when this causes a problem with the error \"RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\".\n","\n","You can implement the warning in the bonus section but for now your system will silently compute the wrong gradients - user beware!\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_(x: Tensor, other: Tensor, alpha: float = 1.0) -> Tensor:\n","    \"\"\"Like torch.add_. Compute x += other * alpha in-place and return tensor.\"\"\"\n","    np.add(x.array, other.array * alpha, out=x.array)\n","    return x\n","\n","\n","def safe_example():\n","    \"\"\"This example should work properly.\"\"\"\n","    a = Tensor([0.0, 1.0, 2.0, 3.0], requires_grad=True)\n","    b = Tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True)\n","    a.add_(b)\n","    c = a * b\n","    c.sum().backward()\n","    assert a.grad is not None and np.allclose(a.grad.array, [2.0, 3.0, 4.0, 5.0])\n","    assert b.grad is not None and np.allclose(b.grad.array, [2.0, 4.0, 6.0, 8.0])\n","\n","\n","def unsafe_example():\n","    \"\"\"This example is expected to compute the wrong gradients.\"\"\"\n","    a = Tensor([0.0, 1.0, 2.0, 3.0], requires_grad=True)\n","    b = Tensor([2.0, 3.0, 4.0, 5.0], requires_grad=True)\n","    c = a * b\n","    a.add_(b)\n","    c.sum().backward()\n","    if a.grad is not None and np.allclose(a.grad.array, [2.0, 3.0, 4.0, 5.0]):\n","        print(\"Grad wrt a is OK!\")\n","    else:\n","        print(\"Grad wrt a is WRONG!\")\n","    if b.grad is not None and np.allclose(b.grad.array, [0.0, 1.0, 2.0, 3.0]):\n","        print(\"Grad wrt b is OK!\")\n","    else:\n","        print(\"Grad wrt b is WRONG!\")\n","\n","\n","if MAIN:\n","    safe_example()\n","    unsafe_example()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Mixed scalar-tensor operations\n","\n","You may have been wondering why our `Tensor` class has to define both `__mul__` and `__rmul__` magic methods.\n","\n","Without `__rmul__` defined, executing `2 * a` when `a` is a `Tensor` would try to call `2.__mul__(a)`, and the built-in class `int` would be confused about how to handle this.\n","\n","Since we have defined `__rmul__` for you at the start, and you implemented multiply to work with floats as arguments, the following should \"just work\".\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MAIN:\n","    a = Tensor([0, 1, 2, 3], requires_grad=True)\n","    (a * 2).sum().backward()\n","    b = Tensor([0, 1, 2, 3], requires_grad=True)\n","    (2 * b).sum().backward()\n","    assert a.grad is not None\n","    assert b.grad is not None\n","    assert np.allclose(a.grad.array, b.grad.array)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Splitting Gradients - elementwise maximum\n","\n","Since this is an elementwise function, we can think about the scalar case. For scalar $x$, $y$, the derivative for $max(x, y)$ wrt x is 1 when $x > y$ and 0 when $x < y$. What should happen when $x = y$?\n","\n","<details>\n","\n","<summary>Solution - derivative of maximum</summary>\n","\n","$max(x, x)$ is equivalent to the identity function, which has a derivative of 1 wrt x. The sum of our partial derivatives wrt x and y must also therefore total 1.\n","\n","PyTorch splits the derivative evenly between the two arguments. We will follow this behavior for compatibility, but it's just as legitimate to say it's 1 wrt x and 0 wrt y, or some other arbitrary combination that sums to one.\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def maximum_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr):\n","    \"\"\"Backwards function for max(x, y) wrt x.\"\"\"\n","    pass\n","\n","\n","def maximum_back1(grad_out: Arr, out: Arr, x: Arr, y: Arr):\n","    \"\"\"Backwards function for max(x, y) wrt y.\"\"\"\n","    pass\n","\n","\n","maximum = wrap_forward_fn(np.maximum)\n","BACK_FUNCS.add_back_func(np.maximum, 0, maximum_back0)\n","BACK_FUNCS.add_back_func(np.maximum, 1, maximum_back1)\n","if MAIN:\n","    w1d3_test.test_maximum(Tensor)\n","    w1d3_test.test_maximum_broadcasted(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Functional ReLU\n","\n","A simple and correct ReLU function can be defined in terms of your maximum function. Note the PyTorch version also supports in-place operation, which we are punting on for now.\n","\n","Again, at $x = 0$ your derivative could reasonably be anything between 0 and 1 inclusive, but we've followed PyTorch in making it 0.5.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def relu(x: Tensor) -> Tensor:\n","    \"\"\"Like torch.nn.function.relu(x, inplace=False).\"\"\"\n","    pass\n","\n","\n","if MAIN:\n","    w1d3_test.test_relu(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## 2D Matrix Multiply\n","\n","Implement your version of `torch.matmul`, restricting it to the simpler case where both inputs are 2D.\n","\n","<details>\n","\n","<summary>I'm confused about matmul2d_back!</summary>\n","\n","Try working it out on paper, starting from a couple 2x2 matrices. Can you express the answer in terms of another matrix multiply and a transpose?\n","\n","</details>\n","\n","<details>\n","\n","<summary>I'm still confused about matmul2d_back!</summary>\n","\n","With respect to x, it is `grad_out @ transpose(y)`.\n","With respect to y, it is `transpose(x) @ grad_out`.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def _matmul2d(x: Arr, y: Arr) -> Arr:\n","    \"\"\"Matrix multiply restricted to the case where both inputs are exactly 2D.\"\"\"\n","    pass\n","\n","\n","def matmul2d_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr:\n","    pass\n","\n","\n","def matmul2d_back1(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr:\n","    pass\n","\n","\n","matmul = wrap_forward_fn(_matmul2d)\n","BACK_FUNCS.add_back_func(_matmul2d, 0, matmul2d_back0)\n","BACK_FUNCS.add_back_func(_matmul2d, 1, matmul2d_back1)\n","if MAIN:\n","    w1d3_test.test_matmul2d(Tensor)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Build Your Own `nn.Parameter`\n","\n","We've now written enough backwards passes that we can go up a layer and write our own `nn.Parameter` and `nn.Module`.\n","\n","We don't need much for `Parameter`. It is itself a `Tensor`, shares storage with the provided `Tensor` and requires_grad is `True` by default - that's it!\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Parameter(Tensor):\n","    def __init__(self, tensor: Tensor, requires_grad=True):\n","        \"\"\"Share the array with the provided tensor.\"\"\"\n","        pass\n","\n","    def __repr__(self):\n","        return f\"Parameter containing:\\n{super().__repr__()}\"\n","\n","\n","if MAIN:\n","    x = Tensor([1.0, 2.0, 3.0])\n","    p = Parameter(x)\n","    assert p.requires_grad\n","    assert p.array is x.array\n","    assert repr(p) == \"Parameter containing:\\nTensor(array([1., 2., 3.]), requires_grad=True)\"\n","    x.add_(Tensor(np.array(2.0)))\n","    assert np.allclose(\n","        p.array, np.array([3.0, 4.0, 5.0])\n","    ), \"in-place modifications to the original tensor should affect the parameter\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Build Your Own `nn.Module`\n","\n","`nn.Module` is like `torch.Tensor` in that it has a lot of functionality, most of which we don't care about today. We will just implement enough to get our network training.\n","\n","Implement the indicated methods.\n","\n","Tip: you can bypass `__getattr__` by accessing `self.__dict__` inside a method.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Module:\n","    _modules: dict[str, \"Module\"]\n","    _parameters: dict[str, Parameter]\n","\n","    def __init__(self):\n","        pass\n","\n","    def modules(self):\n","        \"\"\"Return the direct child modules of this module.\"\"\"\n","        return self.__dict__[\"_modules\"].values()\n","\n","    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n","        \"\"\"Return an iterator over Module parameters.\n","\n","        recurse: if True, the iterator includes parameters of submodules, recursively.\n","        \"\"\"\n","        pass\n","\n","    def __setattr__(self, key: str, val: Any) -> None:\n","        \"\"\"\n","        If val is a Parameter or Module, store it in the appropriate _parameters or _modules dict.\n","        Otherwise, call the superclass.\n","        \"\"\"\n","        pass\n","\n","    def __getattr__(self, key: str) -> Union[Parameter, \"Module\"]:\n","        \"\"\"\n","        If key is in _parameters or _modules, return the corresponding value.\n","        Otherwise, raise KeyError.\n","        \"\"\"\n","        pass\n","\n","    def __call__(self, *args, **kwargs):\n","        return self.forward(*args, **kwargs)\n","\n","    def forward(self):\n","        raise NotImplementedError(\"Subclasses must implement forward!\")\n","\n","    def __repr__(self):\n","        pass\n","\n","\n","if MAIN:\n","\n","    class TestInnerModule(Module):\n","        def __init__(self):\n","            super().__init__()\n","            self.param1 = Parameter(Tensor([1.0]))\n","            self.param2 = Parameter(Tensor([2.0]))\n","\n","    class TestModule(Module):\n","        def __init__(self):\n","            super().__init__()\n","            self.inner = TestInnerModule()\n","            self.param3 = Parameter(Tensor([3.0]))\n","\n","    mod = TestModule()\n","    assert list(mod.modules()) == [mod.inner]\n","    assert list(mod.parameters()) == [\n","        mod.param3,\n","        mod.inner.param1,\n","        mod.inner.param2,\n","    ], \"parameters should come before submodule parameters\"\n","    print(\"Manually verify that the repr looks reasonable:\")\n","    print(mod)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Build Your Own Linear Layer\n","\n","You may have a `Linear` written already that you can adapt to use our own `Parameter`, `Module`, and `Tensor`. If your `Linear` used `einsum`, use a `matmul` instead. You can implement a backward function for `einsum` in the bonus section.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Linear(Module):\n","    weight: Parameter\n","    bias: Optional[Parameter]\n","\n","    def __init__(self, in_features: int, out_features: int, bias=True):\n","        \"\"\"A simple linear (technically, affine) transformation.\n","\n","        The fields should be named `weight` and `bias` for compatibility with PyTorch.\n","        If `bias` is False, set `self.bias` to None.\n","        \"\"\"\n","        pass\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        \"\"\"\n","        x: shape (*, in_features)\n","        Return: shape (*, out_features)\n","        \"\"\"\n","        pass\n","\n","    def extra_repr(self) -> str:\n","        pass\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","Now we can define a MLP suitable for classifying MNIST, with zero PyTorch dependency!\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MLP(Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear1 = Linear(28 * 28, 64)\n","        self.linear2 = Linear(64, 64)\n","        self.output = Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = x.reshape((x.shape[0], 28 * 28))\n","        x = relu(self.linear1(x))\n","        x = relu(self.linear2(x))\n","        x = self.output(x)\n","        return x\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Build Your Own Cross-Entropy Loss\n","\n","Make use of your integer array indexing to implement `cross_entropy`.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor:\n","    \"\"\"Like torch.nn.functional.cross_entropy with reduction='none'.\n","\n","    logits: shape (batch, classes)\n","    true_labels: shape (batch,). Each element is the index of the correct label in the logits.\n","\n","    Return: shape (batch, ) containing the per-example loss.\n","    \"\"\"\n","    pass\n","\n","\n","if MAIN:\n","    w1d3_test.test_cross_entropy(Tensor, cross_entropy)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Build Your Own `no_grad`\n","\n","The last thing our backpropagation system needs is the ability to turn it off completely like `torch.no_grad`.\n","\n","Implement the `NoGrad` context manager so that it reads and writes the `grad_tracking_enabled` flag from the top of the file. Update your `tensor_func` inside `wrap_forward_fn` to check the flag. In general, using mutable global variables is not ideal because multiple threads will be a problem, but we will leave that for another day.\n","\n","Tip: to write to a global variable, you need to use the `global` keyword as in `global grad_tracking_enabled`, otherwise Python thinks that you want to create a new local variable.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NoGrad:\n","    \"\"\"Context manager that disables grad inside the block. Like torch.no_grad.\"\"\"\n","\n","    was_enabled: bool\n","\n","    def __enter__(self):\n","        pass\n","\n","    def __exit__(self, type, value, traceback):\n","        pass\n","\n","\n","if MAIN:\n","    w1d3_test.test_no_grad(Tensor, NoGrad)\n","    w1d3_test.test_no_grad_nested(Tensor, NoGrad)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Training Your Network\n","\n","Tomorrow is optimizer and training day where you'll be investigating SGD in much more depth and writing your own training loop, so we'll provide a minimal version of these today as well as the data loading code.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualize(dataloader):\n","    \"\"\"Call this if you want to see some of your data.\"\"\"\n","    plt.figure(figsize=(12, 12))\n","    (sample, sample_labels) = next(iter(dataloader))\n","    for i in range(10):\n","        plt.subplot(5, 5, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.imshow(sample[i, 0], cmap=plt.cm.binary)\n","    plt.show()\n","\n","\n","if MAIN:\n","    subsample = 20 if IS_CI else None\n","    (train_loader, test_loader) = w1d3_utils.get_mnist(subsample)\n","\n","\n","class SGD:\n","    def __init__(self, params: Iterable[Parameter], lr: float):\n","        \"\"\"Vanilla SGD with no additional features.\"\"\"\n","        self.params = list(params)\n","        self.lr = lr\n","        self.b = [None for _ in self.params]\n","\n","    def zero_grad(self) -> None:\n","        for p in self.params:\n","            p.grad = None\n","\n","    def step(self) -> None:\n","        with NoGrad():\n","            for (i, p) in enumerate(self.params):\n","                assert isinstance(p.grad, Tensor)\n","                p.add_(p.grad, -self.lr)\n","\n","\n","def train(model, train_loader, optimizer, epoch):\n","    for (batch_idx, (data, target)) in enumerate(train_loader):\n","        data = Tensor(data.numpy())\n","        target = Tensor(target.numpy())\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = cross_entropy(output, target).sum() / len(output)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 50 == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    loss.item(),\n","                )\n","            )\n","\n","\n","def test(model, test_loader):\n","    test_loss = 0\n","    correct = 0\n","    with NoGrad():\n","        for (data, target) in test_loader:\n","            data = Tensor(data.numpy())\n","            target = Tensor(target.numpy())\n","            output = model(data)\n","            test_loss += cross_entropy(output, target).sum().item()\n","            pred = output.argmax(dim=1, keepdim=True)\n","            correct += (pred == target.reshape(pred.shape)).sum().item()\n","    test_loss /= len(test_loader.dataset)\n","    print(\n","        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n","            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n","        )\n","    )\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Training Loop\n","\n","To finish the day, let's see if everything works correctly and our MLP learns to classify MNIST. It's normal to encounter some bugs and glitches at this point - just go back and fix them until everything runs.\n","\n","My MLP was able to reach 88% accuracy after 5 epochs (around 15 seconds).\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MAIN:\n","    num_epochs = 5\n","    model = MLP()\n","    start = time.time()\n","    optimizer = SGD(model.parameters(), 0.01)\n","    for epoch in range(num_epochs):\n","        train(model, train_loader, optimizer, epoch)\n","        test(model, test_loader)\n","        optimizer.step()\n","    print(f\"Completed in {time.time() - start: .2f}s\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Bonus\n","\n","Congratulations on finishing the day's main content!\n","\n","### In-Place Operation Warnings\n","\n","The most severe issue with our current system is that it can silently compute the wrong gradients when in-place operations are used. Have a look at how [PyTorch handles it](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd) and implement a similar system yourself so that it either computes the right gradients, or raises a warning.\n","\n","### In-Place ReLU\n","\n","Instead of implementing ReLU in terms of maximum, implement your own forward and backward functions that support `inplace=True`.\n","\n","### Backward for einsum\n","\n","Write the backward pass for your equivalent of `torch.einsum`.\n","\n","### Reuse of Module during forward\n","\n","Consider the following MLP, where the same `nn.ReLU` instance is used twice in the forward pass. Without running the code, explain whether this works correctly or not with reference to the specifics of your implementation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class MyModule(Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear1 = Linear(28*28, 64)\n","        self.linear2 = Linear(64, 64)\n","        self.linear3 = Linear(64, 10)\n","        self.relu = ReLU()\n","\n","    def forward(self, x):\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        return self.linear3(x)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### ResNet Support\n","\n","Make a list of the features that would need to be implemented to support ResNet inference, and training. It will probably take too long to do all of them, but pick some interesting features to start implementing.\n","\n","### Central Difference Checking\n","\n","Write a function that compares the gradients from your backprop to a central difference method. See [Wikipedia](https://en.wikipedia.org/wiki/Finite_difference) for more details.\n","\n","### Non-Differentiable Function Support\n","\n","Your `Tensor` does not currently support equivalents of `torch.all`, `torch.any`, `torch.floor`, `torch.less`, etc. which are non-differentiable functions of Tensors. Implement them so that they are usable in computational graphs, but gradients shouldn't flow through them (their contribution is zero).\n","\n","### Differentiation wrt Keyword Arguments\n","\n","In the real PyTorch, you can sometimes pass tensors as keyword arguments and differentiation will work, as in `t.add(other=t.tensor([3,4]), input=t.tensor([1,2]))`. In other similar looking cases like `t.dot`, it raises an error that the argument must be passed positionally. Decide on a desired behavior in your system and implement and test it.\n","\n","### torch.stack\n","\n","So far we've registered a separate backwards for each input argument that could be a Tensor. This is problematic if the function can take any number of tensors like torch.stack or numpy.stack. Think of and implement the backward function for stack. It may require modification to your other code.\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"579b2684d86ca0df9eb99ef90db86ec500bac59b7b51493c9c97f9c9aabc92e6"}}},"nbformat":4,"nbformat_minor":4}
