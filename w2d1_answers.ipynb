{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# Week 2 Day 1 - Build Your Own BERT\n","\n","BERT (Bidirectional Encoder Representations from Transformers) is the most famous in a line of Muppet-themed language research, originating with [ELMo](https://arxiv.org/pdf/1802.05365v2.pdf) (Embeddings from Language Models) and continuing with a series of increasingly strained acronyms:\n","\n","- [Big BIRD](https://arxiv.org/pdf/1910.13034.pdf) - Big Bidirectional Insertion Representations for Documents\n","- [Ernie](https://arxiv.org/pdf/1904.09223.pdf) - Enhanced Representation through kNowledge IntEgration\n","- [Grover](https://arxiv.org/pdf/1905.12616.pdf) - Generating aRticles by Only Viewing mEtadata Records\n","- [Kermit](https://arxiv.org/pdf/1906.01604.pdf) - Kontextuell Encoder Representations Made by Insertion Transformations\n","\n","Today you'll implement your own BERT model such that it can load the weights from a full size pretrained BERT, and use it to predict some masked tokens.\n","\n","## Table of Contents\n","\n","- [Readings](#readings)\n","- [BERT architecture](#bert-architecture)\n","    - [Language model vs. classifier](#language-model-vs-classifier)\n","    - [Schematic](#schematic)\n","- [Batched Self-Attention](#batched-self-attention)\n","    - [Attention Pattern Pre-Softmax](#attention-pattern-pre-softmax)\n","    - [Attention Forward Function](#attention-forward-function)\n","- [Layer Normalization](#layer-normalization)\n","- [Embedding](#embedding)\n","- [BertMLP](#bertmlp)\n","- [Bert Block](#bert-block)\n","- [Putting it All Together](#putting-it-all-together)\n","    - [utils.StaticModuleList](#utilsstaticmodulelist)\n","- [BertLanguageModel](#bertlanguagemodel)\n","- [Loading Pretrained Weights](#loading-pretrained-weights)\n","- [Tokenization](#tokenization)\n","    - [Vocabulary](#vocabulary)\n","    - [Special Tokens](#special-tokens)\n","    - [Predicting Masked Tokens](#predicting-masked-tokens)\n","- [Model debugging](#model-debugging)\n","\n","## Readings\n","\n","- [Language Modelling with Transformers](https://docs.google.com/document/d/1XJQT8PJYzvL0CLacctWcT0T5NfL7dwlCiIqRtdTcIqA/edit#)\n","\n","You don't need to read the other Muppet papers for today's content.\n","\n","## BERT architecture\n","\n","There are various sizes of BERT, differing only in the number of BERT transformer blocks (\"BertBlock\") and the embedding size. We'll be playing with [bert-base-cased](https://huggingface.co/bert-base-cased) today, which has 12 layers and an embedding size of 768. Note that the link points to Hugging Face, which provides a repository of pretrained models (often, transformer models) as well as other valuable documentation.\n","\n","Refer to the below schematics for the architecture of BERT. Today we will be using BERT for language modelling, and tomorrow we will use it for classification. As most of the architecture is shared, we will be able to reuse most of the code as well.\n","\n","### Language model vs. classifier\n","\n","```mermaid\n","graph TD\n","    subgraph \" \"\n","            subgraph BertLanguageModel\n","            LBertCommon[Input<br/>From BertCommon] -->|embedding_size| LMHead[Linear<br/>GELU<br/>Layer Norm<br/>Tied Unembed]--> |vocab size|Output[Logit Output]\n","            end\n","\n","            subgraph BertClassifier\n","            CBertCommon[Input<br/>From BertCommon] -->|embedding_size| ClassHead[First Position Only<br/>Dropout<br/>Linear] -->|num_classes| ClsOutput[Classification<br/>Output]\n","            end\n","\n","    end\n","```\n","\n","### Schematic\n","\n","Note the \"zoomed-in\" view into `BertAttention` (and in turn, `BertSelfAttention`) as well as `BertMLP`.\n","\n","```mermaid\n","graph TD\n","    subgraph \" \"\n","            subgraph BertCommon\n","            Token --> |integer|TokenEmbed[Token<br/>Embedding] --> AddEmbed[Add] --> CommonLayerNorm[Layer Norm] --> Dropout --> BertBlocks[<u>BertBlock x12</u><br/>BertAttention<br/>BertMLP] --> Output\n","            Position --> |integer|PosEmbed[Positional<br/>Embedding] --> AddEmbed\n","            TokenType --> |integer|TokenTypeEmb[Token Type<br/>Embedding] --> AddEmbed\n","        end\n","\n","        subgraph BertAttention\n","            Input --> BertSelfInner[BertSelfAttention] --> AtnDropout[Dropout] --> AtnLayerNorm[Layer Norm] --> AtnOutput[Output]\n","            Input --> AtnLayerNorm\n","        end\n","\n","        subgraph BertSelfAttention\n","            SA[Input] --> Q & K & V\n","            V -->|head size| WeightedSum\n","            Q & K --> |head size|Dot[Dot<br/>Scale Down<br/>Softmax] -->WeightedSum -->|head size| O --> SAOutput[Output]\n","        end\n","\n","        subgraph BertMLP\n","            MLPInput[Input] --> Linear1 -->|intermediate size|GELU --> |intermediate size|Linear2 --> MLPDropout[Dropout] --> MLPLayerNorm --> MLPOutput[Output]\n","            MLPInput --> MLPLayerNorm[Layer Norm]\n","        end\n","    end\n","```\n","\n","# Implementation\n","\n","We will begin by importing necessary modules and defining `BertConfig` to store the model architecture parameters. Review the list of config entries and consider what each one means, reviewing the reading to familiarize yourself with transformer models if necessary.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (4.26.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (0.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (23.0)\n","Requirement already satisfied: filelock in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (1.24.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->transformers) (3.0.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: einops in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (0.6.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: fancy_einsum in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (0.0.3)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: torchtext in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (0.14.1)\n","Requirement already satisfied: numpy in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from torchtext) (1.24.2)\n","Requirement already satisfied: torch in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from torchtext) (1.13.1)\n","Requirement already satisfied: requests in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from torchtext) (2.28.2)\n","Requirement already satisfied: tqdm in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from torchtext) (4.64.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->torchtext) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->torchtext) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->torchtext) (3.0.1)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from requests->torchtext) (3.4)\n","Requirement already satisfied: typing-extensions in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from torch->torchtext) (4.4.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: torch in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (1.13.1)\n","Requirement already satisfied: typing-extensions in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from torch) (4.4.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Requirement already satisfied: joblib in /Users/davidmcsharry/.pyenv/versions/3.10.7/lib/python3.10/site-packages (1.2.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install transformers\n","!pip install einops\n","!pip install fancy_einsum\n","!pip install torchtext\n","!pip install torch\n","!pip install joblib\n","\n","\n","\n","import os\n","from dataclasses import dataclass\n","from typing import List, Optional, Union\n","import torch as t\n","import transformers\n","from einops import rearrange, repeat\n","from fancy_einsum import einsum\n","from torch import nn\n","from torch.nn import functional as F\n","import utils\n","import w2d1_test\n","\n","MAIN = __name__ == \"__main__\"\n","IS_CI = os.getenv(\"IS_CI\")\n","\n","\n","@dataclass(frozen=True)\n","class BertConfig:\n","    \"\"\"Constants used throughout the Bert model. Most are self-explanatory.\n","\n","    intermediate_size is the number of hidden neurons in the MLP (see schematic)\n","    type_vocab_size is only used for pretraining on \"next sentence prediction\", which we aren't doing.\n","\n","    Note that the head size happens to be hidden_size // num_heads, but this isn't necessarily true and your code shouldn't assume it.\n","    \"\"\"\n","\n","    vocab_size: int = 28996\n","    intermediate_size: int = 3072\n","    hidden_size: int = 768\n","    num_layers: int = 12\n","    num_heads: int = 12\n","    head_size: int = 64\n","    max_position_embeddings: int = 512\n","    dropout: float = 0.1\n","    type_vocab_size: int = 2\n","    layer_norm_epsilon: float = 1e-12\n","\n","\n","if MAIN:\n","    config = BertConfig()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Batched Self-Attention\n","\n","We're going to implement a version of self-attention that computes all sequences in a batch at once, and all heads at once. Make sure you understand how single sequence, single head attention works first, again consulting the reading to review this mechanism if you haven't already done so.\n","\n","\n","### Attention Pattern Pre-Softmax\n","\n","Write the attention_pattern_pre_softmax function as specified in the diagram.\n","\n","The \"Scale Down\" factor means dividing by the square root of the head size. Empirically, this helps training. [This article](https://github.com/BAI-Yeqi/Statistical-Properties-of-Dot-Product/blob/master/proof.pdf) gives some math to justify this, but it's not important.\n","\n","### Attention Forward Function\n","\n","Your forward should call `attention_pattern_pre_softmax`, add the attention mask to the result if present, and then finish the computations using `einsum` and `rearrange` again. Remember to apply the output projection.\n","\n","\n","Spend 5 minutes thinking about how to batch the computation before looking at the spoilers below.\n","\n","<details>\n","\n","<summary>What should the shape of `project_query` be?</summary>\n","\n","`project_query` should go from `hidden_size` to `num_heads * self.head_size`. In this case, the latter is equal to `hidden_size`. This represents all the heads's `Q` matrices concatenated together, and one call to it now computes all the queries at once (broadcasting over the leading batch and seq dimensions of the input `x`).\n","\n","</details>\n","\n","<details>\n","\n","<summary>Should my Linear layers have a bias?</summary>\n","\n","While these Linear layers are traditionally referred to as projections, and the BERT paper implies that they don't have a bias, in the official reference implementation of BERT they DO have a bias.\n","\n","</details>\n","\n","<details>\n","\n","<summary>What does the einsum to make the attention pattern look like?</summary>\n","\n","We need to sum out the head_size and keep the seq_q dimension before the seq_k dimension. For a single batch and single head, it would be: `einsum(\"seq_q head_size, seq_k head_size -> seq_q seq_k\")`. You'll want to do a `rearrange` before your `einsum`.\n","\n","</details>\n","\n","<details>\n","\n","<summary>Which dimension do I softmax over?</summary>\n","\n","The desired property is that after softmax, for any indices `batch`, `head`, and `q`, the vector `pattern[batch,head,q]` sums to 1. So the softmax needs to be over the `k` dimension.\n","\n","</details>\n","\n","<details>\n","\n","<summary>I'm still confused about how to batch the computation.</summary>\n","\n","Pre-softmax:\n","\n","- Apply `project_query`, `project_key`, and `project_value` to `x` to obtain `q`, `k`, and `v`.\n","- rearrange `q` and `k` to split the `head * head_size` dimension apart into `head` and `head_size` dimensions. The shape should go from `(batch seq (head * head_size))` to `(batch head seq head_size)`\n","- Einsum `q` and `k` to get a (batch, head, seq_q, seq_k) shape.\n","- Divide by the square root of the head size.\n","\n","Forward:\n","\n","- Softmax over the `k` dimension to obtain attention probs\n","- rearrange `v` just like `q` and `k` previously\n","- einsum `v` and your attention probs to get the weighted `v`\n","- rearrange weighted `v` to combine head and head_size and put that at the end\n","- apply `project_output`\n","\n","</details>\n","\n","Name your `Linear` layers as indicated in the class definition; otherwise the tests won't work and you'll have more trouble loading weights.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w2d1_test.test_attention_pattern_pre_softmax passed in 0.03s.\n","w2d1_test.test_attention passed in 0.00s.\n"]}],"source":["class BertSelfAttention(nn.Module):\n","    project_query: nn.Linear\n","    project_key: nn.Linear\n","    project_value: nn.Linear\n","    project_output: nn.Linear\n","\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.layer_norm_epsilon = config.layer_norm_epsilon\n","        self.head_size = config.head_size\n","        self.num_heads = config.num_heads\n","        self.project_query = nn.Linear(config.hidden_size, config.num_heads * config.head_size)\n","        self.project_key = nn.Linear(config.hidden_size, config.num_heads * config.head_size)\n","        self.project_value = nn.Linear(config.hidden_size, config.num_heads * config.head_size)\n","        self.project_output = nn.Linear(config.num_heads * config.head_size, config.hidden_size)\n","\n","    def attention_pattern_pre_softmax(self, x: t.Tensor) -> t.Tensor:\n","        \"\"\"\n","        x: shape (batch, seq, hidden_size)\n","        Return the attention pattern after scaling but before softmax.\n","\n","        pattern[batch, head, q, k] should be the match between a query at sequence position q and a key at sequence position k.\n","        \"\"\"\n","        # output QK^T/sqrt(d_k)        \n","               \n","        Q = rearrange(self.project_query(x), 'bs q (nh hs) -> bs nh q hs', nh = self.num_heads)\n","        K = rearrange(self.project_key(x), 'bs q (nh hs) -> bs nh hs q', nh = self.num_heads)\n","        pre_softmax_attention_scores = t.einsum(\n","            'ab ij, ab jk -> ab ik',\n","            Q,K\n","        )\n","        return pre_softmax_attention_scores / self.head_size ** 0.5\n","\n","\n","    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n","        \"\"\"\n","        additive_attention_mask: shape (batch, head=1, seq_q=1, seq_k) - used in training to prevent copying data from padding tokens. Contains 0 for a real input token and a large negative number for a padding token. If provided, add this to the attention pattern (pre softmax).\n","\n","        Return: (batch, seq, hidden_size)\n","        \"\"\"\n","        pre_softmax_attention_scores = self.attention_pattern_pre_softmax(x)\n","        if additive_attention_mask is not None:\n","            pre_softmax_attention_scores += additive_attention_mask\n","        attention_scores = F.softmax(pre_softmax_attention_scores, dim=-1)\n","\n","        V = rearrange(\n","            self.project_value(x),\n","            'bs q (nh hs) -> bs nh q hs',\n","            nh = self.num_heads\n","        )\n","\n","        attention_weighted_value = t.einsum(\n","            'ab ij, ab jk -> ab ik',\n","            attention_scores, V\n","        )\n","\n","        attention_weighted_value = rearrange(\n","            attention_weighted_value,\n","            'bs nh q hs -> bs q (nh hs)'\n","        )\n","        \n","        return self.project_output(attention_weighted_value)\n","\n","if MAIN:\n","    w2d1_test.test_attention_pattern_pre_softmax(BertSelfAttention)\n","    w2d1_test.test_attention(BertSelfAttention)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Layer Normalization\n","\n","Use the ([PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)) for Layer Normalization to implement your own version which exactly mimics the official API. Use the biased estimator for $Var[x]$ as shown in the docs.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w2d1_test.test_layernorm_mean_1d passed in 0.00s.\n","w2d1_test.test_layernorm_mean_2d passed in 0.00s.\n","w2d1_test.test_layernorm_std passed in 0.00s.\n","w2d1_test.test_layernorm_exact passed in 0.00s.\n","w2d1_test.test_layernorm_backward passed in 0.01s.\n"]}],"source":["class LayerNorm(nn.Module):\n","    weight: nn.Parameter\n","    bias: nn.Parameter\n","\n","    def __init__(\n","        self, normalized_shape: Union[int, tuple, t.Size], eps=1e-12, elementwise_affine=True, device=None, dtype=None\n","    ):\n","        super().__init__()\n","        if isinstance(normalized_shape, int):\n","            normalized_shape = (normalized_shape,)\n","        normalized_shape = tuple(normalized_shape)\n","        self.normalized_shape = normalized_shape\n","        self.eps = eps\n","        self.elementwise_affine = elementwise_affine\n","        if self.elementwise_affine:\n","            self.weight = nn.Parameter(t.empty(normalized_shape, device=device, dtype=dtype))\n","            self.bias = nn.Parameter(t.empty(normalized_shape, device=device, dtype=dtype))\n","        else:\n","            self.register_parameter(\"weight\", None)\n","            self.register_parameter(\"bias\", None)\n","\n","        self.reset_parameters()\n","\n","    \n","    def reset_parameters(self) -> None:\n","        \"\"\"Initialize the weight and bias, if applicable.\"\"\"\n","        if self.elementwise_affine:\n","            nn.init.ones_(self.weight)\n","            nn.init.zeros_(self.bias)\n","\n","    def forward(self, x: t.Tensor) -> t.Tensor:\n","        \"\"\"x and the output should both have shape (batch, *).\"\"\"\n","        x = (x - x.mean(dim=-1, keepdim=True)) / t.sqrt(x.var(dim=-1, keepdim=True, unbiased=False) + self.eps)\n","        if self.elementwise_affine:\n","            x = x * self.weight + self.bias\n","        return x\n","\n","if MAIN:\n","    w2d1_test.test_layernorm_mean_1d(LayerNorm)\n","    w2d1_test.test_layernorm_mean_2d(LayerNorm)\n","    w2d1_test.test_layernorm_std(LayerNorm)\n","    w2d1_test.test_layernorm_exact(LayerNorm)\n","    w2d1_test.test_layernorm_backward(LayerNorm)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Embedding\n","\n","Implement your version of PyTorch's `nn.Embedding` module. The PyTorch version has some extra options in the constructor, but you don't need to implement those since BERT doesn't use them.\n","\n","The `Parameter` should be named `weight` and initialized with normally distributed random values with a mean of 0 and std of 0.02.\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w2d1_test.test_embedding passed in 0.00s.\n","w2d1_test.test_embedding_std passed in 0.00s.\n"]}],"source":["class Embedding(nn.Module):\n","    num_embeddings: int\n","    embedding_dim: int\n","    weight: nn.Parameter\n","\n","    def __init__(self, num_embeddings: int, embedding_dim: int):\n","        super().__init__()\n","        self.num_embeddings = num_embeddings\n","        self.embedding_dim = embedding_dim\n","        self.weight = nn.Parameter(t.empty(num_embeddings, embedding_dim))\n","        nn.init.normal_(self.weight, mean=0, std=0.02)\n","\n","\n","\n","    def forward(self, x: t.LongTensor) -> t.Tensor:\n","        \"\"\"For each integer in the input, return that row of the embedding.\n","\n","        Don't convert x to one-hot vectors - this works but is too slow.\n","        \"\"\"\n","        return self.weight[x]\n","\n","    def extra_repr(self) -> str:\n","        return f\"{self.num_embeddings}, {self.embedding_dim}\"\n","\n","\n","if MAIN:\n","    assert repr(Embedding(10, 20)) == repr(t.nn.Embedding(10, 20))\n","    w2d1_test.test_embedding(Embedding)\n","    w2d1_test.test_embedding_std(Embedding)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## BertMLP\n","\n","Make the MLP block, following the schematic. Use `nn.Dropout` for the dropout layer.\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w2d1_test.test_bert_mlp_zero_dropout passed in 0.01s.\n","w2d1_test.test_bert_mlp_one_dropout passed in 0.00s.\n"]}],"source":["class BertMLP(nn.Module):\n","    first_linear: nn.Linear\n","    second_linear: nn.Linear\n","    layer_norm: LayerNorm\n","\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.first_linear = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.second_linear = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.layer_norm = LayerNorm(config.hidden_size, eps = config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","\n","    def forward(self, x: t.Tensor) -> t.Tensor:\n","        original_x = x\n","        x = self.first_linear(x)\n","        x = F.gelu(x)\n","        x = self.second_linear(x)\n","        x = self.dropout(x)\n","        x = self.layer_norm(x + original_x)\n","        return x\n","\n","if MAIN:\n","    w2d1_test.test_bert_mlp_zero_dropout(BertMLP)\n","    w2d1_test.test_bert_mlp_one_dropout(BertMLP)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Bert Block\n","\n","Assemble the `BertAttention` and `BertBlock` classes following the schematic.\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["w2d1_test.test_bert_attention_dropout passed in 0.01s.\n","Test failed. Max absolute deviation: 1.430511474609375e-06\n","Actual:\n","tensor([[[-0.5345, -0.5477,  1.0330,  ..., -0.4000, -0.8045, -0.1134],\n","         [ 1.1591, -0.9310, -1.1030,  ...,  0.8801,  1.0987,  0.3142],\n","         [-1.0729, -1.0277,  0.3382,  ...,  1.2628, -0.0032, -1.1757]],\n","\n","        [[-0.1461, -1.1046, -0.0057,  ...,  1.6682, -1.3070, -0.8908],\n","         [-1.3667,  1.1291,  1.6672,  ...,  0.6607,  0.2909, -0.9596],\n","         [-0.5694, -1.3173, -1.0107,  ..., -0.0240,  0.6005, -1.5292]]],\n","       grad_fn=<AddBackward0>)\n","Expected:\n","tensor([[[-0.5345, -0.5477,  1.0330,  ..., -0.4000, -0.8045, -0.1134],\n","         [ 1.1591, -0.9310, -1.1030,  ...,  0.8801,  1.0987,  0.3142],\n","         [-1.0729, -1.0277,  0.3382,  ...,  1.2628, -0.0032, -1.1757]],\n","\n","        [[-0.1461, -1.1046, -0.0057,  ...,  1.6682, -1.3070, -0.8908],\n","         [-1.3667,  1.1291,  1.6672,  ...,  0.6607,  0.2909, -0.9596],\n","         [-0.5694, -1.3173, -1.0107,  ..., -0.0240,  0.6005, -1.5292]]],\n","       grad_fn=<AddBackward0>)\n"]},{"ename":"AssertionError","evalue":"allclose failed with 5 / 4608 entries outside tolerance","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m MAIN:\n\u001b[1;32m     41\u001b[0m     \u001b[39m# tiny weird error that's failing test\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     w2d1_test\u001b[39m.\u001b[39;49mtest_bert_block(BertBlock)\n","File \u001b[0;32m~/Documents/mlab/utils.py:148\u001b[0m, in \u001b[0;36mreport.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@wraps\u001b[39m(test_func)\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m run_and_report(test_func, name, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Documents/mlab/utils.py:155\u001b[0m, in \u001b[0;36mrun_and_report\u001b[0;34m(test_func, name, *test_func_args, **test_func_kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_and_report\u001b[39m(test_func: Callable, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39mtest_func_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest_func_kwargs):\n\u001b[1;32m    154\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 155\u001b[0m     out \u001b[39m=\u001b[39m test_func(\u001b[39m*\u001b[39;49mtest_func_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtest_func_kwargs)\n\u001b[1;32m    156\u001b[0m     elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n\u001b[1;32m    157\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m passed in \u001b[39m\u001b[39m{\u001b[39;00melapsed\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms.\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Documents/mlab/w2d1_test.py:208\u001b[0m, in \u001b[0;36mtest_bert_block\u001b[0;34m(BertBlock)\u001b[0m\n\u001b[1;32m    206\u001b[0m ref\u001b[39m.\u001b[39meval()\n\u001b[1;32m    207\u001b[0m yours\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 208\u001b[0m allclose(yours(x), ref(x))\n","File \u001b[0;32m~/Documents/mlab/utils.py:74\u001b[0m, in \u001b[0;36mallclose\u001b[0;34m(actual, expected, rtol)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest failed. Max absolute deviation: \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mActual:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mactual\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mExpected:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexpected\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallclose failed with \u001b[39m\u001b[39m{\u001b[39;00mnum_wrong\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mnelement()\u001b[39m}\u001b[39;00m\u001b[39m entries outside tolerance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[39melif\u001b[39;00m DEBUG_TOLERANCES:\n\u001b[1;32m     76\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest passed with max absolute deviation of \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: allclose failed with 5 / 4608 entries outside tolerance"]}],"source":["class BertAttention(nn.Module):\n","    self_attn: BertSelfAttention\n","    layer_norm: LayerNorm\n","\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.self_attn = BertSelfAttention(config)\n","        self.layer_norm = LayerNorm(config.hidden_size, eps = config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","\n","    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n","        original_x = x\n","        x = self.self_attn(x, additive_attention_mask)\n","        x = self.dropout(x)\n","        x = self.layer_norm(x + original_x)\n","        return x\n","\n","\n","if MAIN:\n","    w2d1_test.test_bert_attention_dropout(BertAttention)\n","\n","\n","class BertBlock(nn.Module):\n","    attention: BertAttention\n","    mlp: BertMLP\n","\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.attention = BertAttention(config)\n","        self.mlp = BertMLP(config)\n","\n","\n","    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n","        x = self.attention(x, additive_attention_mask)\n","        x = self.mlp(x)\n","        return x\n","\n","\n","if MAIN:\n","    # tiny weird error that's failing test\n","    w2d1_test.test_bert_block(BertBlock)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Putting it All Together\n","\n","Now put the pieces together. We're going to have a `BertLMHead`, noting the following:\n","\n","- The language modelling `Linear` layer after the blocks has shape `(embedding_size, embedding_size)`.\n","- If `token_type_ids` isn't provided to `forward`, make it the same shape as `input_ids` but filled with all zeros.\n","- The unembedding at the end that takes data from `hidden_size` to `vocab_size` shouldn't be its own `Linear` layer because it shares the same data as `token_embedding.weight`. Just reuse `token_embedding.weight` and add a bias term.\n","- Print your model out to see if it resembles the schematic.\n","\n","The tokenizer will produce `one_zero_attention_mask`, but our `SelfAttention` needs `additive_attention_mask`. This mask is the same for every layer, so we can compute it once at the beginning of BERT's forward method. This will prevent `SelfAttention` from reading any data from the padding tokens.\n","\n","### utils.StaticModuleList\n","\n","If you use a regular `nn.ModuleList` to hold your `BertBlock`s, the typechecker can't tell they are `BertBlock`s anymore and only knows that they're `nn.Module`.\n","\n","We've provided a subclass `utils.StaticModuleList`, allowing us to declare in the class definition that this container really only contains `BertBlock` and no other types. The `repr` of `nn.ModuleList` also prints out all the children, which produces unreadable output for large numbers of layers; our `repr` is more concise.\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class BertCommon(nn.Module):\n","    token_embedding: Embedding\n","    pos_embedding: Embedding\n","    token_type_embedding: Embedding\n","    layer_norm: LayerNorm\n","    blocks: nn.ModuleList\n","\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.token_embedding = Embedding(config.vocab_size, config.hidden_size)\n","        self.pos_embedding = Embedding(config.max_position_embeddings, config.hidden_size)\n","        self.token_type_embedding = Embedding(config.type_vocab_size, config.hidden_size)\n","        self.layer_norm = LayerNorm(config.hidden_size, eps = config.layer_norm_epsilon)\n","        self.blocks = nn.ModuleList([BertBlock(config) for _ in range(config.num_layers)])\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def _make_additive_attention_mask(\n","        self,\n","        one_zero_attention_mask: t.Tensor,\n","        big_negative_number: float = -10000\n","    ) -> t.Tensor:\n","\n","        \"\"\"\n","        one_zero_attention_mask: shape (batch, seq). Contains 1 if this is a valid token and 0 if it is a padding token.\n","        big_negative_number: Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n","\n","        Out: shape (batch, heads, seq, seq). Contains 0 if attention is allowed, and big_negative_number if it is not allowed.\n","        \"\"\"\n","        return rearrange((1 - one_zero_attention_mask), 'b s -> b 1 1 s') * big_negative_number\n","\n","    \n","    def forward(\n","        self,\n","        input_ids: t.Tensor,\n","        token_type_ids: Optional[t.Tensor] = None,\n","        one_zero_attention_mask: Optional[t.Tensor] = None,\n","    ) -> t.Tensor:\n","        \"\"\"\n","        input_ids: (batch, seq) - the token ids\n","        token_type_ids: (batch, seq) - only used for next sentence prediction.\n","        one_zero_attention_mask: (batch, seq) - only used in training. See make_additive_attention_mask.\n","        \"\"\"\n","        token_embeddings = self.token_embedding(input_ids)\n","        pos_embeddings = self.pos_embedding(t.arange(input_ids.shape[1], device=input_ids.device))\n","        \n","        if not token_type_ids:\n","            token_type_ids = t.zeros_like(input_ids)\n","\n","        token_type_embeddings = self.token_type_embedding(token_type_ids)\n","        x = token_embeddings + pos_embeddings + token_type_embeddings\n","        x = self.layer_norm(x)\n","        x = self.dropout(x)\n","\n","        if one_zero_attention_mask:\n","            additive_attention_mask = self._make_additive_attention_mask(one_zero_attention_mask)\n","        else:\n","            additive_attention_mask = None\n","\n","        for block in self.blocks:\n","            x = block(x, additive_attention_mask)\n","        return x\n","\n","\n","\n","\n","\n","        \n","        \n","                \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## BertLanguageModel\n","\n","<details>\n","\n","<summary>I can't figure out why my model's outputs are off by a very small amount, like 0.0005!</summary>\n","\n","Check that you're passing the correct layer norm epsilon through the network. The PyTorch default is 1e-5, but BERT used 1e-12.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test failed. Max absolute deviation: 3.606081008911133e-06\n","Actual:\n","tensor([[[-0.5903,  0.3522,  0.3753,  ..., -0.6550,  0.2556, -1.0716],\n","         [ 0.1514,  0.3186,  0.7801,  ..., -0.0321,  0.7688, -1.4717],\n","         [ 0.2311,  0.2211,  1.2228,  ..., -0.2644,  0.5473, -1.0289],\n","         ...,\n","         [-0.0398,  0.8780,  0.4739,  ..., -0.0110,  1.0243, -0.6003],\n","         [-0.0889,  0.1358,  1.1366,  ..., -0.7023, -0.0771, -0.7481],\n","         [-0.0919, -0.2449,  0.8229,  ...,  0.1497,  0.5532, -0.5897]]],\n","       grad_fn=<AddBackward0>)\n","Expected:\n","tensor([[[-0.5903,  0.3522,  0.3753,  ..., -0.6550,  0.2556, -1.0716],\n","         [ 0.1514,  0.3186,  0.7801,  ..., -0.0321,  0.7688, -1.4717],\n","         [ 0.2311,  0.2211,  1.2228,  ..., -0.2644,  0.5473, -1.0289],\n","         ...,\n","         [-0.0398,  0.8780,  0.4739,  ..., -0.0110,  1.0243, -0.6003],\n","         [-0.0889,  0.1358,  1.1366,  ..., -0.7023, -0.0771, -0.7481],\n","         [-0.0919, -0.2449,  0.8229,  ...,  0.1497,  0.5532, -0.5897]]],\n","       grad_fn=<AddBackward0>)\n"]},{"ename":"AssertionError","evalue":"allclose failed with 1457 / 202972 entries outside tolerance","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m MAIN:\n\u001b[0;32m---> 36\u001b[0m     w2d1_test\u001b[39m.\u001b[39;49mtest_bert(BertLanguageModel)\n","File \u001b[0;32m~/Documents/mlab/utils.py:148\u001b[0m, in \u001b[0;36mreport.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m@wraps\u001b[39m(test_func)\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m run_and_report(test_func, name, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Documents/mlab/utils.py:155\u001b[0m, in \u001b[0;36mrun_and_report\u001b[0;34m(test_func, name, *test_func_args, **test_func_kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_and_report\u001b[39m(test_func: Callable, name: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39mtest_func_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtest_func_kwargs):\n\u001b[1;32m    154\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 155\u001b[0m     out \u001b[39m=\u001b[39m test_func(\u001b[39m*\u001b[39;49mtest_func_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtest_func_kwargs)\n\u001b[1;32m    156\u001b[0m     elapsed \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n\u001b[1;32m    157\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m passed in \u001b[39m\u001b[39m{\u001b[39;00melapsed\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms.\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Documents/mlab/w2d1_test.py:244\u001b[0m, in \u001b[0;36mtest_bert\u001b[0;34m(your_module)\u001b[0m\n\u001b[1;32m    242\u001b[0m theirs\u001b[39m.\u001b[39mload_state_dict(reference\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m    243\u001b[0m input_ids \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor([[\u001b[39m101\u001b[39m, \u001b[39m1309\u001b[39m, \u001b[39m6100\u001b[39m, \u001b[39m1660\u001b[39m, \u001b[39m1128\u001b[39m, \u001b[39m1146\u001b[39m, \u001b[39m102\u001b[39m]], dtype\u001b[39m=\u001b[39mt\u001b[39m.\u001b[39mint64)\n\u001b[0;32m--> 244\u001b[0m allclose(theirs(input_ids\u001b[39m=\u001b[39;49minput_ids), reference(input_ids\u001b[39m=\u001b[39;49minput_ids))\n","File \u001b[0;32m~/Documents/mlab/utils.py:74\u001b[0m, in \u001b[0;36mallclose\u001b[0;34m(actual, expected, rtol)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest failed. Max absolute deviation: \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mActual:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mactual\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mExpected:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mexpected\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallclose failed with \u001b[39m\u001b[39m{\u001b[39;00mnum_wrong\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mnelement()\u001b[39m}\u001b[39;00m\u001b[39m entries outside tolerance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[39melif\u001b[39;00m DEBUG_TOLERANCES:\n\u001b[1;32m     76\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest passed with max absolute deviation of \u001b[39m\u001b[39m{\u001b[39;00mleft\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: allclose failed with 1457 / 202972 entries outside tolerance"]}],"source":["class BertLanguageModel(nn.Module):\n","    common: BertCommon\n","    lm_linear: nn.Linear\n","    lm_layer_norm: LayerNorm\n","    unembed_bias: nn.Parameter\n","\n","    def __init__(self, config: BertConfig):\n","        super().__init__()\n","        self.common = BertCommon(config)\n","        self.lm_linear = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.lm_layer_norm = LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n","        self.unembed_bias = nn.Parameter(t.zeros(config.vocab_size))\n","\n","    def forward(\n","        self,\n","        input_ids: t.Tensor,\n","        token_type_ids: Optional[t.Tensor] = None,\n","        one_zero_attention_mask: Optional[t.Tensor] = None,\n","    ) -> t.Tensor:\n","        \"\"\"Compute logits for each token in the vocabulary.\n","\n","        Return: shape (batch, seq, vocab_size)\n","        \"\"\"\n","        x = self.common(input_ids, token_type_ids, one_zero_attention_mask)\n","        x = self.lm_linear(x)\n","        x = F.gelu(x)\n","        x = self.lm_layer_norm(x)\n","        # unembed with original embedding matrix\n","        x = x @ self.common.token_embedding.weight.t() + self.unembed_bias\n","        return x\n","\n","\n","    \n","if MAIN:\n","    w2d1_test.test_bert(BertLanguageModel)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Loading Pretrained Weights\n","\n","Now copy parameters from the pretrained BERT returned by `utils.load_pretrained_bert()` into your BERT. This is definitely tedious and it's traditional to groan about how boring this is, but is representative of real ML work and we want you to have an Authentic ML Experience.\n","\n","Remember that the embedding and unembedding weights are tied, so `hf_bert.bert.embeddings.word_embeddings.weight` and `hf_bert.cls.predictions.decoder.weight` should be equal and you should only use one of them.\n","\n","Feel free to copy over the solution if you get frustrated.\n","\n","<details>\n","\n","<summary>I'm confused about my `Parameter` not being a leaf!</summary>\n","\n","When you copied data from the HuggingFace version, PyTorch tracked the history of the copy operation. This means if you were to call `backward`, it would try to backpropagate through your Parameter back to the HuggingFace version, which is not what we want. To fix this, you can call `detach()` to make a new tensor that shares storage with the original but doesn't have copy its history for backpropagation.\n","\n","</details>\n","\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def load_pretrained_weights(config: BertConfig) -> BertLanguageModel:\n","    hf_bert = utils.load_pretrained_bert()\n","    my_bert = BertLanguageModel(config)\n","    # print(hf_bert)\n","    # print(my_bert)\n","    \n","    def _copy(ours, src):\n","        ours.weight.detach().copy_(src.weight)\n","        if getattr(ours, \"bias\", None) is not None:\n","            ours.bias.detach().copy_(src.bias)\n","\n","    # init the my_bert weights as Nan\n","    for p in my_bert.parameters():\n","        p = p.detach().fill_(float(\"nan\"))\n","    \n","    _copy(my_bert.lm_linear, hf_bert.cls.predictions.transform.dense)  \n","    _copy(my_bert.lm_layer_norm, hf_bert.cls.predictions.transform.LayerNorm)\n","    my_bert.unembed_bias.detach().copy_(hf_bert.cls.predictions.decoder.bias)\n","    _copy(my_bert.common.token_embedding, hf_bert.bert.embeddings.word_embeddings)\n","    _copy(my_bert.common.pos_embedding, hf_bert.bert.embeddings.position_embeddings)\n","    _copy(my_bert.common.token_type_embedding, hf_bert.bert.embeddings.token_type_embeddings)\n","    _copy(my_bert.common.layer_norm, hf_bert.bert.embeddings.LayerNorm)\n","\n","    for i, block in enumerate(my_bert.common.blocks):\n","        _copy(block.mlp.first_linear, hf_bert.bert.encoder.layer[i].intermediate.dense)\n","        _copy(block.mlp.second_linear, hf_bert.bert.encoder.layer[i].output.dense)\n","        _copy(block.mlp.layer_norm, hf_bert.bert.encoder.layer[i].output.LayerNorm)\n","        _copy(block.attention.layer_norm, hf_bert.bert.encoder.layer[i].attention.output.LayerNorm)\n","        _copy(block.attention.self_attn.project_query, hf_bert.bert.encoder.layer[i].attention.self.query)\n","        _copy(block.attention.self_attn.project_key, hf_bert.bert.encoder.layer[i].attention.self.key)\n","        _copy(block.attention.self_attn.project_value, hf_bert.bert.encoder.layer[i].attention.self.value)\n","        _copy(block.attention.self_attn.project_output, hf_bert.bert.encoder.layer[i].attention.output.dense)\n","\n","\n","    for p in my_bert.parameters():\n","        assert not t.isnan(p).any(), f\"Parameter {p} is NaN\"\n","    \n","    return my_bert\n","\n","    \n","\n","if MAIN:\n","    my_bert = load_pretrained_weights(config)\n","    for (name, p) in my_bert.named_parameters():\n","        assert (\n","            p.is_leaf\n","        ), \"Parameter {name} is not a leaf node, which will cause problems in training. Try adding detach() somewhere.\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Tokenization\n","\n","We're going to use a HuggingFace tokenizer for now to encode text into a sequence of tokens that our model can use. The tokenizer has to match the model - our model was trained with the `bert-base-cased` tokenizer which is case-sensitive. If you tried to use the `bert-base-uncased` tokenizer which is case-insensitive, it wouldn't work at all.\n","\n","Use `transformers.AutoTokenizer.from_pretrained` to automatically fetch the appropriate tokenizer and try encoding and decoding some text.\n","\n","### Vocabulary\n","\n","Check out `tokenizer.vocab` to get an idea of what sorts of strings are assigned to tokens. In WordPiece, tokens represent a whole word unless they start with `##`, which denotes this token is part of a word.\n","\n","### Special Tokens\n","\n","Check out `tokenizer.special_tokens_map`. The strings here are mapped to tokens which have special meanings - for example `tokenizer.mask_token`, which is the literal string '[MASK]', is converted to `tokenizer.mask_token_id`, equal to 103.\n","\n","### Predicting Masked Tokens\n","\n","Write the `predict` function which takes a string with one or more instances of the substring '[MASK]', runs it through your model, finds the top K predictions and decodes each prediction.\n","\n","Tips:\n","\n","- `torch.topk()` is useful for identifying the `k` largest elements.\n","- The model should be in evaluation mode for predictions - this disables dropout and makes the predictions deterministic.\n","- If your model gives different predictions than the HuggingFace section, proceed to the next section on debugging.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS] I like to eat pizza. [SEP] I like to eat pasta. [SEP]\n"]},{"ename":"IndexError","evalue":"too many indices for tensor of dimension 2","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[124], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m sen1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI like to eat pizza.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m sen2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI like to eat pasta.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m next_sentence_prediction(my_bert, sen1, sen2, tokenizer)\n\u001b[1;32m     45\u001b[0m \u001b[39m#print(next_sentence_prediction(my_bert, sen1, sen2, tokenizer))\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m#predictions = predict(my_bert, tokenizer, your_text)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m#print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\u001b[39;00m\n","Cell \u001b[0;32mIn[124], line 26\u001b[0m, in \u001b[0;36mnext_sentence_prediction\u001b[0;34m(model, sen1, sen2, tokenizer)\u001b[0m\n\u001b[1;32m     24\u001b[0m out \u001b[39m=\u001b[39m model(input_ids)\n\u001b[1;32m     25\u001b[0m \u001b[39m# get the value for the [CLS] token in the output\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m cls_value \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39;49m][:, \u001b[39m0\u001b[39;49m, :]\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(cls_value)\n","\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"]}],"source":["def predict(model: BertLanguageModel, tokenizer, text: str, k=15) -> List[List[str]]:\n","    \"\"\"\n","    Return a list of k strings for each [MASK] in the input.\n","    \"\"\"\n","    model.eval()\n","    #return_tensors=\"pt\" jsut returns a tensor of the corrext shape to be fed into the model\n","    input_ids = tokenizer(text, return_tensors=\"pt\")['input_ids']\n","    out = model(input_ids)\n","    log_likelihoods = out[input_ids == tokenizer.mask_token_id]\n","    top_k_likely_indices = t.topk(log_likelihoods, k, dim=-1).indices\n","    top_k_likely_tokens = [[tokenizer.decode([i]) for i in top_k_likely_indices_for_each_mask] for top_k_likely_indices_for_each_mask in top_k_likely_indices]\n","    return top_k_likely_tokens\n","\n","def next_sentence_prediction(model: BertLanguageModel, sen1: str, sen2: str, tokenizer) -> bool:\n","    \"\"\"\n","    Predict whether a sentence is the next sentence in a sequence.\n","    \"\"\"\n","    # .eval() is a method from the nn.Module parent class to all our models\n","    model.eval()\n","    # tokenize then concatonate the two sentences seperated by the [SEP] token\n","    input_ids = tokenizer(sen1 + tokenizer.sep_token + sen2, return_tensors=\"pt\")['input_ids']\n","    print(tokenizer.decode(input_ids[0]))\n","    # get the output of the model\n","    out = model(input_ids)\n","    # get the value for the [CLS] token in the output\n","    cls_value = out[0][:, 0, :]\n","    print(cls_value)\n","\n","    \n","\n","if MAIN and (not IS_CI):\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","    #w2d1_test.test_bert_prediction(predict, my_bert, tokenizer)\n","    #your_text = \"My favourite food is [MASK].\"\n","    sen1 = \"I like to eat pizza.\"\n","    sen2 = \"I like to eat pasta.\"\n","    next_sentence_prediction(my_bert, sen1, sen2, tokenizer)\n","    #print(next_sentence_prediction(my_bert, sen1, sen2, tokenizer))\n","    #predictions = predict(my_bert, tokenizer, your_text)\n","    #print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Model debugging\n","\n","If your model works correctly at this point then congratulations, you can skip this section.\n","\n","The challenge with debugging ML code is that it often silently computes the wrong result instead of erroring out. Some things you can check:\n","\n","- Do I have any square matrices transposed, so the shapes still match but they do the wrong thing?\n","- Did I forget to pass any optional arguments, and the wrong default is being used?\n","- If I `print` my model, do the layers look right?\n","- Can I add `assert`s in my code to check assumptions that I've made? In particular, sometimes unintentional broadcasting creates outputs of the wrong shape.\n","- Is a tensor supposed to consist of `float`s, but might actually consist of `int`s? This can be tricky, because `t.tensor([1,2,3])` will produce a integer tensor, but if any one of the elements is a float, like `t.tensor([1,2,3.])`, then it will be a float tensor.\n","\n","You won't always have a reference implementation, but given that you do courtesy of HuggingFace, a good technique is to use hooks to collect the inputs and outputs that should be identical and compare when they start to diverge. This narrows down the number of places where you have to look for the bug.\n","\n","Read the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) for `register_forward_hook` on a `nn.Module` and try logging the input and output of each block on your model and the HuggingFace version. Note that you can use your forward hook to access model parameters upon the completion of a `forward()` output and use these parameters in ordinary Python data structures. Also, you may use `utils.allclose_atol()` to, as with many tests that you have already encountered, check whether two tensors have values within a specified tolerance.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells requires jupyter and notebook package.\n","\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n","\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n","\u001b[1;31mor\n","\u001b[1;31mconda install jupyter notebook -U'\n","\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."]}],"source":["if MAIN and (not IS_CI):\n","    \"TODO: YOUR CODE HERE\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","# Bonus\n","\n","Congratulations on finishing the day's content! No bonus section today.\n","\n","Tomorrow you'll have the same partner, so feel free to get started on W2D2. Or, play with your BERT and post your favorite completions in the Slack!\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"a393d4e541e00c4aa30fc32a8b3ef24cc2e7a308be35e595311f3139cfea23ef"}}},"nbformat":4,"nbformat_minor":4}
