{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W1D4 Part 1 - Optimization\n\nToday's material is divided into two parts. In the first part, you're going to learn more about the training loop and different optimizers. In the second part, you'll practice experimenting with different hyperparameters and learn how to do a distributed hyperparameter search.\n\nBy the end of the day, you'll have trained your ResNet from W1D2 using your own optimizer and hyperparameters!\n\n## Table of Contents\n\n- [Readings](#readings)\n- [Gradient Descent](#gradient-descent)\n- [Hyperparameters](#hyperparameters)\n- [Stochastic Gradient Descent](#stochastic-gradient-descent)\n- [Batch Size](#batch-size)\n- [Computing Gradients in PyTorch - W1D3 Review](#computing-gradients-in-pytorch---wd-review)\n    - [Stopping gradients with `torch.no_grad` or `torch.inference_mode`](#stopping-gradients-with-torchnograd-or-torchinferencemode)\n- [Common Themes in Gradient-Based Optimizers](#common-themes-in-gradient-based-optimizers)\n    - [Weight Decay](#weight-decay)\n    - [Momentum](#momentum)\n    - [Many More Optimizer Variants](#many-more-optimizer-variants)\n- [Exercise: Learning To Reproduce a Picture](#exercise-learning-to-reproduce-a-picture)\n    - [The \"device\" variable](#the-device-variable)\n- [Build Your Own TensorDataset](#build-your-own-tensordataset)\n    - [Slice Objects in Python](#slice-objects-in-python)\n- [Data Preprocessing](#data-preprocessing)\n    - [Train-Test Split](#train-test-split)\n    - [Visualizing the Training Data](#visualizing-the-training-data)\n- [DataLoaders](#dataloaders)\n- [Visualising Optimization With Rosenbrock's Banana](#visualising-optimization-with-rosenbrocks-banana)\n- [Build Your Own Optimizers](#build-your-own-optimizers)\n    - [Gotcha: In-Place Operations](#gotcha-in-place-operations)\n    - [More Tips](#more-tips)\n    - [SGD](#sgd)\n    - [RMSprop](#rmsprop)\n    - [Adam](#adam)\n- [Onward to Part 2](#onward-to-part-)\n- [Bonus](#bonus)\n\n## Readings\n\nNone for today!\n\n## Gradient Descent\n\nYesterday, you implemented backpropagation. Today, we're going to use the gradients produced by backpropagation for optimizing a loss function using gradient descent.\n\nA loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n\nWe actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\lambda$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n\n$$ \\theta_t \\leftarrow \\theta_{t-1} - \\lambda \\nabla L(\\theta_{t-1}) $$\n\nWe know that an infinitesimal step will decrease the loss, but a finite step will only do so if the loss function is linear enough in the neighbourhood of the current parameters. If the loss function is too curved, we might actually increase our loss.\n\nThe biggest advantage of this algorithm is that for N bytes of parameters, you only need N additional bytes of memory to store the gradients, which are of the same shape as the parameters. GPU memory is very limited, so this is an extremely relevant consideration. The amount of computation needed is also minimal: one multiply and one add per parameter.\n\nThe biggest disadvantage is that we're completely ignoring the curvature of the loss function, not captured by the gradient consisting of partial derivatives. Intuitively, we can take a larger step if the loss function is flat in some direction or a smaller step if it is very curved. Generally, you could represent this by some matrix P that pre-multiplies the gradients to rescale them to account for the curvature. P is called a preconditioner, and gradient descent is equivalent to approximating P by an identity matrix, which is a very bad approximation.\n\nMost competing optimizers can be interpreted as trying to do something more sensible for P, subject to the constraint that GPU memory is at a premium. In particular, constructing P explicitly is infeasible, since it's an $N \\times N$ matrix and N can be hundreds of billions. One idea is to use a diagonal P, which only requires N additional memory. An example of a more sophisticated scheme is [Shampoo](https://arxiv.org/pdf/1802.09568.pdf).\n\n<details>\n\n<summary>Why is it called Shampoo?</summary>\n\nYou put shampoo on your hair before using conditioner, and this method is a pre-conditioner. :D\n\n</details>\n\n## Hyperparameters\n\nThe learning rate is an example of a **hyperparameter**, which will be described below. As a reminder, a regular parameter is an adjustable value with the special and extremely convenient property that we can differentiate the loss with respect to the parameter, allowing us to efficiently learn good values for the parameter using gradient descent. In other words, the process of training is a function that takes a dataset, a model architecture, and a random seed and outputs model parameters.\n\nThe learning rate, in contrast, cannot be determined by this scheme. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters using a dataset of pairs of $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each iteration (a single pair) requires running the inner training loop.\n\nDue to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n\nIn addition, a hyperparameter isn't necessarily a single continuous value like the learning rate. Discrete unordered choices such as padding type as well as discrete ordered choices such as the number of layers in the network or the width of each convolution are all common. You will also need to choose between functions for optimizers, nonlinearities, or learning rate scheduling, of which there are an infinite number of possibilities, requiring us to select a small subset to test.\n\nMore broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1802.01548.pdf) demonstrated that it's possible to find architectures superior to human-designed ones.\n\nIn the second part of today's material, you will learn about various strategies for searching over hyperparameters.\n\n## Stochastic Gradient Descent\n\nThe terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n\n- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n\nThe class `torch.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course.\n\n## Batch Size\n\nIn addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n\nAt a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n\nFor a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n\nYou will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n\nPowers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n\nFinally, there are considerations around the alignment of data in memory and \"coalesced\" accesses to that data. If this sort of thing sounds fun to you, make sure to attend GPU day on W1D6. :)\n\n## Computing Gradients in PyTorch - W1D3 Review\n\nRecall from W1D3 that gradients are only saved for `Tensor`s for which `requires_grad=True`. For convenience, `nn.Parameter` automatically sets `requires_grad=True` on the wrapped `Tensor`. As you call `torch` functions, PyTorch tracks the relevant information needed in case you call `backward` later on, at which point it does the actual computation to compute the gradient and stores it in the `Tensor`'s `grad` field.\n\nAlso recall that PyTorch accumulates gradients across multiple `backward` calls. So if your tensor's `grad` already contains a value, after calling `backward` again it will have the sum of the original value and the new gradient. This behavior comes in handy in many situations, such as computing gradients over multiple runs on a GPU as part of a single batch. Suppose you choose a batch size of 32, but only 8 inputs fit on your GPU. A typical loss function for a batch computes the sum of losses over each example, so you can compute the losses 8 at a time and sum their gradients, producing the same result as running all 32 inputs at once.\n\n### Stopping gradients with `torch.no_grad` or `torch.inference_mode`\n\nYou may not want PyTorch to track gradients for some computations despite involving tensors with `requires_grad=True`. In this case, you can wrap the computation in the `with torch.inference_mode()` context to prevent this tracking. Example:\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "from typing import Iterable, Union, Optional\nimport matplotlib.pyplot as plt\nimport matplotlib.figure\nimport torch as t\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom PIL import Image\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nimport w1d4_part1_test\n\nMAIN = __name__ == \"__main__\"\nif MAIN:\n    x = t.ones(1, 2, 3, requires_grad=True)\n    y = x * x\n    with t.inference_mode():\n        z = x * x\n    print(f\"y requires grad: {y.requires_grad}; z requires grad: {z.requires_grad}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "Result: `y requires grad: True; z requires grad: False`\n\n\n## Common Themes in Gradient-Based Optimizers\n\n### Weight Decay\n\nWeight decay means that on each iteration, in addition to a regular step, we also shrink each parameter very slightly towards 0 by multiplying a scaling factor close to 1, e.g. 0.9999. Empirically, this seems to help but there are no proofs that apply to deep neural networks (if you know of one, let me know!).\n\nIn the case of linear regression, weight decay is mathematically equivalent to having a prior that each parameter is Gaussian distributed - in other words it's very unlikely that the true parameter values are very positive or very negative. This is an example of \"inductive bias\" - we make an assumption that helps us in the case where it's justified, and hurts us in the case where it's not justified.\n\nFor a `Linear` layer, it's common practice to apply weight decay only to the weight and not the bias. It's also common to not apply weight decay to the parameters of a batch normalization layer. Again, there is empirical evidence (such as [Jai et al 2018](https://arxiv.org/pdf/1807.11205.pdf)) and there are heuristic arguments to justify these choices, but no rigorous proofs.\n\n### Momentum\n\nMomentum means that the step includes a term proportional to a moving average of past gradients. [Distill.pub](https://distill.pub/2017/momentum/) has a great article on momentum.\n\n### Many More Optimizer Variants\n\n[Sebastian Ruder's blog](https://ruder.io/optimizing-gradient-descent/) goes into detail on many more variants of gradient descent.\n\n\n## Exercise: Learning To Reproduce a Picture\n\nIn this exercise you will train a neural network to memorize a picture of your choice! Your network will implement a function from the $(x, y)$ coordinates of a pixel to three numbers $(R, G, B)$ representing the color of that pixel. Implement the `ImageMemorizer` network with three Linear layers and two ReLUs (generally, you don't want a ReLU after the last Linear layer). Test that your model matches the reference.\n\n### The \"device\" variable\n\nA useful idiom when writing code to run on both CPU and GPU is to declare a `device` variable at the top of your notebook and then use it later on when creating or moving tensors. We've done this for you today.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\nif MAIN:\n    print(device)\n\n\nclass ImageMemorizer(nn.Module):\n    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        pass\n\n\nif MAIN:\n    w1d4_part1_test.test_mlp(ImageMemorizer)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nChoose a picture and save it on the filesystem, or use the provided image. If your chosen image is much larger than 1 million pixels, crop it with `img.crop((left, top, right, bottom))` and/or resize it with `img.resize((width, height))`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    fname = \"./w1d4_vangogh.jpg\"\n    img = Image.open(fname)\n    print(f\"Image size in pixels: {img.size[0]} x {img.size[1]} = {img.size[0] * img.size[1]}\")\n    plt.imshow(img)\n    pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Build Your Own TensorDataset\n\nThe class `torch.utils.data.dataset.TensorDataset` is a convenient wrapper for passing around multiple tensors that have the same size in the first dimension. The most common example of this is in supervised learning, where you have one tensor of inputs and a second tensor with corresponding labels. Often these tensors will have different `dtype`s, so it doesn't make sense to `torch.stack` them into one big tensor, and it be cumbersome to pass them around as separate variables or as a tuple.\n\n`TensorDataset` accepts and stores any number of tensors in the constructor along with implementing `__getitem__` so that `my_dataset[n]` returns a tuple containing element `n` from each stored `Tensor`. Similarly, `my_dataset[:5]` returns a tuple containing the first five elements from each stored `Tensor`.\n\n### Slice Objects in Python\n\n`slice` is a built-in type containing `start`, `stop`, and `step` fields which can be integers or `None`. Given `x=[1,2,3,4,5,6,7]`, writing `x[1:5:2]` is syntactic sugar for `x[slice(1, 5, 2)]`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class TensorDataset:\n    def __init__(self, *tensors: t.Tensor):\n        \"\"\"Validate the sizes and store the tensors in a field named `tensors`.\"\"\"\n        pass\n\n    def __getitem__(self, index: Union[int, slice]) -> tuple[t.Tensor, ...]:\n        \"\"\"Return a tuple of length len(self.tensors) with the index applied to each.\"\"\"\n        pass\n\n    def __len__(self):\n        \"\"\"Return the size in the first dimension, common to all the tensors.\"\"\"\n        pass\n\n\nif MAIN:\n    w1d4_part1_test.test_tensor_dataset(TensorDataset)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Data Preprocessing\n\nMost of the work in training a neural network is getting the data in top condition first. The relevant saying is \"garbage in, garbage out\".\n\nImplement `preprocess_image` to do the following:\n\n- Use `transforms.ToTensor()(img)` to obtain a tensor of shape `(channels, height, width)`.\n- Remove the fourth (alpha) channel if present and just use the first three channels which are R, G, B values.\n- Build a tensor of all combinations of `(x, y)` from `(0, 0)` up to `(height, width)`. Then, scale these coordinates down to the range `[-1, 1]`. These will be the inputs to your model. Without scaling them down, the training would either be very slow or not work at all.\n- Build a tensor of the corresponding RGB values and scale each color to the range `[-1, 1]`. These will be the labels.\n- Return the inputs and labels wrapped in a `TensorDataset`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def all_coordinates_scaled(height: int, width: int) -> t.Tensor:\n    \"\"\"Return a tensor of shape (height*width, 2) where each row is a (x, y) coordinate.\n\n    The range of x and y should be from [-1, 1] in both height and width dimensions.\n    \"\"\"\n    pass\n\n\ndef preprocess_image(img: Image.Image) -> TensorDataset:\n    \"\"\"Convert an image into a supervised learning problem predicting (R, G, B) given (x, y).\n\n    Return: TensorDataset wrapping input and label tensors.\n    input: shape (num_pixels, 2)\n    label: shape (num_pixels, 3)\n    \"\"\"\n    pass\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Train-Test Split\n\nNext, we will randomly split the data into 1. a training set that the model will use for computing gradients, 2. a validation set that will be used later for choosing hyperparameters, and 3. a held-out test set that will tell us how well the model is generalizing. For validation and test statistics to be a reliable measure of generalization, it is necessary for the training set to not overlap with the validation or test sets.\n\nThis was relatively straightforward in the era of small datasets that could be thoroughly inspected by humans, but is increasingly an issue as models are trained on massive piles of haphazardly cleaned Internet data. When reading ML papers, it's important to evaluate the potential for \"leakage\" between sets.\n\nYou'll see rules of thumb online about how much of your data to use for training/validation/test sets, such as a \"80%/10%/10% split\". In deep learning, these are generally wrong. The size of the validation and test sets only need to be big enough that sampling error doesn't introduce too much noise into the resulting estimate.\n\nFor example, ImageNet has around 1.3 million training images and only 50K validation images. The percentage (under 4%) is irrelevant and what matters is that 50K is large enough in absolute terms to achieve some standard error of the mean. Implement `train_test_split` below to split the dataset as described.\n\nHint: use [`torch.randperm`](https://pytorch.org/docs/stable/generated/torch.randperm.html).\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def train_test_split(all_data: TensorDataset, train_frac=0.8, val_frac=0.01, test_frac=0.01) -> list[TensorDataset]:\n    \"\"\"Return [train, val, test] datasets containing the specified fraction of examples.\n\n    If the fractions add up to less than 1, some of the data is not used.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    all_data = preprocess_image(img)\n    (train_data, val_data, test_data) = train_test_split(all_data)\n    print(f\"Dataset sizes: train {len(train_data)}, val {len(val_data)} test {len(test_data)}\")\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Visualizing the Training Data\n\nMany times, I've made errors in the preprocessing step and not noticed because my model still trains and learns anyway, just at a lower accuracy than was possible. One way to reduce the chance of this happening is to inspect the preprocessed data carefully to see if it still makes sense.\n\nMake a zero tensor of shape `(height, width, 3)` representing the grid of pixels, write your training data into it, and display it with `plt.imshow`.\n\nHint: there are two different ways you can write the data into your grid without using a `for` loop.\n\n<details>\n\n<summary>Spoiler - How to Write the Data</summary>\n\nThere's a special case in the indexing code that allows you to write `grid[y_coords, x_coords] = Y` where `x_coords` and `y_coords` are both of shape `(num_pixels,)`. This PyTorch behavior is the same as NumPy, and you can read about it in the [NumPy docs](https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing).\n\nAnother good way is to transform the height and width into a single `pixel` coordinate, `view` your grid as shape `(pixel, channels)`, and then do a regular indexing to write to the `view`.\n</details>\n\n<details>\n\n<summary>My grid looks mostly right, except for some black stripes of all zeros!</summary>\n\nDid you use `.long()` or `.to(torch.int64)` to make your coordinates integers? Due to floating point error, you might have something like `x=39.99` and `long()` would convert this to 39 while the intended result is 40. In this case you can just add 0.5 first so it rounds to the nearest integer.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def to_grid(X: t.Tensor, Y: t.Tensor, width: int, height: int) -> t.Tensor:\n    \"\"\"Convert preprocessed data from the format used in the Dataset back to an image tensor.\n\n    X: shape (n_pixels, dim=2)\n    Y: shape (n_pixels, channel=3)\n\n    Return: shape (height, width, channels=3)\n    \"\"\"\n    pass\n\n\nif MAIN:\n    (width, height) = img.size\n    (X, Y) = train_data.tensors\n    plt.figure()\n    plt.imshow(to_grid(X, Y, width, height))\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## DataLoaders\n\nToday, our `Dataset` is small enough to fit in memory, so we could just use `torch.randperm` on our training set to fetch random batches from it.\n\nIn general, we only want to load parts of our dataset as they're needed because our dataset may be too large to fit in memory, it may take too long to preprocess the entire dataset, or we may just want the GPU to be active as much as possible instead of waiting for data to be ready.\n\nThis is where `torch.DataLoader` comes in. A `DataLoader` instance is responsible for spawning multiple worker processes which load data in parallel and communicate back to the `DataLoader`. Ideally, the `DataLoader` can prepare the next batch while the GPU is processing the current one, eliminating GPU downtime.\n\nWe'll implement our own version of this another day when we're dealing with parallelism, and just use the PyTorch implementation today. We've provided DataLoaders with `shuffle=True` for the train loader. What would happen if you didn't shuffle the training data?\n\n<details>\n<summary>Answer - Shuffling Training Data</summary>\n\nIf our training data was sorted and we didn't shuffle it at least once, then the learning process could oscillate instead of converging. Suppose that the top half of the image was mostly blue sky and the bottom half was mostly green grass. The model would get gradients that first suggest \"everything is mostly blue\" and later \"everything is mostly green\" successively. In this case, we already used `randperm` above so our training data has been shuffled regardless.\n\nIn practice, SGD is relatively insensitive to whether you shuffle on every epoch, just once, or even sample each minibatch with replacement from the full dataset. For some theory behind this, see [this paper](https://arxiv.org/pdf/2106.06880.pdf).\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n    val_loader = DataLoader(val_data, batch_size=256)\n    test_loader = DataLoader(test_data, batch_size=256)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nImplement the `train_one_epoch` function below.\n\n- Use the `to()` method of a `Tensor` to send the data to the device indicated by the global variable `device`.\n- You can convert a one-element tensor to a regular Python number using the `item` method.\n\n<details>\n\n<summary>It's not working and I'm confused!</summary>\n\n- Did you remember to call `optimizer.zero_grad()` before each forward pass?\n- Does `model.parameters()` return what you expect?\n- Are you calling `backward()` on the mean loss over the batch items? Note that if you don't use the mean, the magnitude of the gradients scales up linearly with the batch size, which is not what you want.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def train_one_epoch(model: ImageMemorizer, dataloader: DataLoader) -> float:\n    \"\"\"Show each example in the dataloader to the model once.\n\n    Use `torch.optim.Adam` for the optimizer (you'll build your own Adam optimizer later today).\n    Use `F.l1_loss(prediction, actual)` for the loss function. This just puts less weight on very bright or dark pixels, which seems to produce nicer images.\n\n    Return: the average loss per example seen, i.e. sum of losses of each batch weighted by the size of the batch, divided by the total number of examples seen\n    \"\"\"\n    pass\n\n\nif MAIN:\n    w1d4_part1_test.test_train(train_one_epoch)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nWrite an evaluate function which calculates the average L1 loss on the provided dataloader.\n\nTip: since you don't need gradients for this step, wrap your forward call in the `with torch.inference_mode()` context to avoid unnecessary computation.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def evaluate(model: ImageMemorizer, dataloader: DataLoader) -> float:\n    \"\"\"Return the total L1 loss over the provided data divided by the number of examples.\"\"\"\n    pass\n\n\nif MAIN:\n    w1d4_part1_test.test_evaluate(evaluate)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nCreate a model with 400 neurons in each hidden layer and train it for an epoch.\n\nIf no errors appeared, do a few more epochs and plot the training loss and validation loss over time as a function of number of epochs. Compute the validation loss using your `evaluate` function. I was able to reach a validation loss below 0.2 after 40 epochs. Your image might be easier or harder to learn.\n\nSet up your model in this cell, initializing the `ImageMemorizer` class.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n1. Train your model in the cell below. Write it so that you can re-run this cell to continue training the model and appending to lists of train/val losses, instead of starting from scratch each time. Remember that you can use `tqdm` to view your progress across epochs.\n\n2. Write code to plot the L1 training loss and validation loss across epochs.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nFinally, in the cell below, write code to display the image your network has memorized by passing all possible `(x, y)` coordinates through the network and using `to_grid` from earlier to prepare the output for `imshow`. Note that your network may have produced outputs beyond the valid range: you can use `torch.clip` to truncate these to the proper `[0.0, 1.0]` range for `imshow`.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nShare your image in Slack if you like it! Here's the one my network learned:\n\n<p align=\"center\">\n    <img src=\"w1d4_vangogh_solution.jpg\" width=300/>\n</p>\n\n\n## Visualising Optimization With Rosenbrock's Banana\n\n\"Rosenbrock's Banana\" is a (relatively) famous function that has a simple equation but is challenging to optimize because of the shape of the loss landscape.\n\nImplement `plot_rosenbrock` using `plt.subplots`, [`plt.contourf`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html) and `plt.colorbar`. With the default rendering it's hard to see the minimum - if you plot the log of the function, the details will be more clear. Where is the minimum?\n\n<details>\n\n<summary>Solution</summary>\n\nThe first term is minimized when $x=a$ and the second term is minimized when $y = x^2 = a^2$. For $a=1$, it's $(1, 1)$. Looking at the plot, this seems reasonable.\n\n</details>\n\n<details>\n\n<summary>How do I generate the $x$ and $y$ values for my contour plot?</summary>\n\nFor each dimension, use `torch.linspace` to generate evenly spaced values, then `einops.repeat`.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n\n\ndef plot_rosenbrock(xmin=-2, xmax=2, ymin=-1, ymax=3, n_points=50, log_scale=False) -> matplotlib.figure.Figure:\n    \"\"\"Plot the rosenbrocks_banana function over the specified domain.\n\n    If log_scale is True, take the logarithm of the output before plotting.\n    \"\"\"\n    pass\n\n\nif MAIN:\n    plot_rosenbrock()\n    fig = plot_rosenbrock(log_scale=True)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n# Optimize The Banana\n\nImplement the `opt_banana` function using `torch.optim.SGD`. Starting from `(-1.5, 2.5)`, run your function and add the resulting trajectory of `(x, y)` pairs to your contour plot. Did it find the minimum? Play with the learning rate and momentum a bit and see how close you can get within 100 iterations.\n\n<details>\n\n<summary>I'm not sure if my `opt_banana` is implemented properly.</summary>\n\nWith a learning rate of `0.001` and momentum of `0.98`, my SGD was able to reach `[ 1.0234299 ,  1.198282 ]` after 100 iterations.\n\n</details>\n\n<details>\n\n<summary>I'm getting \"Can't call numpy() on Tensor that requires grad\" and I don't know why!</summary>\n\nThis is a protective mechanism built into PyTorch. The idea is that once you convert your `Tensor` to NumPy, PyTorch can no longer track gradients, but you might not understand this and expect backprop to work on NumPy arrays.\n\nAll you need to do to convince PyTorch you're a responsible adult is to call `detach()` on the tensor first, which returns a view that does not require grad and isn't part of the computation graph.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def opt_banana(xy: t.Tensor, n_iters: int, lr=0.001, momentum=0.98):\n    \"\"\"Optimize the banana starting from the specified point.\n\n    xy: shape (2,). The (x, y) starting point.\n    n_iters: number of steps.\n\n    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n    \"\"\"\n    assert xy.requires_grad\n    pass\n\n\nif MAIN:\n    xy = t.tensor([-1.5, 2.5], requires_grad=True)\n    xys = opt_banana(xy, n_iters=100).numpy()\n    fig = plot_rosenbrock(log_scale=True)\n    fig.axes[0].plot(xys[:, 0], xys[:, 1], color=\"r\", linewidth=1)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Build Your Own Optimizers\n\nNow let's build our own drop-in replacement for these three classes from `torch.optim`. The documentation pages for these algorithms have pseudocode you can use to implement your step method.\n\n### Gotcha: In-Place Operations\n\nBe careful with expressions like `x = x + y` and `x += y`. They are NOT equivalent in Python.\n\n- The first one allocates a new `Tensor` of the appropriate size and adds `x` and `y` to it, then rebinds `x` to point to the new variable. The original `x` is not modified.\n- The second one modifies the storage referred to by `x` to contain the sum of `x` and `y` - it is an \"in-place\" operation.\n    - Another way to write the in-place operation is `x.add_(y)` (the trailing underscore indicates an in-place operation).\n    - A third way to write the in-place operation is `torch.add(x, y, out=x)`.\n- This is rather subtle, so make sure you are clear on the difference. This isn't specific to PyTorch; the built-in Python `list` follows similar behavior: `x = x + y` allocates a new list, while `x += y` is equivalent to `x.extend(y)`.\n- In general, the first version calls the method `x.__add__(y)` while the second calls `x.__iadd__(y)`, and these two methods can have arbitrary semantics.\n\nThe tricky thing that happens here is that both the optimizer and the `Module` in your model have a reference to the same `Parameter` instance. Do we want to use in-place operations in our optimizer?\n\n<details>\n\n<summary>Solution - In-place Operations</summary>\n\nYou MUST use in-place operations in your optimizer because we want the model to see the change to the Parameter's storage on the next forward pass. If your optimizer allocates a new tensor, the model won't know anything about the new tensor and will continue to use the old, unmodified version.\n\n</details>\n\n### More Tips\n\n- The provided `params` might be a generator, in which case you can only iterate over it once before the generator is exhausted. Copy it into a `list` to be able to iterate over it repeatedly.\n- Your step function shouldn't modify the gradients. Use the `with torch.inference_mode():` context for this. Fun fact: you can instead use `@torch.inference_mode()` (note the preceding `@`) as a method decorator to do the same thing.\n- If you create any new tensors, they should be on the same device as the corresponding parameter. Use `torch.zeros_like()` or similar for this.\n- Be careful not to mix up `Parameter` and `Tensor` types in this step.\n- The actual PyTorch implementations have an additional feature called parameter groups where you can specify different hyperparameters for each group of parameters. You can ignore this for today.\n\n### SGD\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class SGD:\n    def __init__(self, params: Iterable[t.nn.parameter.Parameter], lr: float, momentum: float, weight_decay: float):\n        \"\"\"Implements SGD with momentum.\n\n        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n\n        \"\"\"\n        pass\n\n    def zero_grad(self) -> None:\n        pass\n\n    def step(self) -> None:\n        pass\n\n\nif MAIN:\n    w1d4_part1_test.test_sgd(SGD)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### RMSprop\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class RMSprop:\n    def __init__(\n        self,\n        params: Iterable[t.nn.parameter.Parameter],\n        lr: float,\n        alpha: float,\n        eps: float,\n        weight_decay: float,\n        momentum: float,\n    ):\n        \"\"\"Implements RMSprop.\n\n        Like the PyTorch version, but assumes centered=False\n            https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop\n\n        \"\"\"\n        pass\n\n    def zero_grad(self) -> None:\n        pass\n\n    def step(self) -> None:\n        pass\n\n\nif MAIN:\n    w1d4_part1_test.test_rmsprop(RMSprop)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Adam\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class Adam:\n    def __init__(\n        self,\n        params: Iterable[t.nn.parameter.Parameter],\n        lr: float = 0.001,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-08,\n        weight_decay: float = 0.0,\n    ):\n        \"\"\"Implements Adam.\n\n        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n        \"\"\"\n        pass\n\n    def zero_grad(self) -> None:\n        pass\n\n    def step(self) -> None:\n        pass\n\n\nif MAIN:\n    w1d4_part1_test.test_adam(Adam)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Onward to Part 2\n\nProceed to Part 2 where we will practice some hyperparameter searches. If you have time at the end of the day, come back and do the bonus section below.\n\n\n## Bonus\n\nImplement the matrix form of [Shampoo](https://arxiv.org/pdf/1802.09568.pdf) following Algorithm 1 in the paper.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}