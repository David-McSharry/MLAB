{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W1D4 Part 2 - Hyperparameter Search\n\nIn this section we'll set up an experiment logging service, and spend the remainder of the day running experiments using your ResNet and Adam implementations on CIFAR10. Try various ideas to bring up your accuracy and bring down your training time.\n\nDon't peek at what other people online have done for CIFAR10 (it's a common benchmark), because the point is to develop your own process by which you can figure out how to improve your model. Just reading the results of someone else would prevent you from learning how to get the answers. To get an idea of what's possible: using one V100 and a modified ResNet, one entry in the DAWNBench competition was able to achieve 94% test accuracy in 24 epochs and 76 seconds. 94% is approximately [human level performance](http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/).\n\n## Table of Contents\n\n- [Weights and Biases Setup](#weights-and-biases-setup)\n- [Grid Search and Random Search](#grid-search-and-random-search)\n- [Modeling Hyperparameter Response](#modeling-hyperparameter-response)\n- [Adaptive Search](#adaptive-search)\n- [Some Experiments to Try](#some-experiments-to-try)\n- [Using Weights and Biases for Hyperparameter Search](#using-weights-and-biases-for-hyperparameter-search)\n- [While Your Search Runs](#while-your-search-runs)\n- [The Optimizer's Curse](#the-optimizers-curse)\n- [Bonus](#bonus)\n\n## Weights and Biases Setup\n\nWeights and Biases is a cloud service that allows you to log data from experiments. You can upload your model files to it, and it keeps track of what Git commit was used so you can match the model file to the code. Your logged data is shown in graphs during training, and you can easily compare logs across different runs.\n\nNothing about this is hard technically or conceptually, so we won't bother building our own and will just use this one throughout the remainder of the course to keep things organized.\n\nTo integrate with the service, copy the below code into a script, and modify the imports to use your own ResNet and Adam implementations from previously. Run the script from the command line and follow the instructions to register a personal account on the website and log in. Once the model is training, verify that you can see data appearing on the website.\n\nNote that this looks basically identical to our training loop from W1D2, with a few calls to the `wandb` library sprinkled in. We call `wandb.init` passing in our dictionary of config options, which will be converted to an object and logged to the server. Then we do regular training stuff, calling `wandb.log` to save metrics and finally `wandb.save` to upload our trained model to the server.\n\n## Grid Search and Random Search\n\nOne way to do hyperparameter search is to choose a set of values for each hyperparameter, and then search all combinations of those specific values. This is called **grid search**. The values don't need to be evenly spaced and you can incorporate any knowledge you have about plausible values from similar problems to choose the set of values. Searching the product of sets takes exponential time, so is really only feasible if there are a small number of hyperparameters. I would recommend forgetting about grid search if you have more than 3 hyperparameters, which in deep learning is \"always\".\n\nA much better idea is for each hyperparameter, decide on a sampling distribution and then on each trial just sample a random value from that distribution. This is called **random search** and back in 2012, you could get a [publication](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) for this. The diagram below shows the main reason that random search outperforms grid search. Empirically, some hyperparameters matter more than others, and random search benefits from having tried more distinct values in the important dimensions, increasing the chances of finding a \"peak\" between the grid points.\n\n<p align=\"center\">\n    <img src=\"w1d4_grid_vs_random.png\"/>\n</p>\n\nIt's worth noting that both of these searches are vastly less efficient than gradient descent at finding optima - imagine if you could only train neural networks by randomly initializing them and checking the loss! Either of these search methods without a dose of human (or eventually AI) judgement is just a great way to turn electricity into a bunch of models that don't perform very well.\n\n## Modeling Hyperparameter Response\n\nIn grid and random search, the machine just collects data and the human is responsible for detecting patterns and generalizing to unseen hyperparameter values. But the whole point of ML is to get the machine to detect patterns and generalize for us, so we might hope to train a hyper-model to predict validation set loss as a function of hyperparameter values. The main challenges are:\n\n- Now your hyper-model has hyper-hyperparameters to tune.\n- Validation set loss is a noisy metric because of the randomness in initialization and training.\n- It's computationally expensive to gather hyper-training data because each (hyperparameter, loss) data point requires training the base model.\n- In deep learning, there are so many hyperparameters that it's infeasible to test even two values for each one.\n\nWe won't be trying this today, but it's a possible approach.\n\n## Adaptive Search\n\nInstead of just collecting random samples, it would be nice to be able to use the results of early samples to inform which hyperparameter combinations to try next. This is possible if and only if there's some nice structure present in the problem; otherwise random search is actually just optimal and we shouldn't waste time trying to be smart about it.\n\nThe most basic sort of structure that we could hope for is that the validation loss is smooth for small perturbations of the hyperparameters: for example if a learning rate of 0.020 is good, then we assume that 0.019 and 0.021 should be not too different.\n\nThis is enough to be able to trade off choosing to sample far away from previous tests (exploration) with taking samples close to previous good outputs (exploitation). [Bayesian Optimization](https://scikit-optimize.github.io/stable/) is a way to do this. Another way to use this smoothness assumption is [LIPO](http://blog.dlib.net/2017/12/a-global-optimization-algorithm-worth.html), which explicitly maintains a bound on the Lipschitz constant of the function. The authors claim that LIPO is provably at least as good as random search, which is something.\n\nStart with random search, but feel free to play with some of these methods later in the day and compare how well they work. In practice, I've usually found exciting-sounding methods to not work as well as I'd hope.\n\n## Some Experiments to Try\n\n- First, try to reduce training time.\n    - Starting with a smaller ResNet is a good idea. Good hyperparameters on the small model tend to transfer over to the larger model because the architecture and the data are the same; the main difference is the larger model may require more regularization to prevent overfitting.\n    - Bad hyperparameters are usually clearly worse by the end of the first 1-2 epochs. If you can train for fewer epochs, you can test more hyperparameters with the same compute. You can manually abort runs that don't look promising, or you can try to do it automatically; [Hyperband](https://www.jmlr.org/papers/volume18/16-558/16-558.pdf) is a popular algorithm for this.\n    - Play with optimizations like [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html) to see if you get a speed boost.\n- Random search for a decent learning rate and batch size combination that allows your model to mostly memorize (overfit) the training set.\n    - It's better to overfit at the start than underfit, because it means your model is capable of learning and has enough capacity.\n    - Learning rate is often the most important single hyperparameter, so it's important to get a good-enough value early.\n    - Eventually, you'll want a learning rate schedule. Usually, you'll start low and gradually increase, then gradually decrease but many other schedules are feasible. [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/) has a good blog post on learning rates.\n    - Larger batch size increases GPU memory usage and doubling batch size [often allows doubling learning rate](https://arxiv.org/pdf/1706.02677.pdf), up to a point where this relationship breaks down. The heuristic is that larger batches give a more accurate estimate of the direction to update in. Note that on the test set, you can vary the batch size independently and usually the largest value that will fit on your GPU will be the most efficient.\n- Add regularization to reduce the amount of overfitting and train for longer to see if it's enough.\n    - Data augmention is the first thing to do - flipping the image horizontally and Cutout are known to be effective.\n    - Play with the label smoothing parameter to [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n    - Try adding weight decay to Adam. This is a bit tricky - see this [fast.ai](https://www.fast.ai/2018/07/02/adam-weight-decay/) article if you want to do this, as well as the [PyTorch pseudocode](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html).\n- Try a bit of architecture search: play with various numbers of blocks and block groups. Or pick some fancy newfangled nonlinearity and see if it works better than ReLU.\n- Random search in the vicinity of your current hyperparameters to try and find something better.\n\n## Using Weights and Biases for Hyperparameter Search\n\nHave a look at the file `w1d4_sweep.yaml`. This contains the configuration for our sweep in the [Weights and Biases sweep configuration](https://docs.wandb.ai/guides/sweeps/configuration) format. We've chosen a log-uniform sampling distribution, meaning that the logarithm of the sampled values has a uniform distribution. This is appropriate when the values vary over multiple orders of magnitude. In addition, we're quantizing the batch size to a multiple of 8 in the hopes that this is slightly more efficient on the GPU. Change the 'program' argument to the name of your script file.\n\n1) Run `wandb sweep .\\w1d4_sweep.yaml`, which should print out a sweep ID and a URL where you can view the results. If you like, find a neighbouring pair and join their sweep instead so you can pool your results.\n2) Run `wandb agent SWEEP_ID`, passing the sweep ID from step 1.\n\nAt the command line, you can run `nvidia-smi -l 1` to see your GPU memory usage and % utilization. Our model is pretty small, so GPU memory permitting you can launch multiple copies of `wandb agent` and they can run concurrently. If you happen to have other devices with GPUs available, they can launch `wandb agent` as well. Training on CPU only is likely to be too slow and not worth it.\n\n## While Your Search Runs\n\nAn important part of a ML workflow is finding something useful to do while you're waiting for experiments to run. Some good ideas are to get started coding the next experiment, or reading relevant resources online.\n\n## The Optimizer's Curse\n\nThe [optimizer's curse](https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it) applies to tuning hyperparameters. The main take-aways are:\n\n- You can expect your best hyperparameter combination to actually underperform in the future. You chose it because it was the best on some metric, but that metric has an element of noise/luck, and the more combinations you test the larger this effect is.\n- Look at the overall trends and correlations in context and try to make sense of the values you're seeing. Just because you ran a long search process doesn't mean your best output is really the best.\n\nFor more on this, see [Preventing \"Overfitting\" of Cross-Validation Data](https://ai.stanford.edu/~ang/papers/cv-final.pdf) by Andrew Ng.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import argparse\nimport os\nimport time\nfrom typing import Any\nimport torch as t\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport wandb\nfrom w1d2_solution import IS_CI, ResNet34, get_cifar10\nfrom w1d4_part1_solution import Adam\n\nMAIN = __name__ == \"__main__\"\nIS_CI = os.getenv(\"IS_CI\")\ndevice = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n\n\ndef train(config_dict: dict[str, Any]):\n    wandb.init(project=\"w1d4\", config=config_dict)\n    config = wandb.config\n    print(f\"Training with config: {config}\")\n    (cifar_train, cifar_test) = get_cifar10()\n    trainloader = DataLoader(cifar_train, batch_size=config.batch_size, shuffle=True, pin_memory=True)\n    testloader = DataLoader(cifar_test, batch_size=1024, pin_memory=True)\n    model = ResNet34(n_blocks_per_group=[1, 1, 1, 1], n_classes=10).to(device).train()\n    optimizer = Adam(\n        model.parameters(), lr=config.lr, betas=(config.beta_0, config.beta_1), weight_decay=config.weight_decay\n    )\n    train_loss_fn = t.nn.CrossEntropyLoss()\n    wandb.watch(model, criterion=train_loss_fn, log=\"all\", log_freq=10, log_graph=True)\n    start_time = time.time()\n    examples_seen = 0\n    for epoch in range(config.epochs):\n        for (i, (x, y)) in enumerate(tqdm(trainloader)):\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            y_hat = model(x)\n            loss = train_loss_fn(y_hat, y)\n            acc = (y_hat.argmax(dim=-1) == y).sum() / len(x)\n            loss.backward()\n            optimizer.step()\n            wandb.log(dict(train_loss=loss, train_accuracy=acc, elapsed=time.time() - start_time), step=examples_seen)\n            examples_seen += len(x)\n    test_loss_fn = t.nn.CrossEntropyLoss(reduction=\"sum\")\n    with t.inference_mode():\n        n_correct = 0\n        n_total = 0\n        loss_total = 0.0\n        for (i, (x, y)) in enumerate(tqdm(testloader)):\n            x = x.to(device)\n            y = y.to(device)\n            y_hat = model(x)\n            loss_total += test_loss_fn(y_hat, y).item()\n            n_correct += (y_hat.argmax(dim=-1) == y).sum().item()\n            n_total += len(x)\n        wandb.log(dict(test_loss=loss_total / n_total, test_accuracy=n_correct / n_total, step=examples_seen))\n    filename = f\"{wandb.run.dir}/model_state_dict.pt\"\n    t.save(model.state_dict(), filename)\n    wandb.save(filename)\n\n\nif MAIN and (not IS_CI):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--lr\", type=float, default=0.001)\n    parser.add_argument(\"--batch_size\", type=int, default=1024)\n    parser.add_argument(\"--weight_decay\", type=float, default=0)\n    parser.add_argument(\"--epochs\", type=int, default=1)\n    parser.add_argument(\"--beta_0\", type=float, default=0.9)\n    parser.add_argument(\"--beta_1\", type=float, default=0.999)\n    parser.add_argument(\"--cuda_memory_fraction\", type=float, default=0.5)\n    args = parser.parse_args()\n    config_dict = vars(args)\n    if t.cuda.is_available():\n        t.cuda.set_per_process_memory_fraction(config_dict.pop(\"cuda_memory_fraction\"))\n    train(config_dict)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Bonus\n\nThere's a saying: \"Art is never finished, only abandoned\". Hyperparameter tuning is also never finished, only abandoned. Play around until you're bored or run out of compute, and go back and do the Part 1 bonus if you like.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}