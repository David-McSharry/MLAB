{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W3D4 part 2 -  The DDPM Architecture\n\nThe DDPM paper uses a custom architecture which is based on the PixelCNN++ paper with some modifications, which is in turn based on a couple other papers (U-Net and Wide ResNet) which are in turn modified versions of other papers. For today, just follow along with the diagrams and we won't worry if we exactly match the paper as long as it works.\n\n## Table of Contents\n\n- [The U-Net](#the-u-net)\n- [The DDPM Model](#the-ddpm-model)\n- [Downsampling Block](#downsampling-block)\n- [The MidBlock](#the-midblock)\n- [The UpBlock](#the-upblock)\n- [Residual Block](#residual-block)\n- [Attention Block](#attention-block)\n- [Group Normalization](#group-normalization)\n- [Sinusoidal Positional Embeddings](#sinusoidal-positional-embeddings)\n- [Sigmoid Linear Unit](#sigmoid-linear-unit)\n- [Self-Attention with Two Spatial Dimensions](#self-attention-with-two-spatial-dimensions)\n- [Transposed Conv2d](#transposed-convd)\n- [Assembling the UNet](#assembling-the-unet)\n\n## The U-Net\n\nAt the high level, the shape of this network resembles the U-Net architecture, pictured below. Like ResNet, U-Net is mostly composed of convolutional layers, but whereas ResNets were developed for classifying an image as a whole, U-Net was developed for medical segmentation tasks where the goal is to predict an output class such as \"this is part of a tumour\" for each input pixel in the image. This means that the network has to both get an understanding of the global structure of the image like ResNet does, but also have the ability to make fine-grained predictions at the pixel level.\n\n<p align=\"center\">\n    <img src=\"w3d4_unet_architecture.svg\"/>\n</p>\n\nIn the diagram, the grey rectangles represent tensors with the height of the rectangle being the height (and width) of the image tensor and the width of the rectangle being the number of channels.\n\nThe network is conceptually divided into three parts: the downsampling part, the middle part, and the upsampling part. In the downsampling part starting on the left of the diagram, we do some convolutions and then the yellow downsampling operation halves the width and height. The number of channels increases throughout the downsampling part, but since the spatial dimensions are shrinking, the compute per layer stays similar.\n\nThe middle section is just a couple more convolutions, and then we go into the upsampling part. In these layers, we actually double the spatial dimensions using a transposed convolution. A transposed convolution is also called a deconvolution, but deconvolution can also refer to something different so it's less ambiguous to use the name \"transposed convolution\".\n\nLike a regular convolution, a transposed convolution uses a sliding kernel, but the difference is that the output is larger than the input. Fun fact: a transposed convolution is closely related to the backwards pass of a regular convolution.\n\n<p align=\"center\">\n    <img src=\"w3d4_unet_trans_conv.png\"/>\n</p>\n\nAt the end, a final convolution takes us down to the desired number of output channels. In the medical segmentation case, this might be one channel for each class of tumour that you want to detect. In our case, we're going to have three output channels to predict a RGB image.\n\n## The DDPM Model\n\nThe model used in the DDPM is shown below and has the same three part structure as the U-Net: at first the spatial dimensions half and the channels double, and then the spatial dimensions double and channels are concatenated. It's common to still call this a U-Net and name the class `Unet` because it has this basic shape, even though the majority of components have been modified from the original U-Net.\n\nWe've got some 2D self-attention in there, new nonlinearities, group normalization, and sinusoidal position embeddings. We'll implement these from scratch so you understand what they are. Once you've done that, assembling the network will be routine work for you at this point.\n\nOne complication is that in addition to taking a batch of images, for each image we also have a single integer representing the number of steps of noise added. In the paper, this ranges from 0 to 1000, so the range is too wide to directly pass this as an integer. Instead, these get embedded into a tensor of shape `(batch, emb)` where `emb` is some embedding dimension and passed into the blocks.\n\nThis is going to be the most complicated architecture you've done so far, but the good news is that if you don't do it exactly right, it'll probably still work fine.\n\n```mermaid\ngraph TD\n    subgraph DDPM Architecture\n        subgraph Overview\n            MTime[Num Noise Steps] --> MTimeLayer[SinusoidalEmbedding<br/>Linear: Steps -> 4C</br>GELU<br/>Linear: 4C -> 4C]\n            MTimeLayer -->|emb|DownBlock0 & DownBlock1 & DownBlock2 & MidBlock & UpBlock0 & UpBlock1 & OutBlock\n            Image -->|3, H| InConv[7x7 Conv<br/>Padding 3] -->|C, H| DownBlock0 -->|C, H/2| DownBlock1 -->|2C,H/4| DownBlock2 -->|4C,H/4| MidBlock -->|4C,H/4| UpBlock0 -->|2C,H/2| UpBlock1 -->|C,H| OutBlock[Residual Block] -->|C,H| FinalConv[1x1 Conv] -->|3,H| Output\n            DownBlock2 -->|4C,H/4| UpBlock0\n            DownBlock1 -->|2C,H/2| UpBlock1\n        end\nend\n```\n\n\n## Downsampling Block\n\nThis block takes some input height `h` and returns two things: a skip output of height `h` that connects to a later `UpBlock`, and a downsampled output of height `h//2`. We are going to assume (and it's good practice to assert inside the code) that h is always going to be divisible by 2.\n\n```mermaid\ngraph TD\n    subgraph DownBlock\n        NumSteps -->|emb| DResnetBlock1 & DResnetBlock2\n        DImage[Input] -->|c_in, h| DResnetBlock1[Residual Block 1] -->|c_out, h| DResnetBlock2[Residual Block 2] -->|c_out, h| DAttention[Attention Block] -->|c_out, h| DConv2d[4x4 Conv<br/>Stride 2<br/>Padding 1] -->|c_out, h/2| Output\n        DAttention -->|c_out, h| SkipToUpBlock[Skip To<br/>UpBlock]\n    end\n```\n\n## The MidBlock\n\nAfter the DownBlocks, the image is passed through a MidBlock which doesn't change the number of channels.\n\n```mermaid\ngraph TD\n    subgraph MidBlock\n        UNumSteps[NumSteps] -->|emb| UResnetBlock1 & UResnetBlock2\n        UImage[Image] -->|c_mid, h| UResnetBlock1[Residual Block 1] -->|c_mid, h| UAttention[Attention Block] -->|c_mid, h| UResnetBlock2[Residual Block 2] -->|c_mid, h| UOutput[Output]\n    end\n```\n\n\n## The UpBlock\n\nNote here that the first upsampling block takes a skip connection from the last downsampling block, and the second upsampling block takes a skip connection from the second last downsampling block. In your implementation, pushing and popping a stack is a clean way to handle this. The indicated sizes `c_in` and `c_out` are with respect to the source downsampling block, which is confusing but so is notating it the other way.\n\n```mermaid\ngraph TD\n    subgraph UpBlock\n        UNumSteps[NumSteps] -->|emb| UResnetBlock1 & UResnetBlock2\n        Skip[Skip From<br/>DownBlock<br/>] -->|c_out, h| Concatenate\n        UImage[Image] -->|c_out, h| Concatenate -->|2*c_out, h| UResnetBlock1[Residual Block 1] -->|c_in, h| UResnetBlock2[Residual Block 2] -->|c_in, h| UAttention[Attention Block] -->|c_in, h| DConvTranspose2d[4x4 Transposed Conv<br/>Stride 2<br/>Padding 1] -->|c_in, 2h| UOutput[Output]\n    end\n```\n\n\n## Residual Block\n\nThese are called residual blocks because they're derived from but not identical to the ResNet blocks. You can see the resemblance with a main branch and a residual branch. When the input dimensions don't match the output dimensions, the residual branch uses a 1x1 convolution to keep them consistent.\n\n```mermaid\ngraph TD\nsubgraph ResidualBlock\nImage -->|c_in, h| ResConv[OPTIONAL<br/>Conv 1x1] -->|c_out, h| Out\n        Image -->|c_in, h| Conv1[Conv 3x3, pad 1<br/>GroupNorm<br/>SiLU] -->|c_out, h| AddTimeEmbed[Add] -->|c_out, h| Conv2[Conv 3x3, pad 1<br/>Group Norm</br>SiLU] -->|c_out, h| Out\n        NumSteps[Num Steps<br/>Embedding] -->|emb| TimeLayer[SiLU<br/>Linear] -->|c_out| AddTimeEmbed\n        end\n```\n\n## Attention Block\n\n```mermaid\ngraph TD\n    subgraph AttentionBlock\n        Image --> GroupNorm[Group Norm<br/>1 group] --> Self-Attention[Self-Attention<br/>4 heads] --> Output\n        Image --> Output\n    end\n```\nLet's get started with the new low-level components, and then we'll build bottom-up to the full architecture.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "from typing import Optional, Union\nimport matplotlib.pyplot as plt\nimport torch as t\nfrom einops import rearrange, repeat\nfrom fancy_einsum import einsum\nfrom torch import nn\nimport w3d4_test\nfrom w3d4_part1_diffusion_training_solution import DiffusionModel\n\nMAIN = __name__ == \"__main__\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Group Normalization\n\nIn Layer Normalization, we computed a mean and standard deviation for each training example, across all channels (channels are the same as embedding dimensions in a transformer).\n\nGroup Normalization means we divide our channels into some number of groups, and we have a mean and standard devication for each training example AND group. When the number of groups is 1, GroupNorm can be expressed as a LayerNorm. The main difference is that GroupNorm expects the channel dimension right after the batch dimension, as is conventional in PyTorch for image data. LayerNorm expects the channel (embedding) dimension to be last, as is conventional in PyTorch for NLP.\n\n<p align=\"center\">\n    <img src=\"w3d4_unet_groupnorm.png\"/>\n</p>\n\nThe pixels in blue are normalized by the same mean and standard deviation. For group norm, two groups are depicted.\n\nFor more intuition behind why this could be a good alternative to other normalization schemes, see the [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf) paper.\n\nImplement `GroupNorm` so it behaves identically to `torch.nn.GroupNorm` given a `(batch, channels, height, width)` tensor. While `torch.nn.GroupNorm` supports more than 2 spatial dimensions, you don't need to worry about this.\n\n<details>\n\n<summary>Help! I'm confused about the forward pass!</summary>\n\nUse `rearrange` to introduce a 5th group dimension and then compute the mean and variance over the appropriate dimensions. After you subtract and divide the mean and variance, `rearrange` again back into BCHW before applying the learnable parameters.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class GroupNorm(nn.Module):\n    def __init__(\n        self,\n        num_groups: int,\n        num_channels: int,\n        eps: float = 1e-05,\n        affine: bool = True,\n        device: Optional[Union[t.device, str]] = None,\n        dtype: Optional[t.dtype] = None,\n    ) -> None:\n        pass\n\n    def reset_parameters(self) -> None:\n        \"\"\"Initialize the weight and bias, if applicable.\"\"\"\n        if self.affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"Apply normalization to each group of channels.\n\n        x: shape (batch, channels, height, width)\n        out: shape (batch, channels, height, width)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w3d4_test.test_groupnorm(GroupNorm, affine=False)\n    w3d4_test.test_groupnorm(GroupNorm, affine=True)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Sinusoidal Positional Embeddings\n\nIn GPT and BERT, the mapping from the integer position to embedding vector was learned by the network during training. It's also common practice to just hardcode this mapping using a combination of sine and cosine functions with different frequencies. The argument for doing this is that it slightly reduces the number of parameters in the network, and it seems to work just as well.\n\nIn our network, the equivalent of position index is the number of noise steps added. The network needs this information to \"know\" how much noise to expect, because the amount of noise is increasing with the number of steps.\n\nFor more intuition about why we use sine and cosine functions, see [this blog post](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/). The short version is that it allows a simple linear transformation in the query and key matrices to express \"this head attends to information k positions ago\".\n\nImplement the positional embedding, then reproduce Figure 2 and Figure 3 from the above blog post. Note that the equations in the blog post are confusing (in particular, it doesn't define what \"k\" is). I strongly suggest using the equations under the \"Positional Encoding\" section of [this other blog post](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3) instead.\n\nExercise: does the order of the dimensions (channels) in the positional embedding matter?\n\n<details>\n\n<summary>Solution - Order of Dimensions</summary>\n\nIf we wanted to load pretrained weights, we would need to match the order that the model was trained with. Training from scratch like today, most operations compute channel-wise and then sum, so if we permuted the order of channels and also permuted the weights, the output would be identical.\n\nYou've just learned about group normalization which does in fact group adjacent channels together, so it would matter if we were applying group normalization to the embedding. But we don't have to worry about this today and you can make your dimensions in any order you like.\n\n</details>\n\n\n<details>\n\n<summary>Help! I'm confused about the implementation!</summary>\n\nThere are `embedding_size//2` different frequencies $w_k$ that can be pre-computed in the constructor because they only depend on the embedding size. Register these as a buffer so that they'll be moved to the appropriate device automatically.\n\nIn forward, form the outer product of x and $w_k$, and then call sin and cos to produce the two halves of the output. To interleave the frequencies like in the article, you can `stack` and then `rearrange`, but it's equally valid to just concatenate them and have all the sin terms followed by all the cos terms.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, embedding_size: int):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, ) - for each batch element, the number of noise steps\n        Out: shape (batch, embedding_size)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Sigmoid Linear Unit\n\nThe Sigmoid Linear Unit (SiLU) nonlinearity is just elementwise `x * sigmoid(x)`. Confusingly, this function is also called Swish in the literature - these two names refer to exactly the same thing. Implement the function and plot it on the interval [-5, 5]. Like every other new non-linearity published, its authors claim that it has superior performance on benchmarks, but we don't fully understand why.\n\nFor more on this activation function, see [Swish: A Self-Gated Activation Function](https://arxiv.org/pdf/1710.05941v1.pdf).\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def swish(x: t.Tensor) -> t.Tensor:\n    pass\n\n\nclass SiLU(nn.Module):\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        return swish(x)\n\n\nif MAIN:\n    \"TODO: YOUR CODE HERE\"\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Self-Attention with Two Spatial Dimensions\n\nIn the transformer, we had one spatial (sequence) dimension, but now we have image data with both height and width (which we're assuming to be equal). Implement the code for this - feel free to refer to your previous self-attention implementation.\n\n<details>\n\n<summary>Help! I'm confused about how to handle both spatial dimensions!</summary>\n\nThe most straightforward way to adapt a previous implementation is start by using `rearrange` to merge height and width into one sequence dimension, and then finish by using `rearrange` again to split the sequence dimension back out.\n\nAnother way is to use 1x1 `Conv2d` instead of `Linear` layers, since these will automatically operate two spatial dimensions.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class SelfAttention(nn.Module):\n    def __init__(self, channels: int, num_heads: int = 4):\n        \"\"\"Self-Attention with two spatial dimensions.\n\n        channels: the number of channels. Should be divisible by the number of heads.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, channels, height, width)\n        out: shape (batch, channels, height, width)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w3d4_test.test_self_attention(SelfAttention)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Transposed Conv2d\n\nYou've already implemented the regular Conv2d, and the transposed version is along the same lines. We'll leave this to the bonus section in the interest of saving time. If you feel confused about what this operation does, skimming through Chapter 4 of [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285v1.pdf) and looking at the pictures should help to clarify.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "ConvTranspose2d = nn.ConvTranspose2d\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Assembling the UNet\n\nImplement the various blocks according to the diagram.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class AttentionBlock(nn.Module):\n    def __init__(self, channels: int):\n        pass\n\n    def forward(self, x: t.Tensor) -> t.Tensor:\n        pass\n\n\nif MAIN:\n    w3d4_test.test_attention_block(SelfAttention)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_channels: int, output_channels: int, step_dim: int, groups: int):\n        \"\"\"\n        input_channels: number of channels in the input to foward\n        output_channels: number of channels in the returned output\n        step_dim: embedding dimension size for the number of steps\n        groups: number of groups in the GroupNorms\n\n        Note that the conv in the left branch is needed if c_in != c_out.\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor, time_emb: t.Tensor) -> t.Tensor:\n        \"\"\"\n        Note that the output of the (silu, linear) block should be of shape (batch, c_out). Since we would like to add this to the output of the first (conv, norm, silu) block, which will have a different shape, we need to first add extra dimensions to the output of the (silu, linear) block.\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w3d4_test.test_residual_block(ResidualBlock)\n\n\nclass DownBlock(nn.Module):\n    def __init__(self, channels_in: int, channels_out: int, time_emb_dim: int, groups: int, downsample: bool):\n        pass\n\n    def forward(self, x: t.Tensor, step_emb: t.Tensor) -> tuple[t.Tensor, t.Tensor]:\n        \"\"\"\n        x: shape (batch, channels, height, width)\n        step_emb: shape (batch, emb)\n        Return: (downsampled output, full size output to skip to matching UpBlock)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w3d4_test.test_downblock(DownBlock, downsample=True)\n    w3d4_test.test_downblock(DownBlock, downsample=False)\n\n\nclass UpBlock(nn.Module):\n    def __init__(self, dim_in: int, dim_out: int, time_emb_dim: int, groups: int, upsample: bool):\n        \"\"\"\n        IMPORTANT: arguments are with respect to the matching DownBlock.\n\n        \"\"\"\n        pass\n\n    def forward(self, x: t.Tensor, step_emb: t.Tensor, skip: t.Tensor) -> t.Tensor:\n        \"\"\" \"\"\"\n        pass\n\n\nif MAIN:\n    w3d4_test.test_upblock(UpBlock, upsample=True)\n    w3d4_test.test_upblock(UpBlock, upsample=False)\n\n\nclass MidBlock(nn.Module):\n    def __init__(self, mid_dim: int, time_emb_dim: int, groups: int):\n        pass\n\n    def forward(self, x: t.Tensor, step_emb: t.Tensor):\n        pass\n\n\nif MAIN:\n    w3d4_test.test_midblock(MidBlock)\n\n\nclass Unet(DiffusionModel):\n    def __init__(\n        self,\n        image_shape: tuple[int, int, int],\n        channels: int = 128,\n        dim_mults: tuple[int, ...] = (1, 2, 4, 8),\n        groups: int = 4,\n        max_steps: int = 1000,\n    ):\n        \"\"\"\n        image_shape: the input and output image shape, a tuple of (C, H, W)\n        channels: the number of channels after the first convolution.\n        dim_mults: the number of output channels for downblock i is dim_mults[i] * channels. Note that the default arg of (1, 2, 4, 8) will contain one more DownBlock and UpBlock than the DDPM image above.\n        groups: number of groups in the group normalization of each ResnetBlock (doesn't apply to attention block)\n        max_steps: the max number of (de)noising steps. We also use this value as the sinusoidal positional embedding dimension (although in general these do not need to be related).\n        \"\"\"\n        self.noise_schedule = None\n        self.img_shape = image_shape\n        pass\n\n    def forward(self, x: t.Tensor, num_steps: t.Tensor) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, channels, height, width)\n        num_steps: shape (batch, )\n\n        out: shape (batch, channels, height, width)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w3d4_test.test_unet(Unet)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}