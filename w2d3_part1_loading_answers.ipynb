{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# W2D3 - GPT-2\n\nIn Part 1, you'll implement your own GPT-2 architecture and load pretrained weights from the real GPT-2. In Part 2, you'll implement algorithms for sampling text from autoregressive models!\n\n## Table of Contents\n\n- [Readings](#readings)\n- [Inspecting the Pretrained Model](#inspecting-the-pretrained-model)\n- [Unidirectional Attention](#unidirectional-attention)\n- [GPT-2 Block](#gpt--block)\n- [Full GPT-2](#full-gpt-)\n- [Loading Pretrained Weights](#loading-pretrained-weights)\n- [Model Evaluation](#model-evaluation)\n\n## Readings\n\n- [Language Modelling with Transformers](https://docs.google.com/document/d/1XJQT8PJYzvL0CLacctWcT0T5NfL7dwlCiIqRtdTcIqA/edit#)\n- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) - don't worry about the API of HuggingFace - we are going to implement these algorithms ourselves.\n\n\nUnless otherwise indicated on the schematic, all arrows represent vectors of size `embedding_size = 768`.\n\nWe are only doing inference today, so while you should include the dropout layers, they won't be used and the tests won't check for them.\n\n```mermaid\ngraph TD\n    subgraph \" \"\n            subgraph GPT2\n            Token --> |integer|TokenEmbed[Token<br>Embedding] --> AddEmbed[Add] --> Dropout --> Blocks --> FinalLayerNorm[Final Layer Norm] --> Unembed --> |vocab size|Output\n            Position --> |integer|PosEmbed[Positional<br>Embedding] --> AddEmbed\n        end\n        subgraph One GPT2Block\n        Input --> LayerNorm1[Layer Norm 1<br>Attention] --> Residual1[Add] --> LayerNorm2[Layer Norm 2<br>MLP]--> Residual2 --> BlOutput[Output]\n        Input --> Residual1\n        Residual1 --> Residual2[Add]\n        end\n\n        subgraph SubMLP[MLP]\n            MLPInput[Input] --> Linear1 -->|4x hidden size|GELU --> |4x hidden size|Linear2 --> MLPDropout[Dropout] --> MLPOutput[Output]\n        end\n\n        subgraph SubAttention[Attention]\n            AtnInput[Input] --> Q & K & V\n            Q & K --> |head size|Dot[Dot<br>Scale Down<br>Mask<br>Softmax<br>Dropout] -->WeightedSum -->|head size| O --> LastDropout[Dropout] --> AtnOutput[Output]\n            V -->|head size| WeightedSum\n        end\n    end\n```\n\n## Inspecting the Pretrained Model\n\nLoad the pretrained GPT-2 using the provided `load_pretrained_gpt` function. Note that this is the small version of GPT-2, which at 4 bytes per parameter is roughly 500MB. The largest GPT-2 version is around 6GB. Inspect the shapes of parameters in an attention layer. Make a list of things that look different than you expected before proceeding.\n\n<details>\n<summary>Spoiler - Show Differences</summary>\n\n- `c_attn` and `c_proj` use a `Conv1D` instead of `Linear`. These are equivalent in this case and you can load the weights into your `Linear` directly.\n- `c_attn.weight` has shape `[768, 2304]` - this is the `Q`, `K`, and `V` matrixes concatenated in that order. The point of concatenating them is to apply them all to the input in one call. You can `torch.split()` the result.\n- `c_proj.weight` has shape `[768, 768]` - this is the `O` matrices for each head concatenated.\n\n</details>\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "import math\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Optional\nimport torch as t\nimport transformers\nfrom einops import rearrange\nfrom fancy_einsum import einsum\nfrom torch import nn\nimport utils\nimport w2d3_test\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nMAIN = __name__ == \"__main__\"\n\n\n@dataclass(frozen=True)\nclass GPTConfig:\n    \"\"\"Constants used throughout the GPT2 model.\"\"\"\n\n    activation_function: str = \"gelu\"\n    num_layers: int = 12\n    num_heads: int = 12\n    vocab_size: int = 50257\n    hidden_size: int = 768\n    max_position_embeddings: int = 1024\n    dropout: float = 0.1\n    layer_norm_epsilon: float = 1e-05\n\n\nconfig = GPTConfig()\nif MAIN:\n    pretrained_gpt = utils.load_pretrained_gpt()\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Unidirectional Attention\n\nImplement unidirectional attention with multiple heads and batching, keeping in mind:\n\n- Use the names `qkv_proj` and `output_proj` as indicated in the class definition.\n- Wherever q cannot attend to k (because q is earlier in the sequence than k), set the corresponding attention score (pre-softmax) to -1e4\n- Don't forget to divide your attention scores by $\\sqrt{head size}$\n- Make sure any created tensors are on the correct device.\n- `head_size` should default to `hidden_size // num_heads` if not specified.\n- Ignore the cache parameter for now - we'll use it in a later section.\n- attn_dropout is applied to the attention probabilities, before multiplying with v.\n- resid_dropout is applied to the final output.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class UnidirectionalAttention(nn.Module):\n    qkv_proj: nn.Linear\n    output_proj: nn.Linear\n    attn_dropout: nn.Dropout\n    resid_dropout: nn.Dropout\n\n    def __init__(self, hidden_size: int, num_heads: int, head_size: Optional[int] = None, dropout=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        assert hidden_size % num_heads == 0\n        pass\n\n    def forward(self, x: t.Tensor, cache: Optional[Any] = None) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, seq, hidden_size)\n\n        Return: shape (batch, seq, hidden_size)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w2d3_test.test_unidirectional_attn(UnidirectionalAttention)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## GPT-2 Block\n\nBuild the GPTBlock class according to the diagram.\n\n- Initialize your sub-modules in the order they're used.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "ACTIVATION_FUNCTIONS = dict(relu=nn.ReLU(), gelu=nn.GELU())\n\n\nclass GPT2Block(nn.Module):\n    attn: UnidirectionalAttention\n    linear1: nn.Linear\n    linear2: nn.Linear\n    ln1: nn.LayerNorm\n    ln2: nn.LayerNorm\n\n    def __init__(\n        self, hidden_size: int, num_heads: int, dropout: float, layer_norm_epsilon: float, activation_function: str\n    ):\n        super().__init__()\n        pass\n\n    def forward(self, x: t.Tensor, cache: Optional[Any] = None) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, seq, hidden_size)\n\n        Return: shape (batch, seq, hidden_size)\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w2d3_test.test_gpt_block(GPT2Block)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Full GPT-2\n\nNow build the GPT2 class according to the diagram.\n\n- The unembedding shouldn't be its own `nn.Embedding`. Instead, in `forward`, write an `einsum` using `token_embedding.weight` to perform the unembedding.\n- Again, initialize your sub-modules in the order they're used, and token embedding before position embedding.\n- Again, make sure any created tensors are on the correct device.\n\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "class GPT2(nn.Module):\n    token_embedding: nn.Embedding\n    pos_embedding: nn.Embedding\n    ln: nn.LayerNorm\n    blocks: utils.StaticModuleList[GPT2Block]\n\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        pass\n\n    def forward(self, x: t.Tensor, cache: Optional[Any] = None) -> t.Tensor:\n        \"\"\"\n        x: shape (batch, seq), dtype t.int64 - the token ids\n\n        Return: shape (batch, seq, vocab_size), dtype t.float32- the output logits\n        \"\"\"\n        pass\n\n\nif MAIN:\n    w2d3_test.test_gpt(GPT2)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Loading Pretrained Weights\n\nTweak the provided code to load the weights into your model. You should only need to adjust the variable names to match the ones in your model.\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "def _copy_weight_bias(mine, theirs, transpose=False):\n    mine.weight.copy_(theirs.weight.T if transpose else theirs.weight)\n    if mine.bias is not None:\n        mine.bias.copy_(theirs.bias)\n\n\ndef load_pretrained_weights():\n    pretrained_gpt = utils.load_pretrained_gpt()\n    my_gpt = GPT2(config)\n    for p in my_gpt.parameters():\n        p.requires_grad = False\n    my_gpt.token_embedding.weight.copy_(pretrained_gpt.transformer.wte.weight)\n    my_gpt.pos_embedding.weight.copy_(pretrained_gpt.transformer.wpe.weight)\n    _copy_weight_bias(my_gpt.ln, pretrained_gpt.transformer.ln_f)\n    from transformers.models.gpt2.modeling_gpt2 import GPT2Block as HFGPT2Block\n\n    my_block: GPT2Block\n    hf_block: HFGPT2Block\n    for (my_block, hf_block) in zip(my_gpt.blocks, pretrained_gpt.transformer.h):\n        _copy_weight_bias(my_block.ln1, hf_block.ln_1)\n        _copy_weight_bias(my_block.attn.qkv_proj, hf_block.attn.c_attn, transpose=True)\n        _copy_weight_bias(my_block.attn.output_proj, hf_block.attn.c_proj, transpose=True)\n        _copy_weight_bias(my_block.ln2, hf_block.ln_2)\n        _copy_weight_bias(my_block.linear1, hf_block.mlp.c_fc, transpose=True)\n        _copy_weight_bias(my_block.linear2, hf_block.mlp.c_proj, transpose=True)\n    for p in my_gpt.parameters():\n        p.requires_grad_(True)\n    return my_gpt\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Model Evaluation\n\nIf everything worked correctly, your GPT-2 should have an idea about which presidents's names start with George:\n\n\n"}, {"cell_type": "code", "metadata": {}, "source": "if MAIN:\n    my_gpt = load_pretrained_weights()\n    my_gpt.eval()\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n    w2d3_test.test_load_pretrained_weights(my_gpt, tokenizer)\n\n", "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "\nOn to part 2!\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.15"}}, "nbformat": 4, "nbformat_minor": 4}