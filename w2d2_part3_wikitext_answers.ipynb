{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# W2D2 Part 3 - WikiText Data Prep\n","\n","Now we'll prepare text data to train a BERT from scratch! The largest BERT would require days of training and a large training set, so we're going to train a tiny BERT on a small training set: [WikiText](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/). This comes in a small version WikiText-2 (the 2 means 2 million tokens in the train set) and a medium version WikiText-103 with 103 million tokens. For the sake of fast feedback loops, we'll be using the small version but in the bonus you'll be able to use the medium version with the same code by changing the `DATASET` variable below. Both versions consist of text taken from Good and Featured articles on Wikipedia.\n","\n","## Table of Contents\n","\n","- [Data Preparation](#data-preparation)\n","    - [Vocab Size](#vocab-size)\n","    - [Context Length](#context-length)\n","    - [Data Inspection](#data-inspection)\n","    - [Use of zipfile library](#use-of-zipfile-library)\n","    - [Preprocessing](#preprocessing)\n","    - [Masking](#masking)\n","    - [Loss Function](#loss-function)\n","    - [Cross Entropy of MLM](#cross-entropy-of-mlm)\n","- [Bonus](#bonus)\n","    - [Context Length Experimentation](#context-length-experimentation)\n","    - [Whole Word Masking](#whole-word-masking)\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import hashlib\n","import os\n","import sys\n","import zipfile\n","import torch as t\n","import transformers\n","from einops import rearrange, repeat\n","from torch import nn\n","from torch.nn import functional as F\n","from tqdm.auto import tqdm\n","import w2d2_test\n","from torch.utils.data import DataLoader, TensorDataset\n","from w2d2_part2_sentiment_answers import BertClassifier\n","import requests\n","\n","    \n","def maybe_download(url: str, path: str) -> None:\n","    \"\"\"Download the file from url and save it to path. If path already exists, do nothing.\"\"\"\n","    if not os.path.exists(path):\n","        with requests.get(url, stream=True) as r:\n","            r.raise_for_status()\n","            with open(path, \"wb\") as f:\n","                for chunk in r.iter_content(chunk_size=8192):\n","                    f.write(chunk)    \n","\n","MAIN = __name__ == \"__main__\"\n","device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n","DATA_FOLDER = \"./data/w2d2\"\n","DATASET = \"2\"\n","BASE_URL = \"https://s3.amazonaws.com/research.metamind.io/wikitext/\"\n","DATASETS = {\"103\": \"wikitext-103-raw-v1.zip\", \"2\": \"wikitext-2-raw-v1.zip\"}\n","TOKENS_FILENAME = os.path.join(DATA_FOLDER, f\"wikitext_tokens_{DATASET}.pt\")\n","IS_CI = os.getenv(\"IS_CI\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Data Preparation\n","\n","Since we aren't using pretrained weights, we don't have to match the tokenizer like we did when fine-tuning. We're free to use any tokenization strategy we want.\n","\n","### Vocab Size\n","\n","For example, we could use a smaller vocabulary in order to save memory on the embedding weights. It's straightforward to train a new tokenizer, but for the sake of time we'll continue using the existing tokenizer and its vocabulary size.\n","\n","### Context Length\n","\n","We're also free to use a shorter or longer context length, and this doesn't require training a new tokenizer. The only thing that this really affects is the positional embeddings. For a fixed compute budget, it's not obvious whether we should decrease the context length or increase it.\n","\n","The computational cost of attention is quadratic in the context length, so decreasing it would allow us to use more compute elsewhere, or just finish training earlier. Increasing it would allow for longer range dependencies; for example, our model could learn that if a proper noun appears early in a Wikipedia article, it's likely to appear again.\n","\n","The authors pretrain using a length of 128 for 90% of the steps, then use 512 for the rest of the steps. The idea is that early in training, the model is mostly just learning what tokens are more or less frequent, and isn't able to really take advantage of the longer context length until it has the basics down. Since our model is small, we'll do the simple thing to start: a constant context length of 128.\n","\n","### Data Inspection\n","\n","Run the below cell and inspect the text. It is one long string, so don't try to print the whole thing. What are some things you notice?\n","\n","<details>\n","\n","<summary>Spoiler - Things to Notice</summary>\n","\n","There is still some preprocessing done even though this is allegedly \"raw\" text. For example, there are spaces before and after every comma.\n","\n","There are Japanese characters immediately at the start of the training set, which in a real application we might want to do something with depending on our downstream use case.\n","\n","There is some markup at least for section headings. Again, this might be something we'd want to manually handle.\n","\n","</details>\n","\n","### Use of zipfile library\n","\n","It's important to know that the `zipfile` standard library module is written in pure Python, and while this makes it portable it is extremely slow as a result. It's fine here, but for larger datasets, definitely don't use it - it's better to launch a subprocess and use an appropriate decompression program for your system like `unzip` or `7-zip`.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using dataset WikiText-2 - options are 2 and 103\n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," = Homarus gammarus = \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," = = Description = = \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," Homarus gammarus is a large crustacean , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 – 6 kilograms ( 11 – 13 lb ) , although the lobsters caught in lobster pots are usually 23 – 38 cm ( 9 – 15 in ) long and weigh 0 @.@ 7 – 2 @.@ 2 kg ( 1 @.@ 5 – 4 @.@ 9 lb ) . Like other crustaceans , lobsters have a hard exoskeleton which they must shed in order to grow , in a process called ecdysis ( moulting ) . This may occur several times a year for young lobsters , but decreases to once every 1 – 2 years for larger animals . \n","------------------------------------------------------------\n"," The first pair of pereiopods is armed with a large , asymmetrical pair of claws . The larger one is the \" crusher \" , and has rounded nodules used for crushing prey ; the other is the \" cutter \" , which has sharp inner edges , and is used for holding or tearing the prey . Usually , the left claw is the crusher , and the right is the cutter . \n","------------------------------------------------------------\n"," The exoskeleton is generally blue above , with spots that coalesce , and yellow below . The red colour associated with lobsters only appears after cooking . This occurs because , in life , the red pigment astaxanthin is bound to a protein complex , but the complex is broken up by the heat of cooking , releasing the red pigment . \n","------------------------------------------------------------\n"," The closest relative of H. gammarus is the American lobster , Homarus americanus . The two species are very similar , and can be crossed artificially , although hybrids are unlikely to occur in the wild since their ranges do not overlap . The two species can be distinguished by a number of characteristics : \n","------------------------------------------------------------\n"," The rostrum of H. americanus bears one or more spines on the underside , which are lacking in H. gammarus . \n","------------------------------------------------------------\n"," The spines on the claws of H. americanus are red or red @-@ tipped , while those of H. gammarus are white or white @-@ tipped . \n","------------------------------------------------------------\n"," The underside of the claw of H. americanus is orange or red , while that of H. gammarus is creamy white or very pale red . \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," = = Life cycle = = \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," Female H. gammarus reach sexual maturity when they have grown to a carapace length of 80 – 85 millimetres ( 3 @.@ 1 – 3 @.@ 3 in ) , whereas males mature at a slightly smaller size . Mating typically occurs in summer between a recently moulted female , whose shell is therefore soft , and a hard @-@ shelled male . The female carries the eggs for up to 12 months , depending on the temperature , attached to her pleopods . Females carrying eggs are said to be \" berried \" and can be found throughout the year . \n","------------------------------------------------------------\n"," The eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on zooplankton . This stage involves three moults and lasts for 15 – 35 days . After the third moult , the juvenile takes on a form closer to the adult , and adopts a benthic lifestyle . The juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . It is estimated that only 1 larva in every 20 @,@ 000 survives to the benthic phase . When they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," = = Distribution = = \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," Homarus gammarus is found across the north @-@ eastern Atlantic Ocean from northern Norway to the Azores and Morocco , not including the Baltic Sea . It is also present in most of the Mediterranean Sea , only missing from the section east of Crete , and along only the north @-@ west coast of the Black Sea . The northernmost populations are found in the Norwegian fjords Tysfjorden and Nordfolda , inside the Arctic Circle . \n","------------------------------------------------------------\n"," The species can be divided into four genetically distinct populations , one widespread population , and three which have diverged due to small effective population sizes , possibly due to adaptation to the local environment . The first of these is the population of lobsters from northern Norway , which have been referred to as the \" midnight @-@ sun lobster \" . The populations in the Mediterranean Sea are distinct from those in the Atlantic Ocean . The last distinct population is found in the Netherlands : samples from the Oosterschelde were distinct from those collected in the North Sea or English Channel . \n","------------------------------------------------------------\n"," Attempts have been made to introduce H. gammarus to New Zealand , alongside other European species such as the edible crab , Cancer pagurus . Between 1904 and 1914 , one million lobster larvae were released from hatcheries in Dunedin , but the species did not become established there . \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," = = Ecology = = \n","------------------------------------------------------------\n"," \n","------------------------------------------------------------\n"," Adult H. gammarus live on the continental shelf at depths of 0 – 150 metres ( 0 – 492 ft ) , although not normally deeper than 50 m ( 160 ft ) . They prefer hard substrates , such as rocks or hard mud , and live in holes or crevices , emerging at night to feed . \n","------------------------------------------------------------\n"," The diet of H. gammarus mostly consists of other benthic invertebrates . These include crabs , molluscs , sea urchins , starfish and polychaete worms . \n"]}],"source":["if MAIN and (not IS_CI):\n","    path = os.path.join(DATA_FOLDER, DATASETS[DATASET])\n","    maybe_download(BASE_URL + DATASETS[DATASET], path)\n","    expected_hexdigest = {\"103\": \"0ca3512bd7a238be4a63ce7b434f8935\", \"2\": \"f407a2d53283fc4a49bcff21bc5f3770\"}\n","    with open(path, \"rb\") as f:\n","        actual_hexdigest = hashlib.md5(f.read()).hexdigest()\n","        assert actual_hexdigest == expected_hexdigest[DATASET]\n","if MAIN:\n","    print(f\"Using dataset WikiText-{DATASET} - options are 2 and 103\")\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","if MAIN and (not IS_CI):\n","    z = zipfile.ZipFile(path)\n","\n","    def decompress(split: str) -> str:\n","        return z.read(f\"wikitext-{DATASET}-raw/wiki.{split}.raw\").decode(\"utf-8\")\n","\n","    train_text = decompress(\"train\").splitlines()\n","    val_text = decompress(\"valid\").splitlines()\n","    test_text = decompress(\"test\").splitlines()\n","    for i in range(30):\n","        print('------------------------------------------------------------')\n","        print(val_text[i])\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Preprocessing\n","\n","To prepare data for the next sentence prediction task, we would want to use a library like [spaCy](https://spacy.io/) to break the text into sentences - it's tricky to do this yourself in a robust way. We'll ignore this task and just do masked language modelling today.\n","\n","Right now we have a list of lines, but we need (batch, seq) of tokens. We could use padding and truncation as before, but let's play with a different strategy:\n","\n","- [Call the tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__) on the list of lines with `truncation=False` to obtain lists of tokens. These will be of varying length, and you'll notice some are empty due to blank lines.\n","- Build one large 1D tensor containing all the tokens in sequence\n","- Reshape the 1D tensor into (batch, sequence).\n","\n","Instead of padding, we'll just discard tokens at the very end that would form an incomplete sequence. This will only discard up to (max_seq - 1) tokens, so it's negligible.\n","\n","This is nice because we won't waste any space or compute on padding tokens, and we don't have to truncate long lines. Some fraction of sequences will contain both the end of one article and the start of another, but this won't happen too often and there will be clues the model can use, like the markup for a heading appearing.\n","\n","Note we won't need the attention mask, because we're not using padding. We'll also not need the `token_type_ids` or the special tokens CLS or SEP. You can pass arguments into the tokenizer to prevent it from returning these.\n","\n","Don't shuffle the tokens here. This allows us to change the context length at load time, without having to re-run this preprocessing step.\n","\n","You can ignore a warning about 'Token indices sequence length is longer than the specified maximum sequence length' - this is expected.\n","\n","<details>\n","\n","<summary>Why pass lists of lines instead of one big string</summary>\n","\n","The \"fast\" version of the tokenizer is written in Rust and will spawn threads to process the lines in parallel. If you only pass one big string, it doesn't know where to split the string and will only use one thread.\n","\n","</details>\n","\n","<details>\n","\n","<summary>I'm doing WikiText-103; how long should tokenization take?</summary>\n","\n","It takes about 2 minutes on my machine to tokenize the full WikiText-103.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing training text...\n","torch.Size([2452352])\n","128\n","Training data shape is:  torch.Size([128, 19159])\n","Tokenizing validation text...\n","torch.Size([253184])\n","128\n","Validation data shape is:  torch.Size([128, 1978])\n","Tokenizing test text...\n","torch.Size([290304])\n","128\n","Test data shape is:  torch.Size([128, 2268])\n","Saving tokens to:  ./data/w2d2/wikitext_tokens_2.pt\n"]}],"source":["def tokenize_1d(tokenizer, lines: list[str], max_seq: int) -> t.Tensor:\n","    \"\"\"Tokenize text and rearrange into chunks of the maximum length.\n","\n","    Return (batch, seq) and an integer dtype\n","    \"\"\"\n","\n","    tokenized_lines = []\n","    for line in lines:\n","        tokenized_lines.extend(tokenizer.encode(line))\n","    \n","    tokenized_lines = t.tensor(tokenized_lines, dtype=t.long)[:(len(tokenized_lines) // max_seq) * max_seq]\n","    print(tokenized_lines.shape)\n","    print(max_seq)\n","\n","    tokenized_lines = rearrange(tokenized_lines, \"(seq batch) -> batch seq\", batch=max_seq)\n","    return tokenized_lines\n","    \n","    \n","\n","\n","if MAIN and (not IS_CI):\n","    max_seq = 128\n","    print(\"Tokenizing training text...\")\n","    train_data = tokenize_1d(tokenizer, train_text, max_seq)\n","    print(\"Training data shape is: \", train_data.shape)\n","    print(\"Tokenizing validation text...\")\n","    val_data = tokenize_1d(tokenizer, val_text, max_seq)\n","    print(\"Validation data shape is: \", val_data.shape)\n","    print(\"Tokenizing test text...\")\n","    test_data = tokenize_1d(tokenizer, test_text, max_seq)\n","    print(\"Test data shape is: \", test_data.shape)\n","    print(\"Saving tokens to: \", TOKENS_FILENAME)\n","    t.save((train_data, val_data, test_data), TOKENS_FILENAME)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Masking\n","\n","Implement `random_mask`, which we'll call during training on each batch.\n","\n","Hint: ensure any tensors that you create are on the same device as `input_ids`. When sampling a random token, sample uniformly at random from [0..vocabulary size).\n","\n","<details>\n","\n","<summary>Help, I'm confused about the indexing!</summary>\n","\n","I found it easier to flatten batch and seq together into one dimension, so that indexes can be a simple integer instead of having to index into multiple dimensions. You can use `randperm` to select random indexes, and then partition the indexes among the possible modifications.\n","\n","Unflatten at the end.\n","\n","</details>\n","\n","<details>\n","\n","<summary>Is there anything special or optimal about the numbers 15%, 80%, and 10%?</summary>\n","\n","No, these are just some ad-hoc numbers that the BERT authors chose. The paper [Should You Mask 15% in Masked Language Modelling](https://arxiv.org/pdf/2202.08005.pdf) suggests that you can do better.\n","\n","</details>\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def flat(x: t.Tensor) -> t.Tensor:\n","    \"\"\"Helper function for combining batch and sequence dimensions.\"\"\"\n","    return rearrange(x, \"b s ... -> (b s) ...\")\n","\n","\n","def unflat(x: t.Tensor, max_seq: int) -> t.Tensor:\n","    \"\"\"Helper function for separating batch and sequence dimensions.\"\"\"\n","    return rearrange(x, \"(b s) ... -> b s ...\", s=max_seq)\n","\n","\n","def random_mask(\n","    input_ids: t.Tensor, mask_token_id: int, vocab_size: int, select_frac=0.15, mask_frac=0.8, random_frac=0.1\n",") -> tuple[t.Tensor, t.Tensor]:\n","    \"\"\"Given a batch of tokens, return a copy with tokens replaced according to Section 3.1 of the paper.\n","\n","    input_ids: (batch, seq)\n","\n","    Return: (model_input, was_selected) where:\n","\n","    model_input: (batch, seq) - a new Tensor with the replacements made, suitable for passing to the BertLanguageModel. Don't modify the original tensor!\n","\n","    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n","    \"\"\"\n","    pass\n","\n","\n","if MAIN:\n","    w2d2_test.test_random_mask(random_mask, input_size=10000, max_seq=max_seq)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Loss Function\n","\n","Exercise: what should the loss be if the model is predicting tokens uniformly at random? Use the [formula for discrete cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy).\n","\n","<details>\n","\n","<summary>Solution - Random Cross-Entropy Loss</summary>\n","\n","Let $k$ be the vocabulary size. For each token to be predicted, the expected probability assigned by the model to the true token is $1/k$. Plugging this into the cross entropy formula gives an expected loss of $log(k)$, which for $k=28996$ is about 10.2.\n","\n","Importantly, this is the loss per predicted token, and we have to decide how to aggregate these over the batch and sequence dimensions.\n","\n","For a batch, we can aggregate the loss per token in any way we want, as long as we're clear and consistent about what we're doing. Taking the mean loss per predicted token has the nice property that we can compare models with a different number of predicted tokens per batch.\n","\n","</details>\n","\n","Now, find the cross-entropy loss of the distribution of unigram frequencies. This is the loss you'd see when predicting words purely based on word frequency without the context of other words. During pretraining, your model should reach this loss very quickly, as it only needs to learn the final unembedding bias to predict this unigram frequency.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MAIN:\n","    \"TODO: YOUR CODE HERE\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Cross Entropy of MLM\n","\n","For our loss function, we only want to sum up the loss at the tokens that were chosen with probability `select_frac`. As a reminder, when a token is selected, that input token could be replaced with either [MASK], a random token, or left as-is and the target is the original, unmodified input token.\n","\n","Write a wrapper around [torch.nn.functional.cross_entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy) that only looks at the selected positions. It should output the total loss divided by the number of selected tokens.\n","\n","`torch.nn.functional.cross_entropy` divides by the batch size by default, which means that the magnitude of the loss will be larger if there are more predictions made per batch element. We will want to divide by the number of tokens predicted: this ensures that we can interpret the resulting value and we can compare models with different sequence lengths.\n","\n","<details>\n","\n","<summary>I'm confused about how to do this!</summary>\n","\n","Again, it's simpler to flatten batch and seq together so that you only have to think about one spatial dimension.\n","\n","You can either slice both input and target arguments to `cross_entropy` so that you're only passing the contributing parts, or you can make use of the `ignore_index` keyword argument of `cross_entropy` and set the target to -100 when it shouldn't contribute.\n","\n","</details>\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cross_entropy_selected(pred: t.Tensor, target: t.Tensor, was_selected: t.Tensor) -> t.Tensor:\n","    \"\"\"\n","    pred: (batch, seq, vocab_size) - predictions from the model\n","    target: (batch, seq, ) - the original (not masked) input ids\n","    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n","\n","    Out: the mean loss per predicted token\n","    \"\"\"\n","    pass\n","\n","\n","if MAIN:\n","    w2d2_test.test_cross_entropy_selected(cross_entropy_selected)\n","if MAIN and (not IS_CI):\n","    batch_size = 8\n","    seq_length = 512\n","    batch = t.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n","    pred = t.rand((batch_size, seq_length, tokenizer.vocab_size))\n","    (masked, was_selected) = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n","    loss = cross_entropy_selected(pred, batch, was_selected).item()\n","    print(f\"Random MLM loss on random tokens - does this make sense? {loss:.2f}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Bonus\n","\n","Go on to step 4!\n","\n","### Context Length Experimentation\n","\n","Play with a shorter context length and observe the difference in training speed. Does it decrease performance, or was the small model unable to make much use of the longer context length anyway?\n","\n","### Whole Word Masking\n","\n","The [official BERT repo](https://github.com/google-research/bert) has a README section on a different way of computing the mask. Try implementing it and see if you get any benefit.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
